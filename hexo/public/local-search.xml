<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>README</title>
    <link href="/2020/09/29/README/"/>
    <url>/2020/09/29/README/</url>
    
    <content type="html"><![CDATA[<h1 id="权掌天下的博客"><a href="#权掌天下的博客" class="headerlink" title="权掌天下的博客"></a>权掌天下的博客</h1><p>主要是docker、kubernetes的相关工作总结文档，以及一些其他的技术文档.</p><p>在线浏览：<a href="https://www.my-blog.wang" target="_blank" rel="noopener">www.my-blog.wang</a></p><p>Github地址：<a href="https://github.com/ILIKETWICE/iliketwice.github.io" target="_blank" rel="noopener">https://github.com/ILIKETWICE/iliketwice.github.io</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>博客中的文章为个人工作总结文档以及优质文章的汇总，主要偏向于Linux运维方面。</p><h1 id="交流"><a href="#交流" class="headerlink" title="交流"></a>交流</h1><ul><li>博客地址：<a href="https://www.my-blog.wang" target="_blank" rel="noopener">权掌天下的博客</a></li><li>个人邮箱：<a href="mailto:15210198546@163.com">15210198546@163.com</a></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title> Dockerfile就这么简单 </title>
    <link href="/2020/07/20/2020-07-20-dockerfile/"/>
    <url>/2020/07/20/2020-07-20-dockerfile/</url>
    
    <content type="html"><![CDATA[<p>当我们在使用docker时，最重要的就是镜像，只要有了镜像，我们就可以随时随地的根据镜像来创建一个容器，从而做到让我们的服务可以在任何时间任何地点任何环境下运行起来。那么镜像是怎么制作的呢？总体来讲，制作镜像有两种方法：</p><ol><li>根据一个已有的镜像运行容器，然后根据这个容器来制作我们自己的镜像；</li><li>使用DockerFile来制作一个镜像模板文件，使用这个文件来创建镜像；</li></ol><p>对于第一种方法，我们在上一篇文章中最后有提及，就是利用docker commit命令，将我们的变更打包成一个新的镜像。但是这种方法需要我们每次都运行一个容器，然后在容器中做更改后再打包，很明显这种方式效率很低，而且更改不方便。所以这种方式一般不建议大家采用。我们更多的要使用DockerFile的方式来定制我们的镜像，接下来，我们就详细的介绍一下DockerFile的制作方法。</p><h1 id="一、利用Dockerfile制作镜像的准备工作"><a href="#一、利用Dockerfile制作镜像的准备工作" class="headerlink" title="一、利用Dockerfile制作镜像的准备工作"></a>一、利用Dockerfile制作镜像的准备工作</h1><p>在制作Dockerfile前，我们需要做一系列的准备工作。首先，我们要创建一个目录，用来存储我们的Dockerfile，我们需要打包进镜像中的所有文件也都要放在这个这个目录下，我们制作镜像的时候也要在这个目录下来完成。其次，我们要创建一个文件名为Dockerfile，这个文件必须是大写开头，文件名必须为Dockerfile。当我们编写好我们的Dockerfile文件后，我们需要用docker build命令来执行创建镜像。</p><h1 id="二、Dockerfile指令"><a href="#二、Dockerfile指令" class="headerlink" title="二、Dockerfile指令"></a>二、Dockerfile指令</h1><p>我们准备好相关的目录和文件后，我们就可以开始编写我们的Dockerfile了，Dockerfile其实就是由一些指令组合成的，在Dockerfile中一行就是一条指令，每行开头的第一个单词就是指令本身，指令可以用大写也可以用小写，但是一般我们使用大写来表示指令。</p><h2 id="1-FROM指令"><a href="#1-FROM指令" class="headerlink" title="1. FROM指令"></a>1. FROM指令</h2><p>每一个Dockerfile的第一个非注释行都必须使用FROM指令，这个指令指明了我们制作镜像使用的基础镜像，格式如下：</p><pre><code class="hljs pf">FROM <span class="hljs-variable">&lt;镜像仓库名&gt;</span>[:<span class="hljs-keyword">tag</span>]FROM <span class="hljs-variable">&lt;镜像仓库名&gt;</span>@<span class="hljs-variable">&lt;镜像哈希值&gt;</span></code></pre><p>默认情况下，docker build命令会优先从本地查找我们使用到的基础镜像，如果找不到则自动去我们的镜像仓库中查找。我们在指定基础镜像的过程中可以使用镜像名，但是此时会出现一个问题，如果有人恶意更改了镜像名，用一个错误的镜像替换了我们正常的镜像，那么此时我们就会拉取到错误的镜像。为了避免这个问题的出现，我们可以使用镜像的哈希值来指定基础镜像，就是我们上面提到的使用@符号，这样一来我们使用的镜像就不会被恶意替换掉了。</p><p>FROM指令在使用镜像名时，可以省略标签名，默认会使用latest标签。</p><p>我们上面说了，每一个Dockerfile的第一个非注释行都必须使用FROM开头，但是ARG指令是唯一一个可以在FROM指令前出现的指令，这是一个例外的情况。</p><h2 id="2-LABEL指令"><a href="#2-LABEL指令" class="headerlink" title="2. LABEL指令"></a>2. LABEL指令</h2><p>LABEL指令用来指定一些元数据的，比如指定这个镜像文件的作者，联系方式，描述信息等等，格式如下：</p><pre><code class="hljs routeros">LABEL <span class="hljs-attribute">key1</span>=value1 <span class="hljs-attribute">key2</span>=value2 <span class="hljs-built_in">..</span>. <span class="hljs-attribute">keyN</span>=valueN</code></pre><p>在docker的早期版本中并没有LABEL指令，而是使用MAINTAINER指令，MAINTAINER指令后面只能跟一个字符串，用来指定作者的信息，在新版的docker中，这个指令已经被弃用，官方推荐使用LABEL指令来实现。</p><h2 id="3-RUN指令"><a href="#3-RUN指令" class="headerlink" title="3. RUN指令"></a>3. RUN指令</h2><p>RUN指令用来在创建镜像过程中执行一些命令，RUN指令有两种格式：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">RUN</span><span class="bash"> &lt;<span class="hljs-built_in">command</span>&gt;      直接跟命令</span><span class="hljs-keyword">RUN</span><span class="bash"> [<span class="hljs-string">"executable"</span>, <span class="hljs-string">"param1"</span>, <span class="hljs-string">"param2"</span>]    命令和其参数作为一个列表传入</span></code></pre><p>这两种方式有不同的效果，RUN指令后直接跟一个命令，会将此命令运行在一个shell中，在linux中默认是/bin/sh，这也就意味着我们可以在命令字符串中引用一些shell变量。但是在第二种方式中，所有的命令和参数放在了一个列表中传入，此时就无法引用shell中的变量。</p><p>除此之外，还有一点需要注意，就是在列表中一定不要用单引号来包裹参数，每个元素都要用双引号，否则会出现docker镜像运行错误的问题。原因就是docker build时会把这些列表当做json来处理，所以要符合json字符串的规则。</p><p>RUN指令执行的命令的结果会被打包到镜像当中，而且Dockerfile中后续的指令也可以使用。使用SHELL指令可以改变默认使用的shell。</p><h2 id="4-CMD指令"><a href="#4-CMD指令" class="headerlink" title="4. CMD指令"></a>4. CMD指令</h2><p>CMD指令是用来指定基于我们的镜像创建容器时，容器中运行的命令的，和RUN不同的地方在于，RUN是在构建镜像时执行的命令，CDM是在创建容器时执行的命令。在一个Dockerfile中只可以有一个CDM指令，如果定义了多个CMD指令，那只有最后一个CMD指令会生效。CMD指令使用格式如下：</p><pre><code class="hljs routeros">CMD [<span class="hljs-string">"executable"</span>,<span class="hljs-string">"param1"</span>,<span class="hljs-string">"param2"</span>] (exec form, this is the preferred form exec格式，这是推荐的格式)CMD [<span class="hljs-string">"param1"</span>,<span class="hljs-string">"param2"</span>] (as<span class="hljs-built_in"> default </span>parameters <span class="hljs-keyword">to</span> ENTRYPOINT 为ENTRYPOINT参数提供参数)CMD command param1 param2 (shell form shell格式的命令)</code></pre><p>CMD指令可以直接指定一个可执行命令，就是上述的第一和第三种方式，当创建容器时会去执行这个命令，而且需要注意的是，第三种方式是默认在shell中执行的，可以引用shell变量，而第一种方式并不会启动shell，所以就无法引用shell变量。</p><p>在采用第二种方式时，此时并没有指定可执行的命令，而是只指定了参数，此时，这些参数将作为ENTRYPOINT指令的参数，关于ENTRYPOINT指令，我们稍后介绍。</p><h2 id="5-ENTRYPOINT指令"><a href="#5-ENTRYPOINT指令" class="headerlink" title="5. ENTRYPOINT指令"></a>5. ENTRYPOINT指令</h2><p>ENTRYPOINT指令也是用来指定基于我们的镜像创建容器时需要执行的命令的，其使用格式如下：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> [<span class="hljs-string">"executable"</span>, <span class="hljs-string">"param1"</span>, <span class="hljs-string">"param2"</span>] (<span class="hljs-built_in">exec</span> form, preferred 推荐格式，使用json)</span><span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> <span class="hljs-built_in">command</span> param1 param2 (shell form shell格式)</span></code></pre><p>既然我们已经有了CMD指令了，那为什么我们还要弄一个ENTRYPOINT指令出来呢？这两者的区别在于，当我们使用CMD指令创建好镜像后，在使用这个镜像启动容器时，我们可以改变容器默认的命令，而自己定义启动容器时的命令，比如我们的CMD指令是启动nginx，但是我们在启动容器的时候可以指定命令来启动一个bash，此时，我们在命令行中指定的指令就替换掉了我们的CMD命令。但是我们如果使用ENTRYPOINT指令来指定执行的命令，那么在命令行中启动镜像时，在镜像名之后我们自己指定的命令将不会执行，而是作为参数传递给了ENTRYPOINT命令。而且，在命令行中指定的命令，第一个参数并没有被传递给<code>ENTRYPOINT</code>，这是因为我们的docker默认认为第一个参数是要执行的命令，而其之后的才是真正的参数，参见如下所示，我们的“echo” 字符串并没有被输出出来：</p><pre><code class="hljs dockerfile">[root@localhost img1]<span class="hljs-comment"># cat Dockerfile </span><span class="hljs-keyword">FROM</span> centos:centos7<span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-string">"abc"</span> <span class="hljs-variable">$@</span></span>[root@localhost img1]<span class="hljs-comment"># docker run --rm centos:testv3 echo aaaaa bbbbb ccccc</span>abc aaaaa bbbbb ccccc[root@localhost img1]<span class="hljs-comment">#</span></code></pre><p>这个特性可以使我们在运行容器时禁止自定义启动命令，保证了容器运行结果与我们的预期完全一致。但是，我们并不是完全不能更改这个命令，docker为我们提供了<code>--entrypoint</code>参数来修改这个命令。但是这个参数和命令要写在镜像名之前才会生效。</p><pre><code class="hljs autoit">[root<span class="hljs-symbol">@localhost</span> img1]<span class="hljs-meta"># cat Dockerfile </span>FROM centos:centos7ENTRYPOINT echo <span class="hljs-string">"abc"</span>[root<span class="hljs-symbol">@localhost</span> img1]<span class="hljs-meta"># docker run -ti --rm centos:testv1 --entrypoint /bin/bash</span>abc[root<span class="hljs-symbol">@localhost</span> img1]<span class="hljs-meta"># docker run -ti --rm --entrypoint /bin/bash centos:testv1</span>[root<span class="hljs-symbol">@a0c502e6ba2f</span> /]<span class="hljs-meta"># exit</span><span class="hljs-keyword">exit</span></code></pre><p>在上面CMD命令的部分，我们可以给<code>CMD</code>命令不指定执行的命令而只指定参数，此时这些参数就会被传递给ENTRYPOINT指令。</p><p>此外，还需要注意一点，我们使用列表的格式来编写命令时，要注意使用双引号来包裹各个参数，而不是单引号。</p><p>Shell形式可防止使用任何CMD或<code>run</code> 命令行参数覆盖掉我们的运行命令，但具有以下缺点：ENTRYPOINT将作为<code>/bin/sh -c</code>的子命令启动，该子命令不传递信号。这意味着可执行文件将不是容器的<code>PID 1</code>，并且不会接收Unix信号，因此您的可执行文件将不会从<code>docker stop &lt;container&gt;</code>接收到<code>SIGTERM</code>。</p><h2 id="6-EXPOSE指令"><a href="#6-EXPOSE指令" class="headerlink" title="6. EXPOSE指令"></a>6. EXPOSE指令</h2><p>EXPOSE指令是用来暴露容器的端口的，其使用格式如下：</p><pre><code class="hljs xml">EXPOSE <span class="hljs-tag">&lt;<span class="hljs-name">port</span>&gt;</span> [<span class="hljs-tag">&lt;<span class="hljs-name">port</span>&gt;</span>/<span class="hljs-tag">&lt;<span class="hljs-name">protocol</span>&gt;</span>...]</code></pre><p>这个指令可以一次性指定暴露多个端口，且可以指定端口的协议，默认情况下是使用TCP协议，我们还可以自己定义使用的协议：</p><pre><code class="hljs angelscript">EXPOSE <span class="hljs-number">8080</span>/udp  暴露UDP协议的<span class="hljs-number">8080</span>端口</code></pre><p>但是需要注意的是，在使用了EXPOSE指令后指定的端口，在运行容器时并不会自动的建立容器和宿主机的映射关系，而是当我们运行容器时指定-P选项后其才会将这些端口映射到宿主机上，且我们在定义Dockerfile时不能指定容器端口映射到宿主机上的端口，只能是随机映射一个宿主机上的端口。</p><h2 id="7-ENV指令"><a href="#7-ENV指令" class="headerlink" title="7. ENV指令"></a>7. ENV指令</h2><p>ENV指令用于创建环境变量，这些环境变量可以在构建镜像阶段供Dockerfile之后的指令所引用，其格式如下：</p><pre><code class="hljs xml">ENV <span class="hljs-tag">&lt;<span class="hljs-name">key</span>&gt;</span> <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>ENV <span class="hljs-tag">&lt;<span class="hljs-name">key</span>&gt;</span>=<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span> ...</code></pre><p>第一种格式用来设置单个的环境变量，ENV指令后被空格分隔的第一个字符串会被当成是环境变量的KEY，后面的所有值都会被当成是该KEY的VALUE值，第二种格式可以一次设置多个环境变量，使用等号来声明KEY和VALUE，如果VALUE部分包含空格，我们可以用引号将VALUE部分引起来，也可以用反斜杠对空格做转义处理。例如：</p><pre><code class="hljs routeros"><span class="hljs-comment"># 第一种格式，一行定义一对环境变量</span>ENV myName John DoeENV myDog Rex The DogENV myCat fluffy<span class="hljs-comment"># 第二种方式，一行定义多对环境变量</span>ENV <span class="hljs-attribute">myName</span>=<span class="hljs-string">"John Doe"</span> <span class="hljs-attribute">myDog</span>=Rex\ The\ Dog \    <span class="hljs-attribute">myCat</span>=fluffy</code></pre><p>通过ENV指令设置的环境变量将被保留在生成的镜像中，我们用此镜像创建容器后，可以用docker inspect 命令来查看，也可以在运行容器时，使用<code>docker run --env &lt;key&gt;=&lt;value&gt;</code>的方式来指定。</p><h2 id="8-ADD指令"><a href="#8-ADD指令" class="headerlink" title="8. ADD指令"></a>8. ADD指令</h2><p>ADD指令用来向镜像中添加文件，其有两种格式：</p><pre><code class="hljs routeros"><span class="hljs-builtin-name">ADD</span> [<span class="hljs-attribute">--chown</span>=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;<span class="hljs-built_in">..</span>. &lt;dest&gt;<span class="hljs-builtin-name">ADD</span> [<span class="hljs-attribute">--chown</span>=&lt;user&gt;:&lt;group&gt;] [<span class="hljs-string">"&lt;src&gt;"</span>,<span class="hljs-built_in">..</span>. <span class="hljs-string">"&lt;dest&gt;"</span>]</code></pre><p><code>--chown</code>选项可以在添加文件时改变文件的属主和属组，但是需要注意，这个特性只支持Linux类型的容器，在windows容器上不起作用。</p><p>ADD指令可以从<code>&lt;src&gt;</code>指定的文件、目录或者URL拷贝文件到镜像文件系统中的<code>&lt;dest&gt;</code>路径下，并且可以指定多个<code>&lt;src&gt;</code>，在有多个<code>&lt;src&gt;</code>时，最后一个作为目的地址，其前面的字段都会作为<code>&lt;src&gt;</code>字段。同时，在原地址字段中，也支持正则匹配。并且，目的地址是一个绝对路径，或者当<code>WORKDIR</code>指令指定了工作目录后，也可以是这个目录下的相对路径。而原文件必须在Dockerfile所在的目录下或其子目录下。</p><p>添加包含特殊字符（例如[和]）的文件或目录时，您需要按照Golang规则转义那些路径，以防止将它们视为匹配模式。例如，要添加名为arr [0] .txt的文件，请使用以下命令：</p><pre><code class="hljs gradle">ADD arr[[]<span class="hljs-number">0</span>].txt <span class="hljs-regexp">/mydir/</span>    # <span class="hljs-keyword">copy</span> a <span class="hljs-keyword">file</span> named <span class="hljs-string">"arr[0].txt"</span> to <span class="hljs-regexp">/mydir/</span></code></pre><p>如果没有添加<code>--chown</code>标志，所有新添加的文件或目录属主属组默认是0。<code>--chown</code>标志允许提供属主名和属组名，如果提供了用户名或组名，则将使用容器的根文件系统<code>/etc/passwd</code>和<code>/etc/group</code>文件分别执行从名称到整数UID或GID的转换，也可以提供其对应的UID和GID，如果只提供了属主，则默认会使用和属主UID相同的GID来指定属组，如下都是正确的定义格式：</p><pre><code class="hljs routeros"><span class="hljs-builtin-name">ADD</span> <span class="hljs-attribute">--chown</span>=55:mygroup files* /somedir/<span class="hljs-builtin-name">ADD</span> <span class="hljs-attribute">--chown</span>=bin files* /somedir/<span class="hljs-builtin-name">ADD</span> <span class="hljs-attribute">--chown</span>=1 files* /somedir/<span class="hljs-builtin-name">ADD</span> <span class="hljs-attribute">--chown</span>=10:11 files* /somedir/</code></pre><p>如果通过<code>--chown</code>标志使用用户名或者属主名来指定属主或属组，而在容器的文件系统中不存在 <code>/etc/passwd</code> 或者 <code>/etc/group</code> 文件，此时构建镜像时会在ADD操作时失败。但是使用数字来指定时，创建镜像的时候并不会去查找此UID或GID是否存在，也不会依赖容器的根文件系统。需要注意的是，如果源文件是一个URL，而这个URL需要登录认证的话，那么需要使用wget或者curl的方式来下载文件，ADD指令并不能完成登录认证。</p><p><strong>「ADD指令遵循如下的规则：」</strong></p><ol><li>如果是URL，并且不以斜杠结尾，则从URL下载文件并将其复制到;</li><li>如果是URL，并且以斜杠结尾，则从URL推断文件名，并将文件下载到/。例如，ADD <a href="http://example.com/foobar" target="_blank" rel="noopener">http://example.com/foobar</a> /，将创建文件 /foobar。该URL必须具有具体的路径及文件名，以便在这种情况下可以找到适当的文件名（例如这样的URL：<a href="http://example.com将不起作用）" target="_blank" rel="noopener">http://example.com将不起作用）</a>;</li><li>如果是目录，则将复制目录的整个内容，包括文件系统元数据。注意，此时目录本身并不会被复制，而是递归复制这个目录下的所有文件;</li><li>当是本地的一个通过gzip, bzip2 or xz压缩的tar压缩包，ADD指令会自动将这个包解压。但是如果是一个URL时则不会解压。</li></ol><blockquote><p>❝</p><p><strong>「注意」</strong>：文件是否被识别为压缩格式仅根据文件的内容而不是文件的名称来确定。例如，如果一个空文件碰巧以.tar.gz结尾，则该文件将不会被识别为压缩文件，并且不会生成任何类型的解压缩错误消息，而是会将文件简单地复制到目标位置。</p><p>❞</p></blockquote><ol><li>如果是任何其他类型的文件，则将其与其元数据一起单独复制。在这种情况下，如果以尾斜杠/结束，则它将被视为目录，并且的内容将写入/base();</li><li>如果直接或由于使用通配符而指定了多个资源，则必须是目录，并且必须以斜杠/结尾;</li><li>如果不以斜杠结尾，它将被视为常规文件，并且的内容将写入;</li><li>如果不存在，它将与路径中所有缺少的目录一起创建。</li></ol><h2 id="9-COPY指令"><a href="#9-COPY指令" class="headerlink" title="9. COPY指令"></a>9. COPY指令</h2><p>COPY用于向镜像中复制文件，用法与ADD指令类似，但是也有一些区别，其格式如下：</p><pre><code class="hljs xml">COPY [--chown=<span class="hljs-tag">&lt;<span class="hljs-name">user</span>&gt;</span>:<span class="hljs-tag">&lt;<span class="hljs-name">group</span>&gt;</span>] <span class="hljs-tag">&lt;<span class="hljs-name">src</span>&gt;</span>... <span class="hljs-tag">&lt;<span class="hljs-name">dest</span>&gt;</span>COPY [--chown=<span class="hljs-tag">&lt;<span class="hljs-name">user</span>&gt;</span>:<span class="hljs-tag">&lt;<span class="hljs-name">group</span>&gt;</span>] ["<span class="hljs-tag">&lt;<span class="hljs-name">src</span>&gt;</span>",... "<span class="hljs-tag">&lt;<span class="hljs-name">dest</span>&gt;</span>"]</code></pre><p>COPY指令也可以复制多个文件，也支持通配符匹配，用法基本类似ADD指令，但是COPY指令只能接受一个本地文件或目录，不能COPY远程的URL。而且COPY的文件也必须放在Dockerfile同级目录或其同级目录之下的目录中。</p><p><strong>「COPY指令遵循如下规则：」</strong></p><ol><li>如果是目录，则将复制目录的整个内容，包括文件系统元数据。且目录本身不被复制，仅其内容被复制；</li><li>如果是任何其他类型的文件，则将其与其元数据一起单独复制。在这种情况下，如果以尾斜杠/结束，则它将被视为目录，并且的内容将写入/base()；</li><li>如果直接或由于使用通配符而指定了多个资源，则必须是目录，并且必须以斜杠/结尾；</li><li>如果不以斜杠结尾，它将被视为常规文件，并且的内容将写入；</li><li>如果不存在，它将与路径中所有缺少的目录一起创建；</li></ol><h2 id="10-VOLUME指令"><a href="#10-VOLUME指令" class="headerlink" title="10. VOLUME指令"></a>10. VOLUME指令</h2><p>VOLUME指令用于挂载宿主机上的目录到容器中，其格式如下：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">VOLUME</span><span class="bash"> [<span class="hljs-string">"/data"</span>]</span></code></pre><p>VOLUME指令创建具有指定名称的挂载点，并将其标记为保存来自本地主机或其他容器的外部安装的卷。该值可以是JSON数组，<code>VOLUME [&quot;/var/log/&quot;]</code> 或具有多个参数的纯字符串，例如<code>VOLUME /var/log</code> 或 <code>VOLUME /var/log/var/db</code>。在指定挂载点后，docker创建容器时，会把挂载点下已经存在的文件移动到卷中。</p><p>关于Dockerfile中的卷，请记住以下几点。</p><ol><li><p>基于Windows的容器上的卷：使用基于Windows的容器时，容器内的卷的目的地必须是以下之一：</p><p>a、不存在的或空目录</p><p>b、C盘以外的磁盘分区</p></li><li><p>从Dockerfile内更改卷：如果在声明了卷后有任何构建步骤更改了卷内的数据，则这些更改将被丢弃;</p></li><li><p>JSON格式：列表被解析为JSON数组。您必须用双引号（”）而不是单引号（’）括起单词;</p></li><li><p>主机目录在容器运行时声明：主机目录（挂载点）从本质上说是依赖于主机的。这是为了保留镜像的可移植性，因为不能保证给定的主机目录在所有主机上都可用。因此，您无法从Dockerfile中挂载主机目录。VOLUME指令不支持指定host-dir参数。创建或运行容器时，必须指定挂载点。</p></li></ol><h2 id="11-USER指令"><a href="#11-USER指令" class="headerlink" title="11. USER指令"></a>11. USER指令</h2><p>USER指令设置运行镜像时要使用的用户名（或UID）以及可选的用户组（或GID），以及Dockerfile中的所有RUN，CMD和ENTRYPOINT指令。其格式如下：</p><pre><code class="hljs routeros">USER &lt;user&gt;[:&lt;group&gt;] <span class="hljs-keyword">or</span>USER &lt;UID&gt;[:&lt;GID&gt;]</code></pre><h2 id="12、WORKDIR指令"><a href="#12、WORKDIR指令" class="headerlink" title="12、WORKDIR指令"></a>12、WORKDIR指令</h2><p>WORKDIR指令为Dockerfile中跟在其后的所有RUN，CMD，ENTRYPOINT，COPY和ADD指令设置工作目录。如果WORKDIR不存在，即使以后的Dockerfile指令中未使用，它也将被创建。其格式如下：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">WORKDIR</span><span class="bash"> /path/to/workdir</span></code></pre><p>WORKDIR指令可在Dockerfile中多次使用。如果提供了相对路径，则它将相对于上一个WORKDIR指令的路径。例如：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">WORKDIR</span><span class="bash"> /a</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> b</span><span class="hljs-keyword">WORKDIR</span><span class="bash"> c</span><span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">pwd</span></span></code></pre><p>该Dockerfile中最后一个pwd命令的输出为<code>/a/b/c</code>。</p><p>WORKDIR指令可以解析以前使用ENV设置的环境变量。你只能使用在Dockerfile中显式设置的环境变量。例如：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">ENV</span> DIRPATH /path<span class="hljs-keyword">WORKDIR</span><span class="bash"> <span class="hljs-variable">$DIRPATH</span>/<span class="hljs-variable">$DIRNAME</span></span><span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">pwd</span></span></code></pre><p>pwd命令的运行结果就是<code>/path/$DIRNAME</code>。</p><h2 id="13-ARG指令"><a href="#13-ARG指令" class="headerlink" title="13. ARG指令"></a>13. ARG指令</h2><p>ARG指令定义了一个变量，用户可以在创建镜像时使用–build-arg=参数将其传递给构建器。如果用户指定了未在Dockerfile中定义的ARG变量，则构建会输出警告。其格式如下：</p><pre><code class="hljs fortran">ARG &lt;<span class="hljs-keyword">name</span>&gt;[=&lt;<span class="hljs-keyword">default</span> <span class="hljs-keyword">value</span>&gt;]</code></pre><p>在Dockerfile中可以包含一个或多个变量。</p><blockquote><p>❝</p><p><strong>「注意:」</strong> 不建议使用创建镜像时使用变量来传递诸如github密钥，用户凭据等机密。创建镜像时变量值对于使用docker history命令的镜像的任何用户都是可见的。</p><p>❞</p></blockquote><p>在定义ARG变量时，可以给变量赋初值，如果在创建镜像时没有传入变量值，那么就会使用这个初始值：</p><pre><code class="hljs routeros"><span class="hljs-keyword">FROM</span> busyboxARG <span class="hljs-attribute">user1</span>=someuserARG <span class="hljs-attribute">buildno</span>=1<span class="hljs-built_in">..</span>.</code></pre><p>ARG变量也遵从先定义后使用的惯例，而且，Dockerfile中后定义的同名变量会覆盖之前的变量的值。</p><p>可以使用ARG或ENV指令来指定RUN指令可用的变量。使用ENV指令定义的环境变量始终会覆盖同名的ARG指令。我们来看一个例子：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">FROM</span> ubuntu<span class="hljs-keyword">ARG</span> CONT_IMG_VER<span class="hljs-keyword">ENV</span> CONT_IMG_VER v1.<span class="hljs-number">0.0</span><span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-variable">$CONT_IMG_VER</span></span></code></pre><p>我们创建镜像时使用如下命令，给<code>CONT_IMG_VER</code>传入不同的变量值：</p><pre><code class="hljs angelscript">$ docker build --build-arg CONT_IMG_VER=v2<span class="hljs-number">.0</span><span class="hljs-number">.1</span> .</code></pre><p>在这种情况下，RUN指令使用v1.0.0而不是用户传递的ARG设置：v2.0.1，就是因为ENV指令定义的环境变量覆盖了同名的ARG变量。</p><p>Docker具有一组预定义的ARG变量，您可以在Dockerfile中使用它们而无需相应的ARG指令:</p><pre><code class="hljs ebnf"><span class="hljs-attribute">HTTP_PROXY</span><span class="hljs-attribute">http_proxy</span><span class="hljs-attribute">HTTPS_PROXY</span><span class="hljs-attribute">https_proxy</span><span class="hljs-attribute">FTP_PROXY</span><span class="hljs-attribute">ftp_proxy</span><span class="hljs-attribute">NO_PROXY</span><span class="hljs-attribute">no_proxy</span></code></pre><p>默认情况下，这些预定义变量从Docker历史记录的输出中删除。删除它们可以降低意外泄漏<code>HTTP_PROXY</code>变量中的敏感身份验证信息的风险。如果需要在docker历史记录中输出这些默认变量值，则需要我们在Dockerfile中显示的使用ARG指令指定这个变量。</p><h2 id="14-ONBUILD指令"><a href="#14-ONBUILD指令" class="headerlink" title="14. ONBUILD指令"></a>14. ONBUILD指令</h2><p>当我们的镜像被作为基础镜像执行构建时，此时ONBUILD指令就会生效。其格式如下：</p><pre><code class="hljs apache"><span class="hljs-attribute">ONBUILD</span><span class="hljs-meta"> [INSTRUCTION]</span></code></pre><p>运作方式如下：当它遇到<code>ONBUILD</code>指令时，构建器将触发器添加到正在构建的镜像的元数据中，该指令不会影响当前版本。构建结束时，所有触发器的列表都存储在镜像清单中的OnBuild键下。可以使用<code>docker inspect</code>命令查看它们。稍后，可以使用FROM指令将该镜像用作新构建的基础镜像，作为处理FROM指令的一部分，下游构建器将查找ONBUILD触发器，并以与注册时相同的顺序执行它们。如果任何触发器失败，那么FROM指令将中止，从而导致构建失败。如果所有触发器都成功，则FROM指令完成，并且构建照常继续。执行完触发器后，将从最终镜像中清除触发器。换句话说，它们不是<code>孙子代</code>版本所继承的。</p><blockquote><p>❝</p><p><strong>「注意」</strong>：在ONBUILD指令中再指定ONBUILD指令是不允许的，ONBUILD指令可能不会触发FROM或者MAINTAINER指令</p><p>❞</p></blockquote><h2 id="15-STOPSIGNAL指令"><a href="#15-STOPSIGNAL指令" class="headerlink" title="15. STOPSIGNAL指令"></a>15. STOPSIGNAL指令</h2><p><code>STOPSIGNAL</code>指令用来设置系统发送给容器的退出信号，该信号可以是内核syscall表中对应的无符号数字，例如9，也可以是SIGNAME格式的信号名称，例如SIGKILL。</p><pre><code class="hljs qml">STOPSIGNAL <span class="hljs-keyword">signal</span><span class="hljs-string"></span></code></pre><h2 id="16-HEALTHCHECK指令"><a href="#16-HEALTHCHECK指令" class="headerlink" title="16. HEALTHCHECK指令"></a>16. HEALTHCHECK指令</h2><p>HEALTHCHECK指令是用来做容器健康检查的，这个指令是在Docker 1.12版本被加入的，在早期版本中并不支持，这个指令可以让我们自定义容器健康状态检查的脚本或者命令。其格式如下：</p><pre><code class="hljs routeros">HEALTHCHECK [OPTIONS] CMD command (check container<span class="hljs-built_in"> health </span>by running a command inside the container)HEALTHCHECK NONE (<span class="hljs-builtin-name">disable</span> any healthcheck inherited <span class="hljs-keyword">from</span> the base image)</code></pre><p>我们为什么需要这样一个指令呢？是因为我们的容器是根据启动命令是否运行来判断容器是否健康的，这就导致一个问题，有时我们的应用程序确实在运行，进程并没有退出，但是此时由于bug或其他原因导致程序已经无法正常对外提供服务，那么此时我们就需要用一个命令或者脚本来检测我们的服务，这就是这个指令存在的意义。</p><p><strong>「HEALTHCHECK指令的OPTIONS字段可以有如下几个选项：」</strong></p><ol><li><code>--interval=DURATION (default: 30s)</code> 健康检测的命令将在容器启动后的DURATION秒后开始第一次运行，然后每隔DURATION秒运行一次，DURATION默认值是30秒;</li><li><code>--timeout=DURATION (default: 30s)</code> 健康检测的命令的超时时间，默认30秒;</li><li><code>--start-period=DURATION (default: 0s)</code> 此选项设置了当容器启动后的DURATION秒后的健康检测如果失败，不计入重试次数，这是为了给容器一个初始化的时间。但是如果这段时间中一旦健康检测为正常，则之后即使在初始化时间内，健康检测如果失败，此时会计入重试次数，默认是0秒;</li><li><code>--retries=N (default: 3)</code> 健康检测的重试次数，重试N次后容器被判断为异常，则退出进程。</li></ol><blockquote><p>❝</p><p><strong>「注意」</strong>：在一个Dockerfile中只能有一个HEALTHCHECK指令，如果指定了多个指令，则只有最后一个指令生效。</p><p>❞</p></blockquote><p>CMD关键字之后的命令可以是shell命令（例如<code>HEALTHCHECK CMD /bin/check-running</code>）或exec数组（与其他Dockerfile命令一样;有关详细信息，请参见ENTRYPOINT）。</p><p><strong>「命令的退出状态指示容器的健康状态。可能的值为：」</strong></p><pre><code class="hljs angelscript"><span class="hljs-number">0</span>：success-容器健康且可以使用<span class="hljs-number">1</span>：unhealthy-容器无法正常工作<span class="hljs-number">2</span>：reserved-请勿使用此退出码</code></pre><p>为了调试方便，健康检测的输出会被记录到健康状态内，我们可以通过docker inspect命令去查询，但是当前最多只能存储输出的前4096个字节，所以，健康检测的命令要尽可能简洁。</p><h2 id="17-SHELL指令"><a href="#17-SHELL指令" class="headerlink" title="17. SHELL指令"></a>17. SHELL指令</h2><p>SHELL指令允许覆盖用于命令的shell形式的默认shell。在Linux上，默认shell程序是<code>[&quot;/bin/sh&quot;,&quot;-c&quot;]</code>，在Windows上，默认shell程序是<code>[&quot;cmd&quot;,&quot;/S&quot;,&quot;/C&quot;]</code>。SHELL指令必须使用JSON形式编写。格式如下：</p><pre><code class="hljs dockerfile"><span class="hljs-keyword">SHELL</span><span class="bash"> [<span class="hljs-string">"executable"</span>, <span class="hljs-string">"parameters"</span>]</span></code></pre><p>SHELL指令可以有多个，每个SHELL指令都会覆盖之前的设置，并且影响其之后的指令。SHELL指令也是在Docker 1.12版本中加入的，所以在更早期的版本中是不支持的。</p><h2 id="18-注意"><a href="#18-注意" class="headerlink" title="18. 注意"></a>18. 注意</h2><p><strong>「很重要:」</strong></p><p>在我们编写Dockerfile时，每一行指令就会生成一个镜像的层，所以，我们应该尽量将相同的操作都写在同一行中，而且我们依然可以使用<code>\</code>来换行，这还是会被当成一层来处理。这样做的好处是可以减小我们的镜像文件的大小，加快容器创建的速度。</p><h1 id="三、构建镜像"><a href="#三、构建镜像" class="headerlink" title="三、构建镜像"></a>三、构建镜像</h1><p>当我们写好了Dockerfile之后，我们就可以使用docker build命令来构建镜像了。命令如下：</p><pre><code class="hljs gherkin">docker build [OPTIONS] PATH |<span class="hljs-string"> URL </span>|<span class="hljs-string"> -</span></code></pre><p>在构建镜像时，我们可以添加各种参数来定制镜像，还可以直接为镜像打好标签。docker build命令支持的参数详见docker build命令官方文档，在此不再赘述。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Linux内核调优 </title>
    <link href="/2020/07/20/2020-07-20-linux-kernel-optimize/"/>
    <url>/2020/07/20/2020-07-20-linux-kernel-optimize/</url>
    
    <content type="html"><![CDATA[<p>以nginx 10k并发连接为优化目标，附简单介绍，不一一解释。</p><h3 id="tcp容量规划"><a href="#tcp容量规划" class="headerlink" title="tcp容量规划"></a>tcp容量规划</h3><pre><code class="hljs ini"><span class="hljs-attr">net.ipv4.tcp_mem</span>  = <span class="hljs-number">262144</span>  <span class="hljs-number">524288</span> <span class="hljs-number">786432</span><span class="hljs-attr">net.core.wmem_max</span> = <span class="hljs-number">16777216</span><span class="hljs-attr">net.core.wmem_default</span> = <span class="hljs-number">131072</span><span class="hljs-attr">net.core.rmem_max</span> = <span class="hljs-number">16777216</span><span class="hljs-attr">net.core.rmem_default</span> = <span class="hljs-number">131072</span><span class="hljs-attr">net.ipv4.tcp_wmem</span> = <span class="hljs-number">4096</span>    <span class="hljs-number">131072</span>  <span class="hljs-number">16777216</span><span class="hljs-attr">net.ipv4.tcp_rmem</span> = <span class="hljs-number">4096</span>    <span class="hljs-number">131072</span>  <span class="hljs-number">16777216</span></code></pre><p>*<em>net.ipv4.tcp_mem *</em> 单位是内存页，一般是4k，三个值分别代表tcp内存使用的水平，低、中、高， 低表示无内存压力，中级表示内存压力状态，高表示内存吃紧，最高峰时系统将会拒绝分配内存。262144 代表1G内存，即（262144x4/1024/1024），其他类推。</p><p>下面的参数单位都是字节 net.core.wmem_max 和net.core.wmem_default 会覆盖net.ipv4.tcp_wmem 的第二第三个值， 同理，net.core.rmem_max 和 net.core.rmem_default 会覆盖net.ipv4.tcp_rmem 的第二第三个值。稍微提高tcp读写缓冲区的容量，可以增加tcp传输效率，比如上文默认值131072=128k，现有一个1M的文件传输，只需8次传输即可，比较适合图片类传输。但也不是越大越好，比如一个文字页面只有15k，使用128k的内存显然有些浪费。上文tcp压力状态下的容量为2G，对应tcp读写缓冲区128k，可应对的连接数为16384 (2048x1024/128)，可满足10k要求。</p><h3 id="tcp连接行为管理"><a href="#tcp连接行为管理" class="headerlink" title="tcp连接行为管理"></a>tcp连接行为管理</h3><pre><code class="hljs ini"><span class="hljs-attr">net.ipv4.tcp_tw_reuse</span> = <span class="hljs-number">1</span><span class="hljs-attr">net.ipv4.tcp_tw_recycle</span> = <span class="hljs-number">1</span><span class="hljs-attr">net.ipv4.tcp_timestamps</span> = <span class="hljs-number">1</span><span class="hljs-attr">net.ipv4.tcp_fin_timeout</span> = <span class="hljs-number">30</span><span class="hljs-attr">net.ipv4.tcp_max_tw_buckets</span> = <span class="hljs-number">8192</span><span class="hljs-attr">net.ipv4.tcp_retries1</span> = <span class="hljs-number">3</span><span class="hljs-attr">net.ipv4.tcp_retries2</span> = <span class="hljs-number">5</span><span class="hljs-attr">net.ipv4.tcp_keepalive_time</span> = <span class="hljs-number">1800</span><span class="hljs-attr">net.ipv4.tcp_keepalive_probes</span> = <span class="hljs-number">5</span><span class="hljs-attr">net.ipv4.tcp_keepalive_intvl</span> = <span class="hljs-number">30</span><span class="hljs-attr">net.ipv4.tcp_max_syn_backlog</span> = <span class="hljs-number">8192</span><span class="hljs-attr">net.ipv4.tcp_max_orphans</span> = <span class="hljs-number">262144</span></code></pre><p>上面主要是tcp连接行为的伴随的参数，主要是tcp重用，增加队列，减少等待重试频率等等来提升效率。</p><h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><pre><code class="hljs ini"><span class="hljs-attr">vm.swappiness</span> = <span class="hljs-number">5</span><span class="hljs-attr">vm.dirty_ratio</span> = <span class="hljs-number">40</span><span class="hljs-attr">vm.min_free_kbytes</span> = <span class="hljs-number">524288</span><span class="hljs-attr">vm.vfs_cache_pressure</span> = <span class="hljs-number">100</span></code></pre><ul><li>vm.swappiness = 5 表示物理内存剩余5%时，才考虑使用swap，默认60，这显然非常不合理</li><li>•vm.dirty_ratio = 40 表示拿出物理内存的40%用于写缓存，而不立即将数据写入硬盘。由于硬盘是众所周知的瓶颈，扩大它可提升写的效率，40%是个比较合适的比例。</li><li>vm.min_free_kbytes = 524288 这个用于控制剩余内存的大小，524288=512M，可根据需要调整。如果某些任务临时需要大量内存，可临时将它调大然后调小，回收页面缓存。它比vm.drop_caches 要温和得多，后者更粗暴。</li><li>vm.vfs_cache_pressure = 100 ，如果要尽快将脏数据刷进硬盘，提高它，比如150 。</li></ul><h3 id="内核其他行为"><a href="#内核其他行为" class="headerlink" title="内核其他行为"></a>内核其他行为</h3><pre><code class="hljs stylus">net<span class="hljs-selector-class">.core</span><span class="hljs-selector-class">.somaxconn</span> = <span class="hljs-number">8192</span>net<span class="hljs-selector-class">.core</span><span class="hljs-selector-class">.netdev_max_backlog</span> = <span class="hljs-number">8192</span>net<span class="hljs-selector-class">.ipv4</span><span class="hljs-selector-class">.ip_local_port_range</span> = <span class="hljs-number">15000</span> <span class="hljs-number">65000</span>net<span class="hljs-selector-class">.netfilter</span><span class="hljs-selector-class">.nf_conntrack_max</span> = <span class="hljs-number">131072</span>net<span class="hljs-selector-class">.nf_conntrack_max</span> = <span class="hljs-number">131072</span>net<span class="hljs-selector-class">.ipv6</span><span class="hljs-selector-class">.conf</span><span class="hljs-selector-class">.all</span><span class="hljs-selector-class">.disable_ipv6</span> =<span class="hljs-number">1</span>net<span class="hljs-selector-class">.netfilter</span><span class="hljs-selector-class">.nf_conntrack_tcp_timeout_established</span> =<span class="hljs-number">3600</span>net<span class="hljs-selector-class">.core</span><span class="hljs-selector-class">.rps_sock_flow_entries</span> = <span class="hljs-number">32768</span></code></pre><p>net.core.somaxconn 表示socket的最大连接数，默认128，对于php-fpm使用unix socket情况下，需要调大。</p><p>net.netfilter.nf_conntrack_tcp_timeout_established = 3600 默认2天时间，多数情况下，调小这个参数是有益的，如果是tcp长连接，这个参数可能不太合适。</p><p>net.core.rps_sock_flow_entries 这个参数启用RPS，自动将网卡中断均匀分配到多个CPU，改进网卡性能和系统负载。</p><p>RPS还需要脚本配合</p><pre><code class="hljs reasonml">for fileRfc <span class="hljs-keyword">in</span> <span class="hljs-constructor">$(<span class="hljs-params">ls</span> <span class="hljs-operator">/</span><span class="hljs-params">sys</span><span class="hljs-operator">/</span><span class="hljs-params">class</span><span class="hljs-operator">/</span><span class="hljs-params">net</span><span class="hljs-operator">/</span><span class="hljs-params">eth</span><span class="hljs-operator">*</span><span class="hljs-operator">/</span><span class="hljs-params">queues</span><span class="hljs-operator">/</span><span class="hljs-params">rx</span>-<span class="hljs-operator">*</span><span class="hljs-operator">/</span><span class="hljs-params">rps_flow_cnt</span>)</span>;<span class="hljs-keyword">do</span> echo <span class="hljs-number">2048</span> &gt; $fileRfc;<span class="hljs-keyword">done</span></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Nginx日志配置详解 </title>
    <link href="/2020/07/20/2020-07-20-nginx-logs/"/>
    <url>/2020/07/20/2020-07-20-nginx-logs/</url>
    
    <content type="html"><![CDATA[<p><strong>前言</strong></p><p>Nginx日志对于统计、系统服务排错很有用。</p><p><strong>Nginx日志主要分为两种：</strong>access_log(访问日志)和error_log(错误日志)。通过访问日志我们可以得到用户的IP地址、浏览器的信息，请求的处理时间等信息。错误日志记录了访问出错的信息，可以帮助我们定位错误的原因。</p><p><strong>本文将详细描述一下如何配置Nginx日志。</strong></p><h2 id="设置access-log"><a href="#设置access-log" class="headerlink" title="设置access_log"></a><strong>设置access_log</strong></h2><p>访问日志主要记录客户端的请求。客户端向Nginx服务器发起的每一次请求都记录在这里。客户端IP，浏览器信息，referer，请求处理时间，请求URL等都可以在访问日志中得到。当然具体要记录哪些信息，你可以通过log_format指令定义。</p><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><pre><code class="hljs inform7">access_log path <span class="hljs-comment">[format <span class="hljs-comment">[buffer=size]</span> <span class="hljs-comment">[gzip<span class="hljs-comment">[=level]</span>]</span> <span class="hljs-comment">[flush=time]</span> <span class="hljs-comment">[if=condition]</span>]</span>; # 设置访问日志access_log off; # 关闭访问日志</code></pre><blockquote><ul><li>path 指定日志的存放位置。</li><li>format 指定日志的格式。默认使用预定义的combined。</li><li>buffer 用来指定日志写入时的缓存大小。默认是64k。</li><li>gzip 日志写入前先进行压缩。压缩率可以指定，从1到9数值越大压缩比越高，同时压缩的速度也越慢。默认是1。</li><li>flush 设置缓存的有效时间。如果超过flush指定的时间，缓存中的内容将被清空。</li><li>if 条件判断。如果指定的条件计算为0或空字符串，那么该请求不会写入日志。</li></ul></blockquote><p>另外，还有一个特殊的值off。如果指定了该值，当前作用域下的所有的请求日志都被关闭。</p><h3 id="作用域"><a href="#作用域" class="headerlink" title="作用域"></a>作用域</h3><p>可以应用access_log指令的作用域分别有http，server，location，limit_except。也就是说，在这几个作用域外使用该指令，Nginx会报错。</p><p>以上是access_log指令的基本语法和参数的含义。下面我们看一几个例子加深一下理解。</p><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><pre><code class="hljs fortran">access_log /var/logs/nginx-<span class="hljs-keyword">access</span>.<span class="hljs-built_in">log</span></code></pre><p>该例子指定日志的写入路径为/var/logs/nginx-access.log，日志格式使用默认的combined。</p><pre><code class="hljs routeros">access_log /var/logs/nginx-access.log <span class="hljs-attribute">buffer</span>=32k gzip <span class="hljs-attribute">flush</span>=1m</code></pre><p>该例子指定日志的写入路径为/var/logs/nginx-access.log，日志格式使用默认的combined，指定日志的缓存大小为32k，日志写入前启用gzip进行压缩，压缩比使用默认值1，缓存数据有效时间为1分钟。</p><h2 id="使用log-format自定义日志格式"><a href="#使用log-format自定义日志格式" class="headerlink" title="使用log_format自定义日志格式"></a><strong>使用log_format自定义日志格式</strong></h2><p>Nginx预定义了名为combined日志格式，如果没有明确指定日志格式默认使用该格式：</p><pre><code class="hljs nginx"><span class="hljs-attribute">log_format</span> combined <span class="hljs-string">'<span class="hljs-variable">$remote_addr</span> - <span class="hljs-variable">$remote_user</span> [<span class="hljs-variable">$time_local</span>] '</span>                    <span class="hljs-string">'"<span class="hljs-variable">$request</span>" <span class="hljs-variable">$status</span> <span class="hljs-variable">$body_bytes_sent</span> '</span>                    <span class="hljs-string">'"<span class="hljs-variable">$http_referer</span>" "<span class="hljs-variable">$http_user_agent</span>"'</span>;</code></pre><p>如果不想使用Nginx预定义的格式，可以通过log_format指令来自定义。</p><h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><pre><code class="hljs qml">log_format name [<span class="hljs-built_in">escape</span>=<span class="hljs-keyword">default</span>|json] <span class="hljs-built_in">string</span> ...;</code></pre><blockquote><ul><li>name 格式名称。在access_log指令中引用。</li><li>escape 设置变量中的字符编码方式是json还是default，默认是default。</li><li>string 要定义的日志格式内容。该参数可以有多个。参数中可以使用Nginx变量。</li></ul></blockquote><p>下面是log_format指令中常用的一些变量：</p><pre><code class="hljs bash"><span class="hljs-variable">$bytes_sent</span>发送给客户端的总字节数<span class="hljs-variable">$body_bytes_sent</span>发送给客户端的字节数，不包括响应头的大小<span class="hljs-variable">$connection</span>连接序列号<span class="hljs-variable">$connection_requests</span>当前通过连接发出的请求数量<span class="hljs-variable">$msec</span>日志写入时间，单位为秒，精度是毫秒<span class="hljs-variable">$pipe</span>如果请求是通过http流水线发送，则其值为<span class="hljs-string">"p"</span>，否则为“.<span class="hljs-string">"</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$request_length</span></span><span class="hljs-string">请求长度（包括请求行，请求头和请求体）</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$request_time</span></span><span class="hljs-string">请求处理时长，单位为秒，精度为毫秒，从读入客户端的第一个字节开始，直到把最后一个字符发送张客户端进行日志写入为止</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$status</span></span><span class="hljs-string">响应状态码</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$time_iso8601</span></span><span class="hljs-string">标准格式的本地时间,形如“2017-05-24T18:31:27+08:00”</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$time_local</span></span><span class="hljs-string">通用日志格式下的本地时间，如"</span>24/May/2017:18:31:27 +0800<span class="hljs-string">"</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$http_referer</span></span><span class="hljs-string">请求的referer地址。</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$http_user_agent</span></span><span class="hljs-string">客户端浏览器信息。</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$remote_addr</span></span><span class="hljs-string">客户端IP</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$http_x_forwarded_for</span></span><span class="hljs-string">当前端有代理服务器时，设置web节点记录客户端地址的配置，此参数生效的前提是代理服务器也要进行相关的x_forwarded_for设置。</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$request</span></span><span class="hljs-string">完整的原始请求行，如 "</span>GET / HTTP/1.1<span class="hljs-string">"</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$remote_user</span></span><span class="hljs-string">客户端用户名称，针对启用了用户认证的请求</span><span class="hljs-string"></span><span class="hljs-string"><span class="hljs-variable">$request_uri</span></span><span class="hljs-string">完整的请求地址，如 "</span>https://daojia.com/<span class="hljs-string">"</span></code></pre><p>下面演示一下自定义日志格式的使用：</p><pre><code class="hljs dart">access_log /<span class="hljs-keyword">var</span>/logs/nginx-access.log mainlog_format  main  <span class="hljs-string">'<span class="hljs-subst">$remote_addr</span> - <span class="hljs-subst">$remote_user</span> [<span class="hljs-subst">$time_local</span>] "<span class="hljs-subst">$request</span>" '</span>                  <span class="hljs-string">'<span class="hljs-subst">$status</span> <span class="hljs-subst">$body_bytes_sent</span> "<span class="hljs-subst">$http_referer</span>" '</span>                  <span class="hljs-string">'"<span class="hljs-subst">$http_user_agent</span>" "<span class="hljs-subst">$http_x_forwarded_for</span>"'</span>;</code></pre><p>我们使用log_format指令定义了一个main的格式，并在access_log指令中引用了它。假如客户端有发起请求：<a href="https://suyunfe.com/，我们看一下我截取的一个请求的日志记录" target="_blank" rel="noopener">https://suyunfe.com/，我们看一下我截取的一个请求的日志记录</a>:</p><pre><code class="hljs 1c"><span class="hljs-number">112.195</span>.<span class="hljs-number">209.90</span> - - [<span class="hljs-number">20</span>/Feb/<span class="hljs-number">2018</span>:<span class="hljs-number">12</span>:<span class="hljs-number">12</span>:<span class="hljs-number">14</span> +<span class="hljs-number">0800</span>] <span class="hljs-string">"GET / HTTP/1.1"</span> <span class="hljs-number">200</span> <span class="hljs-number">190</span> <span class="hljs-string">"-"</span> <span class="hljs-string">"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) </span>AppleWebKit/<span class="hljs-number">537.36</span> (KHTML, like Gecko) Chrome/<span class="hljs-number">63.0</span>.<span class="hljs-number">3239.132</span> Mobile Safari/<span class="hljs-number">537.36</span><span class="hljs-string">" "</span>-<span class="hljs-string">"</span></code></pre><p>我们看到最终的日志记录中<code>$remote_user</code>、<code>$http_referer</code>、<code>$http_x_forwarded_for</code>都对应了一个<code>-</code>，这是因为这几个变量为空。</p><h2 id="设置error-log"><a href="#设置error-log" class="headerlink" title="设置error_log"></a><strong>设置error_log</strong></h2><p>错误日志在Nginx中是通过error_log指令实现的。该指令记录服务器和请求处理过程中的错误信息。</p><h3 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h3><p>配置错误日志文件的路径和日志级别。</p><pre><code class="hljs applescript">error_log <span class="hljs-built_in">file</span> [level];Default:    error_log logs/<span class="hljs-keyword">error</span>.<span class="hljs-built_in">log</span> <span class="hljs-keyword">error</span>;</code></pre><p>第一个参数指定日志的写入位置。</p><p>第二个参数指定日志的级别。level可以是debug, info, notice, warn, error, crit, alert,emerg中的任意值。可以看到其取值范围是按紧急程度从低到高排列的。只有日志的错误级别等于或高于level指定的值才会写入错误日志中。默认值是error。</p><h3 id="基本用法-1"><a href="#基本用法-1" class="headerlink" title="基本用法"></a>基本用法</h3><pre><code class="hljs maxima">error_log /<span class="hljs-built_in">var</span>/logs/nginx/nginx-<span class="hljs-built_in">error</span>.<span class="hljs-built_in">log</span></code></pre><p>它可以配置在：main， http, mail, stream, server, location作用域。</p><p>例子中指定了错误日志的路径为：<code>/var/logs/nginx/nginx-error.log</code>，日志级别使用默认的error。</p><h2 id="open-log-file-cache"><a href="#open-log-file-cache" class="headerlink" title="open_log_file_cache"></a><strong>open_log_file_cache</strong></h2><p>每一条日志记录的写入都是先打开文件再写入记录，然后关闭日志文件。如果你的日志文件路径中使用了变量，如<code>access_log /var/logs/$host/nginx-access.log</code>，为提高性能，可以使用open_log_file_cache指令设置日志文件描述符的缓存。</p><h3 id="语法-3"><a href="#语法-3" class="headerlink" title="语法"></a>语法</h3><pre><code class="hljs routeros">open_log_file_cache <span class="hljs-attribute">max</span>=N [<span class="hljs-attribute">inactive</span>=time] [<span class="hljs-attribute">min_uses</span>=N] [<span class="hljs-attribute">valid</span>=time];</code></pre><blockquote><ul><li>max 设置缓存中最多容纳的文件描述符数量，如果被占满，采用LRU算法将描述符关闭。</li><li>inactive 设置缓存存活时间，默认是10s。</li><li>min_uses 在inactive时间段内，日志文件最少使用几次，该日志文件描述符记入缓存，默认是1次。</li><li>valid：设置多久对日志文件名进行检查，看是否发生变化，默认是60s。</li><li>off：不使用缓存。默认为off。</li></ul></blockquote><h3 id="基本用法-2"><a href="#基本用法-2" class="headerlink" title="基本用法"></a>基本用法</h3><pre><code class="hljs routeros">open_log_file_cache <span class="hljs-attribute">max</span>=1000 <span class="hljs-attribute">inactive</span>=20s <span class="hljs-attribute">valid</span>=1m <span class="hljs-attribute">min_uses</span>=2;</code></pre><p>它可以配置在http、server、location作用域中。</p><p>例子中，设置缓存最多缓存1000个日志文件描述符，20s内如果缓存中的日志文件描述符至少被被访问2次，才不会被缓存关闭。每隔1分钟检查缓存中的文件描述符的文件名是否还存在。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>Nginx中通过access_log和error_log指令配置访问日志和错误日志，通过log_format我们可以自定义日志格式。如果日志文件路径中使用了变量，我们可以通过open_log_file_cache指令来设置缓存，提升性能。</p><p>另外，在access_log和log_format中使用了很多变量，这些变量没有一一列举出来，详细的变量信息可以参考Nginx官方文档：<a href="http://nginx.org/en/docs/varindex.html" target="_blank" rel="noopener">http://nginx.org/en/docs/varindex.html</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 24个Jvm面试题总结 </title>
    <link href="/2020/07/07/2020-07-07-24-jvm/"/>
    <url>/2020/07/07/2020-07-07-24-jvm/</url>
    
    <content type="html"><![CDATA[<h3 id="1、JVN内存结构"><a href="#1、JVN内存结构" class="headerlink" title="1、JVN内存结构"></a>1、JVN内存结构</h3><p> <a href="http://blog.itpub.net/ueditor/php/upload/image/20190822/1566461580220068.jpg" target="_blank" rel="noopener"><img src="http://blog.itpub.net/ueditor/php/upload/image/20190822/1566461580220068.jpg" srcset="/img/loading.gif" alt="img"></a></p><p>方法区和对是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。</p><ul><li>Java堆（Heap）,是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。</li><li>方法区（Method Area）,方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。</li><li>程序计数器（Program Counter Register）,程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。</li><li>JVM栈（JVM Stacks）,与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。</li><li>本地方法栈（Native Method Stacks）,本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。</li></ul><h3 id="2、对象分配规则"><a href="#2、对象分配规则" class="headerlink" title="2、对象分配规则"></a>2、对象分配规则</h3><ul><li>对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。</li><li>大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。</li><li>长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。</li><li>动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。</li><li>空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。</li></ul><h3 id="3、解释内存中的栈-stack-、堆-heap-和静态区-static-area-的用法"><a href="#3、解释内存中的栈-stack-、堆-heap-和静态区-static-area-的用法" class="headerlink" title="3、解释内存中的栈(stack)、堆(heap)和静态区(static area)的用法"></a>3、解释内存中的栈(stack)、堆(heap)和静态区(static area)的用法</h3><p>　　　　通常我们定义一个基本数据类型的变量，一个对象的引用，还有就是函数调用的现场保存都使用内存中的栈空间；而通过new关键字和构造器创建的对象放在堆空间；程序中的字面量（literal）如直接书写的100、”hello”和常量都是放在静态区中。栈空间操作起来最快但是栈很小，通常大量的对象都是放在堆空间，理论上整个内存没有被其他进程使用的空间甚至硬盘上的虚拟内存都可以被当成堆空间来使用。</p><pre><code class="hljs processing">　　　　<span class="hljs-keyword">String</span> <span class="hljs-built_in">str</span> = <span class="hljs-keyword">new</span> <span class="hljs-keyword">String</span>(<span class="hljs-string">"hello"</span>);</code></pre><p>上面的语句中变量str放在栈上，用new创建出来的字符串对象放在堆上，而”hello”这个字面量放在静态区。</p><h3 id="4、Perm-Space中保存什么数据？会引起OutOfMemory吗？"><a href="#4、Perm-Space中保存什么数据？会引起OutOfMemory吗？" class="headerlink" title="4、Perm Space中保存什么数据？会引起OutOfMemory吗？"></a>4、Perm Space中保存什么数据？会引起OutOfMemory吗？</h3><p>　　　　Perm Space中保存的是加载class文件。</p><p>　　　　会引起OutOfMemory，出现异常可以设置 -XX:PermSize 的大小。JDK 1.8后，字符串常量不存放在永久带，而是在堆内存中，JDK8以后没有永久代概念，而是用元空间替代，元空间不存在虚拟机中，二是使用本地内存。</p><h3 id="5、什么是类的加载"><a href="#5、什么是类的加载" class="headerlink" title="5、什么是类的加载"></a>5、什么是类的加载</h3><p>　　　　类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。</p><p>　　　　类加载器</p><p><a href="http://blog.itpub.net/ueditor/php/upload/image/20190822/1566461580890200.jpg" target="_blank" rel="noopener"><img src="http://blog.itpub.net/ueditor/php/upload/image/20190822/1566461580890200.jpg" srcset="/img/loading.gif" alt="img"></a></p><ul><li>启动类加载器：Bootstrap ClassLoader，负责加载存放在JDK\jre\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库</li><li>扩展类加载器：Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\jre\lib\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。</li><li>应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器</li></ul><p>双亲委派机制：类加载器收到类加载请求，自己不加载，向上委托给父类加载，父类加载不了，再自己加载。优势就是避免Java核心API篡改。</p><h3 id="6、如何⾃定义⼀个类加载器？你使⽤过哪些或者你在什么场景下需要⼀个⾃-定义的类加载器吗？"><a href="#6、如何⾃定义⼀个类加载器？你使⽤过哪些或者你在什么场景下需要⼀个⾃-定义的类加载器吗？" class="headerlink" title="6、如何⾃定义⼀个类加载器？你使⽤过哪些或者你在什么场景下需要⼀个⾃ 定义的类加载器吗？"></a>6、如何⾃定义⼀个类加载器？你使⽤过哪些或者你在什么场景下需要⼀个⾃ 定义的类加载器吗？</h3><p>　　自定义类加载的意义：</p><ol><li>加载特定路径的class文件</li><li>加载一个加密的网络class文件</li><li>热部署加载class文件</li></ol><h3 id="7、描述一下JVM加载class文件的原理机制？"><a href="#7、描述一下JVM加载class文件的原理机制？" class="headerlink" title="7、描述一下JVM加载class文件的原理机制？"></a>7、描述一下JVM加载class文件的原理机制？</h3><p>　　　　JVM中类的装载是由类加载器（ClassLoader）和它的子类来实现的，Java中的类加载器是一个重要的Java运行时系统组件，它负责在运行时查找和装入类文件中的类。</p><p>　　　　由于Java的跨平台性，经过编译的Java源程序并不是一个可执行程序，而是一个或多个类文件。当Java程序需要使用某个类时，JVM会确保这个类已经被加载、连接（验证、准备和解析）和初始化。</p><p>　　　　类的加载是指把类的.class文件中的数据读入到内存中，通常是创建一个字节数组读入.class文件，然后产生与所加载类对应的Class对象。加载完成后，Class对象还不完整，所以此时的类还不可用。当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。最后JVM对类进行初始化，包括：1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；2)如果类中存在初始化语句，就依次执行这些初始化语句。类的加载是由类加载器完成的，类加载器包括：根加载器（BootStrap）、扩展加载器（Extension）、系统加载器（System）和用户自定义类加载器（java.lang.ClassLoader的子类）。从Java 2（JDK 1.2）开始，类加载过程采取了父亲委托机制（PDM）。PDM更好的保证了Java平台的安全性，在该机制中，JVM自带的Bootstrap是根加载器，其他的加载器都有且仅有一个父类加载器。类的加载首先请求父类加载器加载，父类加载器无能为力时才由其子类加载器自行加载。JVM不会向Java程序提供对Bootstrap的引用。</p><p>　　　　下面是关于几个类加载器的说明：</p><ul><li><ul><li><ul><li>bootstrap：一般用本地代码实现，负责加载JVM基础核心类库（rt.jar）；</li><li>Extension：从java.ext.dirs系统属性所指定的目录中加载类库，它的父加载器是Bootstrap；</li><li>System：又叫应用类加载器，其父类是Extension。它是应用最广泛的类加载器。它从环境变量classpath或者系统属性java.class.path所指定的目录中记载类，是用户自定义加载器的默认父加载器。</li></ul></li></ul></li></ul><p><strong><em>0\</em></strong>|<strong><em>1**</em></strong>8、Java对象创建过程**</p><ol><li>JVM遇到一条新建对象的指令时首先去检查这个指令的参数是否能在常量池中定义到一个类的符号引用。然后加载这个类（类加载过程在后边讲）</li><li>为对象分配内存。一种办法“指针碰撞”、一种办法“空闲列表”，最终常用的办法“本地线程缓冲分配(TLAB)”</li><li>将除对象头外的对象内存空间初始化为0</li><li>对对象头进行必要设置</li></ol><h3 id="9、类的生命周期"><a href="#9、类的生命周期" class="headerlink" title="9、类的生命周期"></a>9、类的生命周期</h3><p>　　　　类的生命周期包括这几个部分，加载、连接、初始化、使用和卸载，其中前三部是类的加载的过程,如下图:</p><p><a href="http://blog.itpub.net/ueditor/php/upload/image/20190822/1566461580839511.jpg" target="_blank" rel="noopener"><img src="http://blog.itpub.net/ueditor/php/upload/image/20190822/1566461580839511.jpg" srcset="/img/loading.gif" alt="img"></a></p><ul><li>加载，查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象</li><li>连接，连接又包含三块内容：验证、准备、初始化。 1）验证，文件格式、元数据、字节码、符号引用验证； 2）准备，为类的静态变量分配内存，并将其初始化为默认值； 3）解析，把类中的符号引用转换为直接引用</li><li>初始化，为类的静态变量赋予正确的初始值</li><li>使用，new出对象程序中使用</li><li>卸载，执行垃圾回收</li></ul><h3 id="10、Java-中会存在内存泄漏吗，请简单描述。"><a href="#10、Java-中会存在内存泄漏吗，请简单描述。" class="headerlink" title="10、Java 中会存在内存泄漏吗，请简单描述。"></a>10、Java 中会存在内存泄漏吗，请简单描述。</h3><p>　　　　理论上Java因为有垃圾回收机制（GC）不会存在内存泄露问题（这也是Java被广泛使用于服务器端编程的一个重要原因）；然而在实际开发中，可能会存在无用但可达的对象，这些对象不能被GC回收，因此也会导致内存泄露的发生。例如hibernate的Session（一级缓存）中的对象属于持久态，垃圾回收器是不会回收这些对象的，然而这些对象中可能存在无用的垃圾对象，如果不及时关闭（close）或清空（flush）一级缓存就可能导致内存泄露。下面例子中的代码也会导致内存泄露。</p><pre><code class="hljs arduino"><span class="hljs-keyword">import</span> java.util.Arrays;<span class="hljs-keyword">import</span> java.util.EmptyStackException;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MyStack</span>&lt;T&gt; &#123;</span>    <span class="hljs-keyword">private</span> T[] elements;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> <span class="hljs-built_in">size</span> = <span class="hljs-number">0</span>;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> INIT_CAPACITY = <span class="hljs-number">16</span>;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">MyStack</span><span class="hljs-params">()</span> </span>&#123;        elements = (T[]) <span class="hljs-keyword">new</span> Object[INIT_CAPACITY];    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">push</span><span class="hljs-params">(T elem)</span> </span>&#123;        ensureCapacity();        elements[<span class="hljs-built_in">size</span>++] = elem;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> T <span class="hljs-title">pop</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">if</span>(<span class="hljs-built_in">size</span> == <span class="hljs-number">0</span>)            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> EmptyStackException();        <span class="hljs-keyword">return</span> elements[--<span class="hljs-built_in">size</span>];    &#125;    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">ensureCapacity</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">if</span>(elements.length == <span class="hljs-built_in">size</span>) &#123;            elements = Arrays.copyOf(elements, <span class="hljs-number">2</span> * <span class="hljs-built_in">size</span> + <span class="hljs-number">1</span>);        &#125;    &#125;&#125;</code></pre><p>　　　　上面的代码实现了一个栈（先进后出（FILO））结构，乍看之下似乎没有什么明显的问题，它甚至可以通过你编写的各种单元测试。 然而其中的pop方法却存在内存泄露的问题，当我们用pop方法弹出栈中的对象时，该对象不会被当作垃圾回收，即使使用栈的程序不再引用这些对象，因为栈内部维护着对这些对象的过期引用（obsolete reference）。在支持垃圾回收的语言中，内存泄露是很隐蔽的，这种内存泄露其实就是无意识的对象保持。 如果一个对象引用被无意识的保留起来了，那么垃圾回收器不会处理这个对象，也不会处理该对象引用的其他对象，即使这样的对象只有少数几个，也可能会导致很多的对象被排除在垃圾回收之外，从而对性能造成重大影响，极端情况下会引发Disk Paging（物理内存与硬盘的虚拟内存交换数据），甚至造成OutOfMemoryError。</p><h3 id="11、GC是什么？为什么要有GC？"><a href="#11、GC是什么？为什么要有GC？" class="headerlink" title="11、GC是什么？为什么要有GC？"></a>11、GC是什么？为什么要有GC？</h3><p>　　　　GC是垃圾收集的意思，内存处理是编程人员容易出现问题的地方，忘记或者错误的内存回收会导致程序或系统的不稳定甚至崩溃，Java提供的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的，Java语言没有提供释放已分配内存的显示操作方法。 Java程序员不用担心内存管理，因为垃圾收集器会自动进行管理。要请求垃圾收集，可以调用下面的方法之一：System.gc() 或Runtime.getRuntime().gc() ，但JVM可以屏蔽掉显示的垃圾回收调用。 垃圾回收可以有效的防止内存泄露，有效的使用可以使用的内存。垃圾回收器通常是作为一个单独的低优先级的线程运行，不可预知的情况下对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收，程序员不能实时的调用垃圾回收器对某个对象或所有对象进行垃圾回收。 在Java诞生初期，垃圾回收是Java最大的亮点之一，因为服务器端的编程需要有效的防止内存泄露问题，然而时过境迁，如今Java的垃圾回收机制已经成为被诟病的东西。移动智能终端用户通常觉得iOS的系统比Android系统有更好的用户体验，其中一个深层次的原因就在于Android系统中垃圾回收的不可预知性。</p><p>　　　　补充：垃圾回收机制有很多种，包括：分代复制垃圾回收、标记垃圾回收、增量垃圾回收等方式。标准的Java进程既有栈又有堆。栈保存了原始型局部变量，堆保存了要创建的对象。Java平台对堆内存回收和再利用的基本算法被称为标记和清除，但是Java对其进行了改进，采用“分代式垃圾收集”。这种方法会跟Java对象的生命周期将堆内存划分为不同的区域，在垃圾收集过程中，可能会将对象移动到不同区域：</p><ul><li>伊甸园（Eden）：这是对象最初诞生的区域，并且对大多数对象来说，这里是它们唯一存在过的区域。</li><li>幸存者乐园（Survivor）：从伊甸园幸存下来的对象会被挪到这里。</li><li>终身颐养园（Tenured）：这是足够老的幸存对象的归宿。年轻代收集（Minor-GC）过程是不会触及这个地方的。当年轻代收集不能把对象放进终身颐养园时，就会触发一次完全收集（Major-GC），这里可能还会牵扯到压缩，以便为大对象腾出足够的空间。</li></ul><p>与垃圾回收相关的JVM参数：</p><ul><li>-Xms / -Xmx — 堆的初始大小 / 堆的最大大小</li><li>-Xmn — 堆中年轻代的大小</li><li>-XX:-DisableExplicitGC — 让System.gc()不产生任何作用</li><li>-XX:+PrintGCDetails — 打印GC的细节</li><li>-XX:+PrintGCDateStamps — 打印GC操作的时间戳</li><li>-XX:NewSize / XX:MaxNewSize — 设置新生代大小/新生代最大大小</li><li>-XX:NewRatio — 可以设置老生代和新生代的比例</li><li>-XX:PrintTenuringDistribution — 设置每次新生代GC后输出幸存者乐园中对象年龄的分布</li><li>-XX:InitialTenuringThreshold / -XX:MaxTenuringThreshold：设置老年代阀值的初始值和最大值</li><li>-XX:TargetSurvivorRatio：设置幸存区的目标使用率</li></ul><h3 id="12、做GC时，⼀个对象在内存各个Space中被移动的顺序是什么？"><a href="#12、做GC时，⼀个对象在内存各个Space中被移动的顺序是什么？" class="headerlink" title="12、做GC时，⼀个对象在内存各个Space中被移动的顺序是什么？"></a>12、做GC时，⼀个对象在内存各个Space中被移动的顺序是什么？</h3><p>　　　　　　标记清除法，复制算法，标记整理、分代算法。</p><p>　　　　　　新生代一般采用复制算法 GC，老年代使用标记整理算法。</p><p>　　　　　　垃圾收集器：串行新生代收集器、串行老生代收集器、并行新生代收集器、并行老年代收集器。</p><p>　　　　　　CMS（Current Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器，它是一种并发收集器，采用的是Mark-Sweep算法。</p><h3 id="13、你知道哪些垃圾回收算法？"><a href="#13、你知道哪些垃圾回收算法？" class="headerlink" title="13、你知道哪些垃圾回收算法？"></a>13、你知道哪些垃圾回收算法？</h3><p>GC最基础的算法有三种： 标记 -清除算法、复制算法、标记-压缩算法，我们常用的垃圾回收器一般都采用分代收集算法。</p><ul><li>标记-清除算法，“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。</li><li>复制算法，“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。</li><li>标记-压缩算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存</li><li>分代收集算法，“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。</li></ul><h3 id="14、垃圾回收器"><a href="#14、垃圾回收器" class="headerlink" title="14、垃圾回收器"></a>14、垃圾回收器</h3><ul><li>Serial收集器，串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。</li><li>ParNew收集器，ParNew收集器其实就是Serial收集器的多线程版本。</li><li>Parallel收集器，Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。</li><li>Parallel Old 收集器，Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法</li><li>CMS收集器，CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。</li><li>G1收集器，G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征</li></ul><h3 id="15、如何判断一个对象是否应该被回收"><a href="#15、如何判断一个对象是否应该被回收" class="headerlink" title="15、如何判断一个对象是否应该被回收"></a>15、如何判断一个对象是否应该被回收</h3><p>判断对象是否存活一般有两种方式：</p><ul><li>引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。</li><li>可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。</li></ul><h3 id="16、JVM的永久代中会发生垃圾回收么？"><a href="#16、JVM的永久代中会发生垃圾回收么？" class="headerlink" title="16、JVM的永久代中会发生垃圾回收么？"></a>16、JVM的永久代中会发生垃圾回收么？</h3><p>垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。请参考下Java8：从永久代到元数据区 (注：Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区)</p><h3 id="17、引用的分类"><a href="#17、引用的分类" class="headerlink" title="17、引用的分类"></a>17、引用的分类</h3><ul><li>强引用：GC时不会被回收</li><li>软引用：描述有用但不是必须的对象，在发生内存溢出异常之前被回收</li><li>弱引用：描述有用但不是必须的对象，在下一次GC时被回收</li><li>虚引用（幽灵引用/幻影引用）:无法通过虚引用获得对象，用PhantomReference实现虚引用，虚引用用来在GC时返回一个通知。</li></ul><h3 id="18、调优命令"><a href="#18、调优命令" class="headerlink" title="18、调优命令"></a>18、调优命令</h3><p>Sun JDK监控和故障处理命令有jps jstat jmap jhat jstack jinfo</p><ul><li>jps，JVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。</li><li>jstat，JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。</li><li>jmap，JVM Memory Map命令用于生成heap dump文件</li><li>jhat，JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看</li><li>jstack，用于生成java虚拟机当前时刻的线程快照。</li><li>jinfo，JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。</li></ul><h3 id="19、调优工具"><a href="#19、调优工具" class="headerlink" title="19、调优工具"></a>19、调优工具</h3><p>常用调优工具分为两类,jdk自带监控工具：jconsole和jvisualvm，第三方有：MAT(Memory Analyzer Tool)、GChisto。</p><ul><li>jconsole，Java Monitoring and Management Console是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控</li><li>jvisualvm，jdk自带全能工具，可以分析内存快照、线程快照；监控内存变化、GC变化等。</li><li>MAT，Memory Analyzer Tool，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗</li><li>GChisto，一款专业分析gc日志的工具</li></ul><h3 id="20、jstack-是⼲什么的-jstat-呢？如果线上程序周期性地出现卡顿，你怀疑可-能是-GC-导致的，你会怎么来排查这个问题？线程⽇志⼀般你会看其中的什么部分？"><a href="#20、jstack-是⼲什么的-jstat-呢？如果线上程序周期性地出现卡顿，你怀疑可-能是-GC-导致的，你会怎么来排查这个问题？线程⽇志⼀般你会看其中的什么部分？" class="headerlink" title="20、jstack 是⼲什么的? jstat 呢？如果线上程序周期性地出现卡顿，你怀疑可 能是 GC 导致的，你会怎么来排查这个问题？线程⽇志⼀般你会看其中的什么部分？"></a>20、jstack 是⼲什么的? jstat 呢？如果线上程序周期性地出现卡顿，你怀疑可 能是 GC 导致的，你会怎么来排查这个问题？线程⽇志⼀般你会看其中的什么部分？</h3><p>　　　　jstack 用来查询 Java 进程的堆栈信息。</p><p>　　　　jvisualvm 监控内存泄露，跟踪垃圾回收、执行时内存、cpu分析、线程分析。</p><h3 id="21、Minor-GC与Full-GC分别在什么时候发生？"><a href="#21、Minor-GC与Full-GC分别在什么时候发生？" class="headerlink" title="21、Minor GC与Full GC分别在什么时候发生？"></a>21、Minor GC与Full GC分别在什么时候发生？</h3><p>　　　　新生代内存不够用时候发生MGC也叫YGC，JVM内存不够的时候发生FGC</p><h3 id="22、你有没有遇到过OutOfMemory问题？你是怎么来处理这个问题的？处理-过程中有哪些收获？"><a href="#22、你有没有遇到过OutOfMemory问题？你是怎么来处理这个问题的？处理-过程中有哪些收获？" class="headerlink" title="22、你有没有遇到过OutOfMemory问题？你是怎么来处理这个问题的？处理 过程中有哪些收获？"></a>22、你有没有遇到过OutOfMemory问题？你是怎么来处理这个问题的？处理 过程中有哪些收获？</h3><p>　　　　permgen space、heap space 错误。</p><p>　　　　　　常见的原因</p><ul><li><ul><li><ul><li>内存加载的数据量太大：一次性从数据库取太多数据；　　</li><li>集合类中有对对象的引用，使用后未清空，GC不能进行回收；</li><li>代码中存在循环产生过多的重复对象；</li><li>启动参数堆内存值小。</li></ul></li></ul></li></ul><h3 id="23、JDK-1-8之后Perm-Space有哪些变动-MetaSpace⼤⼩默认是⽆限的么-还是你们会通过什么⽅式来指定⼤⼩？"><a href="#23、JDK-1-8之后Perm-Space有哪些变动-MetaSpace⼤⼩默认是⽆限的么-还是你们会通过什么⽅式来指定⼤⼩？" class="headerlink" title="23、JDK 1.8之后Perm Space有哪些变动? MetaSpace⼤⼩默认是⽆限的么? 还是你们会通过什么⽅式来指定⼤⼩？"></a>23、JDK 1.8之后Perm Space有哪些变动? MetaSpace⼤⼩默认是⽆限的么? 还是你们会通过什么⽅式来指定⼤⼩？</h3><p>　　　　JDK 1.8后用元空间替代了 Perm Space；字符串常量存放到堆内存中。</p><p>　　　　MetaSpace大小默认没有限制，一般根据系统内存的大小。JVM会动态改变此值。</p><p>　　　　-XX:MetaspaceSize：分配给类元数据空间（以字节计）的初始大小（Oracle逻辑存储上的初始高水位，the initial high-water-mark）。此值为估计值，MetaspaceSize的值设置的过大会延长垃圾回收时间。垃圾回收过后，引起下一次垃圾回收的类元数据空间的大小可能会变大。</p><p>　　　　-XX:MaxMetaspaceSize：分配给类元数据空间的最大值，超过此值就会触发Full GC，此值默认没有限制，但应取决于系统内存的大小。JVM会动态地改变此值。</p><h3 id="24、StackOverflow异常有没有遇到过？⼀般你猜测会在什么情况下被触发？如何指定⼀个线程的堆栈⼤⼩？⼀般你们写多少？"><a href="#24、StackOverflow异常有没有遇到过？⼀般你猜测会在什么情况下被触发？如何指定⼀个线程的堆栈⼤⼩？⼀般你们写多少？" class="headerlink" title="24、StackOverflow异常有没有遇到过？⼀般你猜测会在什么情况下被触发？如何指定⼀个线程的堆栈⼤⼩？⼀般你们写多少？"></a>24、StackOverflow异常有没有遇到过？⼀般你猜测会在什么情况下被触发？如何指定⼀个线程的堆栈⼤⼩？⼀般你们写多少？</h3><p>　　　　栈内存溢出，一般由栈内存的局部变量过爆了，导致内存溢出。出现在递归方法，参数个数过多，递归过深，递归没有出口。</p><blockquote><p>本文转载：</p><p><strong>作　　者</strong>：<strong><a href="https://www.cnblogs.com/JesseP" target="_blank" rel="noopener">JessePeng</a></strong><br><strong>出　　处</strong>：<a href="https://www.cnblogs.com/JesseP/p/11750847.html" target="_blank" rel="noopener">https://www.cnblogs.com/JesseP/p/11750847.html</a></p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 用户访问网站慢，排查思路 </title>
    <link href="/2020/07/01/2020-07-01-user-slow-server/"/>
    <url>/2020/07/01/2020-07-01-user-slow-server/</url>
    
    <content type="html"><![CDATA[<p>当出现网站慢的时候我们脑子中要映出几点原因：</p><p>1.程序代码执行方面</p><p>2.大量数据库操作</p><p>3.域名DNS解析问题</p><p>4.服务器环境</p><p>5.网络的带宽</p><p>6.用许多javascript特效</p><p>7.访问的东西大</p><p>8.系统资源不足</p><p>9.防火墙的过多使用</p><p>10.网络中某个端口形成了瓶颈导致网速变慢</p><p>\1.打开访问慢的网站观察下情况，通过火狐的fixfox 插件 或者 IE的元素查看工具，你网站里面加载的信息会一览无遗的展现出来，并且那些元素加载耗时多少秒等等情况，如何解决能，把远程耗时久的js下载到本地，或者直接删除。</p><p>\2. 我看了下页面中有多处连接数据库操作的地方，并且有远程的数据库操作，并且还有多余的数据库连接代码，话不多说，改之.</p><p>   解决完了发现的确是快点了，但是还是不理想，于是我把页面执行数据库代码放到了数据库中执行没有耗慢的情况。</p><p>\3. 关于域名DNS的情况只是其中一种情况，不要急着找域名商的问题，你可以写个没有数据操作的页面放在同台服务器域名下，看看是不是访问同样慢，如果是才有可能，你还要让你周围的人也看看，最好别是你同公司的人。</p><p>\4. 我来看看服务器的情况吧，是不是CPU使用率过高造成的呢。</p><p>   a. top  发现cpu使用也不高啊，30% 左右，但是发现一个问题，sleeping 的进程数比较多。擦，最好别是僵尸进程，现在这样的东西不多了。</p><p>   b. 查看了下timewait的量: 发现有mysqld 和 httpd 的，大部分来自于 httpd  ； 命令 netstat -ae|grep TIME_WAIT</p><pre><code>如何来解决timewait的量问题呢？</code></pre><p>TIME_WAIT解决办法：</p><p>vi /etc/sysctl.conf</p><p>编辑文件，加入以下内容：<br>net.ipv4.tcp_syncookies = 1<br>net.ipv4.tcp_tw_reuse = 1<br>net.ipv4.tcp_tw_recycle = 1<br>net.ipv4.tcp_fin_timeout = 30<br>net.ipv4.tcp_keepalive_time = 30  保持连接的时间<br>net.ipv4.tcp_max_tw_buckets = 100 这个是设置服务器同时保持的time_wait的数目 </p><p>然后执行 /sbin/sysctl -p 让参数生效。</p><p>如果还不够满意可以 再设置下Ulimit参数<br>cat &gt;&gt;/etc/security/limits.conf&lt;&lt;EOF<br>* soft nofile 655350<br>* hard nofile 655350<br>EOF<br>然后ulimit -SHn 了 让生效。</p>]]></content>
    
    
    
    <tags>
      
      <tag>面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Linux top命令参数详解 </title>
    <link href="/2020/06/29/2020-06-29-linux-top/"/>
    <url>/2020/06/29/2020-06-29-linux-top/</url>
    
    <content type="html"><![CDATA[<h3 id="一、top前5行统计信息"><a href="#一、top前5行统计信息" class="headerlink" title="一、top前5行统计信息"></a>一、top前5行统计信息</h3><p><strong>第1行：top - 05:43:27 up 4:52, 2 users, load average: 0.58, 0.41, 0.30</strong><br>第1行是任务队列信息，其参数如下：</p><table><thead><tr><th align="left">内容</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">05:43:27</td><td align="left">表示当前时间</td></tr><tr><td align="left">up 4:52</td><td align="left">系统运行时间 格式为时：分</td></tr><tr><td align="left">2 users</td><td align="left">当前登录用户数</td></tr><tr><td align="left">load average: 0.58, 0.41, 0.30</td><td align="left">系统负载，即任务队列的平均长度。 三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。</td></tr></tbody></table><p>load average: 如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 </p><p><strong>第2行：Tasks: 159 total, 1 running, 158 sleeping, 0 stopped, 0 zombie</strong><br><strong>第3行：%Cpu(s): 37.0 us, 3.7 sy, 0.0 ni, 59.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st</strong><br>第2、3行为进程和CPU的信息<br>当有多个CPU时，这些内容可能会超过两行，其参数如下：</p><table><thead><tr><th align="left">内容</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">159 total</td><td align="left">进程总数</td></tr><tr><td align="left">1 running</td><td align="left">正在运行的进程数</td></tr><tr><td align="left">158 sleeping</td><td align="left">睡眠的进程数</td></tr><tr><td align="left">0 stopped</td><td align="left">停止的进程数</td></tr><tr><td align="left">0 zombie</td><td align="left">僵尸进程数</td></tr><tr><td align="left">37.0 us</td><td align="left">用户空间占用CPU百分比</td></tr><tr><td align="left">3.7 sy</td><td align="left">内核空间占用CPU百分比</td></tr><tr><td align="left"><strong>0.0 ni</strong></td><td align="left">用户进程空间内改变过优先级的进程占用CPU百分比</td></tr><tr><td align="left">59.3 id</td><td align="left">空闲CPU百分比</td></tr><tr><td align="left">0.0 wa</td><td align="left">等待输入输出的CPU时间百分比</td></tr><tr><td align="left"><strong>0.0 hi</strong></td><td align="left">硬中断（Hardware IRQ）占用CPU的百分比</td></tr><tr><td align="left"><strong>0.0 si</strong></td><td align="left">软中断（Software Interrupts）占用CPU的百分比</td></tr><tr><td align="left"><strong>0.0 st</strong></td><td align="left"></td></tr></tbody></table><p>第4行：KiB Mem: 1530752 total, 1481968 used, 48784 free, 70988 buffers<br>第5行：KiB Swap: 3905532 total, 267544 used, 3637988 free. 617312 cached Mem<br>第4、5行为内存信息<br>其参数如下：</p><table><thead><tr><th align="left">内容</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">KiB Mem: 1530752 total</td><td align="left">物理内存总量</td></tr><tr><td align="left">1481968 used</td><td align="left">使用的物理内存总量</td></tr><tr><td align="left">48784 free</td><td align="left">空闲内存总量</td></tr><tr><td align="left">70988 buffers（buff/cache）</td><td align="left">用作内核缓存的内存量</td></tr><tr><td align="left">KiB Swap: 3905532 total</td><td align="left">交换区总量</td></tr><tr><td align="left">267544 used</td><td align="left">使用的交换区总量</td></tr><tr><td align="left">3637988 free</td><td align="left">空闲交换区总量</td></tr><tr><td align="left">617312 cached Mem</td><td align="left">缓冲的交换区总量。</td></tr><tr><td align="left">3156100 avail Mem</td><td align="left">代表可用于进程下一次分配的物理内存数量</td></tr></tbody></table><p>上述最后提到的缓冲的交换区总量，这里解释一下，所谓缓冲的交换区总量，即内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小。相应的内存再次被换出时可不必再对交换区写入。 </p><p>计算可用内存数有一个近似的公式：<br>第四行的free + 第四行的buffers + 第五行的cached</p><h3 id="二、进程信息"><a href="#二、进程信息" class="headerlink" title="二、进程信息"></a>二、进程信息</h3><table><thead><tr><th align="left">列名</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">PID</td><td align="left">进程id</td></tr><tr><td align="left">PPID</td><td align="left">父进程id</td></tr><tr><td align="left">RUSER</td><td align="left">Real user name</td></tr><tr><td align="left">UID</td><td align="left">进程所有者的用户id</td></tr><tr><td align="left">USER</td><td align="left">进程所有者的用户名</td></tr><tr><td align="left">GROUP</td><td align="left">进程所有者的组名</td></tr><tr><td align="left">TTY</td><td align="left">启动进程的终端名。不是从终端启动的进程则显示为 ?</td></tr><tr><td align="left">PR</td><td align="left">优先级</td></tr><tr><td align="left">NI</td><td align="left">nice值。负值表示高优先级，正值表示低优先级</td></tr><tr><td align="left">P</td><td align="left">最后使用的CPU，仅在多CPU环境下有意义</td></tr><tr><td align="left">%CPU</td><td align="left">上次更新到现在的CPU时间占用百分比</td></tr><tr><td align="left">TIME</td><td align="left">进程使用的CPU时间总计，单位秒</td></tr><tr><td align="left">TIME+</td><td align="left">进程使用的CPU时间总计，单位1/100秒</td></tr><tr><td align="left">%MEM</td><td align="left">进程使用的物理内存百分比</td></tr><tr><td align="left">VIRT</td><td align="left">进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES</td></tr><tr><td align="left">SWAP</td><td align="left">进程使用的虚拟内存中，被换出的大小，单位kb</td></tr><tr><td align="left">RES</td><td align="left">进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA</td></tr><tr><td align="left">CODE</td><td align="left">可执行代码占用的物理内存大小，单位kb</td></tr><tr><td align="left">DATA</td><td align="left">可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb</td></tr><tr><td align="left">SHR</td><td align="left">共享内存大小，单位kb</td></tr><tr><td align="left">nFLT</td><td align="left">页面错误次数</td></tr><tr><td align="left">nDRT</td><td align="left">最后一次写入到现在，被修改过的页面数。</td></tr><tr><td align="left">S</td><td align="left">进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程</td></tr><tr><td align="left">COMMAND</td><td align="left">命令名/命令行</td></tr><tr><td align="left">WCHAN</td><td align="left">若该进程在睡眠，则显示睡眠中的系统函数名</td></tr><tr><td align="left">Flags</td><td align="left">任务标志</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> kafka和各MQ的优劣对比 </title>
    <link href="/2020/06/29/2020-06-29-kafka-vs-mq/"/>
    <url>/2020/06/29/2020-06-29-kafka-vs-mq/</url>
    
    <content type="html"><![CDATA[<h1 id="目前在业界有哪些比较知名的消息引擎"><a href="#目前在业界有哪些比较知名的消息引擎" class="headerlink" title="目前在业界有哪些比较知名的消息引擎"></a>目前在业界有哪些比较知名的消息引擎</h1><ul><li>ZeroMQ</li><li>推特的Distributedlog</li><li>ActiveMQ：Apache旗下的老牌消息引擎</li><li>RabbitMQ、Kafka</li><li>RocketMQ</li><li>Artemis：Apache的ActiveMQ下的子项目</li><li>Apollo：同样为Apache的ActiveMQ的子项目的号称下一代消息引擎</li><li>商业化的消息引擎IronMQ</li><li>以及实现了JMS(Java Message Service)标准的OpenMQ。</li></ul><h1 id="MQ消息队列的技术应用场景"><a href="#MQ消息队列的技术应用场景" class="headerlink" title="MQ消息队列的技术应用场景"></a>MQ消息队列的技术应用场景</h1><p><strong>1. 解耦</strong><br>   解耦是消息队列要解决的最本质问题。</p><p><strong>2. 最终一致性</strong><br>   最终一致性指的是<strong>两个系统的状态保持一致，要么都成功，要么都失败</strong>。最终一致性不是消息队列的必备特性，但确实可以依靠消息队列来做最终一致性的事情。</p><p><strong>2. 广播</strong><br>   <strong>消息队列的基本功能之一是进行广播。</strong>有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情，无疑极大地减少了开发和联调的工作量。</p><p><strong>3. 错峰与流控</strong><br>   典型的使用场景就是秒杀业务用于流量削峰场景。</p><h1 id="Kafka、RocketMQ、RabbitMQ简单比较"><a href="#Kafka、RocketMQ、RabbitMQ简单比较" class="headerlink" title="Kafka、RocketMQ、RabbitMQ简单比较"></a>Kafka、RocketMQ、RabbitMQ简单比较</h1><h2 id="1-ActiveMQ"><a href="#1-ActiveMQ" class="headerlink" title="1. ActiveMQ"></a>1. ActiveMQ</h2><ul><li>优点<br>单机吞吐量：万级<br>topic数量都吞吐量的影响：<br>时效性：ms级<br>可用性：高，基于主从架构实现高可用性<br>消息可靠性：有较低的概率丢失数据<br>功能支持：MQ领域的功能极其完备</li><li>缺点<br>官方社区现在对ActiveMQ 5.x维护越来越少，较少在大规模吞吐的场景中使用。</li></ul><h2 id="2-Kafka"><a href="#2-Kafka" class="headerlink" title="2. Kafka"></a>2. Kafka</h2><p>   号称大数据的杀手锏，谈到大数据领域内的消息传输，则绕不开Kafka，这款为大数据而生的消息中间件，以其百万级TPS的吞吐量名声大噪，迅速成为大数据领域的宠儿，在数据采集、传输、存储的过程中发挥着举足轻重的作用。<br>Apache Kafka它最初由LinkedIn公司基于独特的设计实现为一个分布式的提交日志系统( a distributed commit log)，之后成为Apache项目的一部分。<br>目前已经被LinkedIn，Uber, Twitter, Netflix等大公司所采纳。</p><ul><li>优点<br>性能卓越，单机写入TPS约在百万条/秒，最大的优点，就是吞吐量高。<br>时效性：ms级<br>可用性：非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用<br>消费者采用Pull方式获取消息, 消息有序, 通过控制能够保证所有消息被消费且仅被消费一次;<br>有优秀的第三方Kafka Web管理界面Kafka-Manager；<br>在日志领域比较成熟，被多家公司和多个开源项目使用；<br>功能支持：功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用</li><li>缺点<br>Kafka单机超过64个队列/分区，Load会发生明显的飙高现象，队列越多，load越高，发送消息响应时间变长<br>使用短轮询方式，实时性取决于轮询间隔时间；<br>消费失败不支持重试；<br>支持消息顺序，但是一台代理宕机后，就会产生消息乱序；<br>社区更新较慢；</li></ul><h2 id="3-RabbitMQ"><a href="#3-RabbitMQ" class="headerlink" title="3. RabbitMQ"></a>3. RabbitMQ</h2><p>   RabbitMQ 2007年发布，是一个在AMQP(高级消息队列协议)基础上完成的，可复用的企业消息系统，是当前最主流的消息中间件之一。</p><ul><li>优点<br>由于erlang语言的特性，mq 性能较好，高并发；<br>吞吐量到万级，MQ功能比较完备<br>健壮、稳定、易用、跨平台、支持多种语言、文档齐全；<br>开源提供的管理界面非常棒，用起来很好用<br>社区活跃度高；</li><li>缺点<br>erlang开发，很难去看懂源码，基本职能依赖于开源社区的快速维护和修复bug，不利于做二次开发和维护。<br>RabbitMQ确实吞吐量会低一些，这是因为他做的实现机制比较重。<br>需要学习比较复杂的接口和协议，学习和维护成本较高。</li></ul><h2 id="4-RocketMQ"><a href="#4-RocketMQ" class="headerlink" title="4. RocketMQ"></a>4. RocketMQ</h2><p>   RocketMQ出自 阿里公司的开源产品，用 Java 语言实现，在设计时参考了 Kafka，并做出了自己的一些改进。<br>   RocketMQ在阿里集团被广泛应用在订单，交易，充值，流计算，消息推送，日志流式处理，binglog分发等场景。</p><ul><li>优点<br>单机吞吐量：十万级<br>可用性：非常高，分布式架构<br>消息可靠性：经过参数优化配置，消息可以做到0丢失<br>功能支持：MQ功能较为完善，还是分布式的，扩展性好<br>支持10亿级别的消息堆积，不会因为堆积导致性能下降<br>源码是java，我们可以自己阅读源码，定制自己公司的MQ，可以掌控</li><li>缺点<br>支持的客户端语言不多，目前是java及c++，其中c++不成熟；<br>社区活跃度一般<br>没有在 mq 核心中去实现JMS等接口，有些系统要迁移需要修改大量代码</li></ul><h1 id="消息队列选择建议"><a href="#消息队列选择建议" class="headerlink" title="消息队列选择建议"></a>消息队列选择建议</h1><h2 id="1-Kafka"><a href="#1-Kafka" class="headerlink" title="1. Kafka"></a>1. Kafka</h2><p>   Kafka主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，适合产生大量数据的互联网服务的数据收集业务。<br>大型公司建议可以选用，如果有日志采集功能，肯定是首选kafka了。</p><h2 id="2-RocketMQ"><a href="#2-RocketMQ" class="headerlink" title="2. RocketMQ"></a>2. RocketMQ</h2><p>   天生为金融互联网领域而生，对于可靠性要求很高的场景，尤其是电商里面的订单扣款，以及业务削峰，在大量交易涌入时，后端可能无法及时处理的情况。<br>RoketMQ在稳定性上可能更值得信赖，这些业务场景在阿里双11已经经历了多次考验，如果你的业务有上述并发场景，建议可以选择RocketMQ。</p><h2 id="3-RabbitMQ-1"><a href="#3-RabbitMQ-1" class="headerlink" title="3. RabbitMQ"></a>3. RabbitMQ</h2><p>   RabbitMQ :结合erlang语言本身的并发优势，性能较好，社区活跃度也比较高，但是不利于做二次开发和维护。不过，RabbitMQ的社区十分活跃，可以解决开发过程中遇到的bug。</p><p>   如果你的数据量没有那么大，小公司优先选择功能比较完备的RabbitMQ。</p><p>本文转载，原文地址： <a href="https://links.jianshu.com/go?to=http%3A%2F%2Fyouzhixueyuan.com%2Fcomparison-of-kafka-rocketmq-rabbitmq.html" target="_blank" rel="noopener">高并发架构系列：Kafka、RocketMQ、RabbitMQ的优劣势比较</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>消息队列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> linux运维面试问题总结 </title>
    <link href="/2020/06/29/2020-06-29-linux-devops/"/>
    <url>/2020/06/29/2020-06-29-linux-devops/</url>
    
    <content type="html"><![CDATA[<h3 id="kube-proxy挂掉的影响"><a href="#kube-proxy挂掉的影响" class="headerlink" title="kube-proxy挂掉的影响"></a>kube-proxy挂掉的影响</h3><p>service的访问请求将不会转发到Pod上，用户无法访问</p><h3 id="k8s-service涉及到的组件"><a href="#k8s-service涉及到的组件" class="headerlink" title="k8s service涉及到的组件"></a>k8s service涉及到的组件</h3><p>kube-proxy、pod、Replication Controller</p><h3 id="ingress和service的区别"><a href="#ingress和service的区别" class="headerlink" title="ingress和service的区别"></a>ingress和service的区别</h3><p>Service可以看作是一组提供相同服务的Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡。Ingress 是全局的，为了代理不同后端 Service 而设置的负载均衡服务。</p><p>Service 有三种对外暴露的方法，但是由于每个 Service 都要有一个负载均衡的服务，所以采用 Service 的话，会造成既浪费成本又高的现象。而ingress是一个全局的负载均衡器,然后我只需要通过访问 URL 就可以把请求转发给不同的后端 Service ，从而可以访问到界面，而不是每个 Service 都需要负载均衡。</p><h3 id="k8s设置node节点污点"><a href="#k8s设置node节点污点" class="headerlink" title="k8s设置node节点污点"></a>k8s设置node节点污点</h3><p>kubectl taint node [node] key=value[effect]<br>     其中[effect] 可取值: [ NoSchedule | PreferNoSchedule | NoExecute ]<br>      NoSchedule: 一定不能被调度<br>      PreferNoSchedule: 尽量不要调度<br>      NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod</p><h3 id="k8s的pause容器有什么用"><a href="#k8s的pause容器有什么用" class="headerlink" title="k8s的pause容器有什么用"></a>k8s的pause容器有什么用</h3><p>pod内的其他容器会共用pause容器的网络栈和存储卷，保证pod内的其他容器的端口不能冲突，彼此都是通过localhost就可以访问，扮演PID1的角色,并在子进程称为”孤儿进程”的时候,通过调用wait()收割这个子进程,这样就不用担心我们的Pod的PID namespace里会堆满僵尸进程了。</p><h3 id="kubernetes中的pause容器主要为每个业务容器提供以下功能："><a href="#kubernetes中的pause容器主要为每个业务容器提供以下功能：" class="headerlink" title="kubernetes中的pause容器主要为每个业务容器提供以下功能："></a>kubernetes中的pause容器主要为每个业务容器提供以下功能：</h3><p>PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID。</p><p>网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围。</p><p>IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信。</p><p>UTS命名空间：Pod中的多个容器共享一个主机名；Volumes（共享存储卷）：</p><p>Pod中的各个容器可以访问在Pod级别定义的Volumes。</p><h3 id="kafka相比mq的优势"><a href="#kafka相比mq的优势" class="headerlink" title="kafka相比mq的优势"></a>kafka相比mq的优势</h3><p>mq的作用：解耦、最终一致性、广播、流浪控制及削峰</p><p>优点<br> 性能卓越，单机写入TPS约在百万条/秒，最大的优点，就是吞吐量高。<br> 时效性：ms级<br> 可用性：非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用<br> 消费者采用Pull方式获取消息, 消息有序, 通过控制能够保证所有消息被消费且仅被消费一次;<br> 有优秀的第三方Kafka Web管理界面Kafka-Manager；<br> 在日志领域比较成熟，被多家公司和多个开源项目使用；<br> 功能支持：功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用</p><p>缺点<br> Kafka单机超过64个队列/分区，Load会发生明显的飙高现象，队列越多，load越高，发送消息响应时间变长<br> 使用短轮询方式，实时性取决于轮询间隔时间；<br> 消费失败不支持重试；<br> 支持消息顺序，但是一台代理宕机后，就会产生消息乱序；</p><h3 id="linux系统执行命令很卡"><a href="#linux系统执行命令很卡" class="headerlink" title="linux系统执行命令很卡"></a>linux系统执行命令很卡</h3><ul><li>top命令查看系统资源使用情况，cpu、内存、负载</li><li>iostat命令查看磁盘IO使用情况</li></ul><h3 id="shell脚本的变量"><a href="#shell脚本的变量" class="headerlink" title="shell脚本的变量"></a>shell脚本的变量</h3><p>$0  文件名及路径</p><p>$1,$2  参数1，参数2</p><p>$#  传递给脚本或函数的参数个数</p><p>$$  当前Shell进程ID</p><p>$?  判断上个命令的执行成功与否，0为成功。</p><p>$@  传递脚本或函数的所有参数</p><p>$*  传递脚本或函数的所有参数</p><h3 id="linux-cpu-负载概念"><a href="#linux-cpu-负载概念" class="headerlink" title="linux cpu 负载概念"></a>linux cpu 负载概念</h3><p>这些数据来自于文件/proc/loadavg，内核会负责统计出这些数据。<br>top和uptime命令显示的内容就来自于这个文件，根据proc的帮助文件可知，这里的值就是单位时间内处于运行状态以及等待磁盘 I/O状态的平均job数量。这里的运行状态和job都是内核的概念，这里进行说明：<br>1、 对于内核而言，进程和线程都是job<br>2、 job处于运行状态指job处于内核的运行队列中，正在或等待被CPU调度（用户空间的进程正在运行不代表需要被CPU调度，有可能在等待I/O，也有可能在sleep等等）</p><h3 id="linux硬链接和软链接原理"><a href="#linux硬链接和软链接原理" class="headerlink" title="linux硬链接和软链接原理"></a>linux硬链接和软链接原理</h3><ul><li>硬链接：在Linux系统中，多个文件名指向同一索引节点(Inode)是正常且允许的。一般这种链接就称为硬链接。硬链接的作用之一是允许一个文件拥有多个有效路径名，这样用户就可以建立硬链接到重要的文件，以防止“误删”源数据。</li><li>软链接： 软链接就是一个普通文件，只是数据块内容有点特殊，文件用户数据块中存放的内容是另一文件的路径名的指向，通过这个方式可以快速定位到软连接所指向的源文件实体。软链接可对文件或目录创建。</li></ul><p>软连接和硬链接的特点：</p><p>软链接：</p><ul><li>1.软链接是存放另一个文件的路径的形式存在。</li><li>2.软链接可以 跨文件系统 ，硬链接不可以。</li><li>3.软链接可以对一个不存在的文件名进行链接，硬链接必须要有源文件。</li><li>4.软链接可以对目录进行链接。</li></ul><p>硬链接：</p><ul><li><ol><li>硬链接，以文件副本的形式存在。但不占用实际空间。</li></ol></li><li><ol start="2"><li>不允许给目录创建硬链接。</li></ol></li><li><ol start="3"><li>硬链接只有在同一个文件系统中才能创建。</li></ol></li><li><ol start="4"><li>删除其中一个硬链接文件并不影响其他有相同 inode 号的文件。</li></ol></li></ul><h3 id="linux进程退出指令"><a href="#linux进程退出指令" class="headerlink" title="linux进程退出指令"></a>linux进程退出指令</h3><p> INT（快速关闭）—-是当用户键入<Control-C>时由终端驱动程序发送的信号。这是一个终止当前操作的请求，如果捕获了这个信号，一些简单的程序应该退出，或者允许自给被终止，这也是程序没有捕获到这个信号时的默认处理方法。拥有命令行或者输入模式的那些程序应该停止它们在做的事情，清除状态，并等待用户的再次输入。</p><p>  TERM（快速关闭）—-是请求彻底终止某项执行操作，它期望接收进程清除自给的状态并退出。</p><p>  HUP—- 平滑启动。如果想要更改配置而不需停止并重新启动服务，请使用该命令。在对配置文件作必要的更改后，发出该命令以动态更新服务配置。</p><p>  QUIT：从容关闭。</p>]]></content>
    
    
    
    <tags>
      
      <tag>面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Nginx location匹配规则 </title>
    <link href="/2020/06/11/2020-06-11-nginx-config-location/"/>
    <url>/2020/06/11/2020-06-11-nginx-config-location/</url>
    
    <content type="html"><![CDATA[<p>#一、location语法</p><pre><code class="hljs crmsh"><span class="hljs-keyword">location</span> <span class="hljs-title">[=|~|~*|^~] uri</span> &#123; … &#125;</code></pre><p>其中，方括号中的四种标识符是可选项，用来改变请求字符串和uri的匹配方式。uri是待匹配的请求字符串，可以是不包含正则的字符串，这种模式被称为“标准的uri”；也可以包含正则，这种模式被称为”正则uri”。例如：</p><p>location ~ .*.(php|php5)?$ {<br>　　root /var/www/html;<br>　　……<br>}</p><h1 id="二、四种可选标识符"><a href="#二、四种可选标识符" class="headerlink" title="二、四种可选标识符"></a>二、四种可选标识符</h1><table><thead><tr><th>标识符</th><th>描述</th></tr></thead><tbody><tr><td>=</td><td><strong>精确匹配：</strong>用于标准uri前，要求请求字符串和uri严格匹配。如果匹配成功就停止匹配，立即执行该location里面的请求。</td></tr><tr><td>~</td><td><strong>正则匹配：</strong>用于正则uri前，表示uri里面包含正则，并且区分大小写。</td></tr><tr><td>~*</td><td><strong>正则匹配：</strong>用于正则uri前，表示uri里面包含正则，不区分大小写。</td></tr><tr><td>^~</td><td><strong>非正则匹配；</strong>用于标准uri前，nginx服务器匹配到前缀最多的uri后就结束，该模式匹配成功后，不会使用正则匹配。</td></tr><tr><td>无</td><td><strong>普通匹配（最长字符匹配）；</strong>与location顺序无关，是按照匹配的长短来取匹配结果。若完全匹配，就停止匹配。</td></tr></tbody></table><h1 id="三、匹配标识符案例"><a href="#三、匹配标识符案例" class="headerlink" title="三、匹配标识符案例"></a>三、匹配标识符案例</h1><h2 id="1-“-”精准匹配"><a href="#1-“-”精准匹配" class="headerlink" title="1. “=”精准匹配"></a>1. “=”精准匹配</h2><pre><code class="hljs crmsh"><span class="hljs-keyword">location</span> <span class="hljs-title">= /news</span>/ &#123;            echo <span class="hljs-string">"test1"</span>;        &#125;</code></pre><pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/news/test1</code></pre><h2 id="2-“-”区分大小写正则匹配"><a href="#2-“-”区分大小写正则匹配" class="headerlink" title="2. “~”区分大小写正则匹配"></a>2. “~”区分大小写正则匹配</h2><pre><code class="hljs nginx"><span class="hljs-attribute">location</span> <span class="hljs-regexp">~ \.(html)</span> &#123;    <span class="hljs-attribute">echo</span> <span class="hljs-string">'test2'</span>;&#125;<span class="hljs-attribute">location</span> <span class="hljs-regexp">~ \.(htmL)</span> &#123;    <span class="hljs-attribute">echo</span> <span class="hljs-string">'test3'</span>;&#125;</code></pre><pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/index.htmltest2[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/index.htmLtest3</code></pre><h2 id="3-“-”不区分大小写的正则匹配"><a href="#3-“-”不区分大小写的正则匹配" class="headerlink" title="3. “~*”不区分大小写的正则匹配"></a>3. “~*”不区分大小写的正则匹配</h2><pre><code class="hljs nginx"><span class="hljs-attribute">location</span> <span class="hljs-regexp">~* \.(html)</span>&#123;            <span class="hljs-attribute">echo</span> <span class="hljs-string">'test4'</span>;&#125;</code></pre><pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/index.htmLtest4[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/index.htmltest4</code></pre><h2 id="4-“-”不进行正则匹配的标准匹配，只匹配前缀"><a href="#4-“-”不进行正则匹配的标准匹配，只匹配前缀" class="headerlink" title="4. “^~”不进行正则匹配的标准匹配，只匹配前缀"></a>4. “^~”不进行正则匹配的标准匹配，只匹配前缀</h2><pre><code class="hljs nginx"><span class="hljs-attribute">location</span><span class="hljs-regexp"> ^~</span> /index/ &#123;            <span class="hljs-attribute">echo</span> <span class="hljs-string">'test5'</span>;&#125;</code></pre><pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/index/test5[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/index/heiheitest5[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>/index/asdnmkalsjdtest5</code></pre><h2 id="5-普通匹配"><a href="#5-普通匹配" class="headerlink" title="5. 普通匹配"></a>5. 普通匹配</h2><pre><code class="hljs crmsh"><span class="hljs-keyword">location</span> <span class="hljs-title">/ &#123;</span><span class="hljs-title">            echo</span> 'test6';&#125;</code></pre><pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>test]# curl <span class="hljs-number">192.168</span><span class="hljs-number">.233</span><span class="hljs-number">.22</span>test6</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Nginx配置location中proxy_pass的&#39;/&#39;号的作用 </title>
    <link href="/2020/05/26/2020-05-26-nginx-config-location-proxypass/"/>
    <url>/2020/05/26/2020-05-26-nginx-config-location-proxypass/</url>
    
    <content type="html"><![CDATA[<p><strong>真实案例，就因为在配置时，少写了一个字符“/”，就造成访问不通报错，因而接到投诉。那么是怎么引起的呢？原因就是：Nginx在配置proxy_pass代理转接时，少些“/”字符造成的。有同学就有疑问，加不加“/”,区别真的那么大吗？我们带着这个疑问，来探究下这个问题。</strong></p><h1 id="location目录匹配详解"><a href="#location目录匹配详解" class="headerlink" title="location目录匹配详解"></a>location目录匹配详解</h1><p>nginx每个location都是一个匹配目录，nginx的策略是：访问请求来时，会对访问地址进行解析，从上到下逐个匹配，匹配上就执行对应location大括号中的策略，并根据策略对请求作出相应。</p><p>依访问地址：<a href="http://www.example.com/book/index.html" target="_blank" rel="noopener">http://www.example.com/book/index.html</a> 为例，nginx配置如下：</p><pre><code class="hljs nginx"><span class="hljs-attribute">location</span> /book/  &#123;                    <span class="hljs-attribute">proxy_connect_timeout</span> <span class="hljs-number">18000</span>; <span class="hljs-comment">##修改成半个小时                        </span><span class="hljs-attribute">proxy_send_timeout</span> <span class="hljs-number">18000</span>;                    <span class="hljs-attribute">proxy_read_timeout</span> <span class="hljs-number">18000</span>;                    <span class="hljs-attribute">proxy_pass</span> http://127.0.0.1:8080;        &#125;</code></pre><p>那访问时就会匹配这个location,从而把请求代理转发到本机的8080Tomcat服务中，Tomcat相应后，信息原路返回。总结：<strong>location如果没有“/”时，请求就可以模糊匹配以字符串开头的所有字符串，而有“/”时，只能精确匹配字符本身。</strong></p><p>下面举个例子说明：</p><p> 配置location /book可以匹配/bookdada请求，也可以匹配/book*/dada等等，只要以book开头的目录都可以匹配到。而location /book/必须精确匹配/book/这个目录的请求,不能匹配/bookdada/或/book*/dada等请求。</p><h1 id="proxy-pass有无“-”的四种区别探究"><a href="#proxy-pass有无“-”的四种区别探究" class="headerlink" title="proxy_pass有无“/”的四种区别探究"></a>proxy_pass有无“/”的四种区别探究</h1><p>访问地址都是以：<a href="http://www.book.com/bddd/index.html" target="_blank" rel="noopener">http://www.book.com/bddd/index.html</a> 为例。请求都匹配目录/bddd/</p><h2 id="第一种：加”-“"><a href="#第一种：加”-“" class="headerlink" title="第一种：加”/“"></a>第一种：加”/“</h2><pre><code class="hljs crmsh"><span class="hljs-keyword">location</span>  <span class="hljs-title">/bddd</span>/ &#123;    proxy_pass  http://<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">8080</span>/;&#125;</code></pre><p>测试结果，请求被代理跳转到：<a href="http://127.0.0.1:8080/index.html" target="_blank" rel="noopener">http://127.0.0.1:8080/index.html</a></p><h2 id="第二种-不加”-“"><a href="#第二种-不加”-“" class="headerlink" title="第二种: 不加”/“"></a>第二种: 不加”/“</h2><pre><code class="hljs crmsh"><span class="hljs-keyword">location</span>  <span class="hljs-title">/bddd</span>/ &#123;            proxy_pass http://<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">8080</span>;&#125;</code></pre><p>测试结果，请求被代理跳转到：<a href="http://127.0.0.1:8080/bddd/index.html" target="_blank" rel="noopener">http://127.0.0.1:8080/bddd/index.html</a></p><p>3# 第三种: 增加目录加”/“</p><pre><code class="hljs awk">location  <span class="hljs-regexp">/bddd/</span> &#123;            proxy_pass http:<span class="hljs-regexp">//</span><span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">8080</span><span class="hljs-regexp">/sun/</span>;&#125;</code></pre><p>测试结果，请求被代理跳转到：<a href="http://127.0.0.1:8080/sun/index.html" target="_blank" rel="noopener">http://127.0.0.1:8080/sun/index.html</a></p><h2 id="第四种：增加目录不加”-“"><a href="#第四种：增加目录不加”-“" class="headerlink" title="第四种：增加目录不加”/“"></a>第四种：增加目录不加”/“</h2><pre><code class="hljs crmsh"><span class="hljs-keyword">location</span>  <span class="hljs-title">/bddd</span>/ &#123;    proxy_pass http://<span class="hljs-number">127.0</span>.<span class="hljs-number">0.1</span>:<span class="hljs-number">8080</span>/sun;&#125;</code></pre><p>测试结果，请求被代理跳转到：<a href="http://127.0.0.1:8080/sunindex.html" target="_blank" rel="noopener">http://127.0.0.1:8080/sunindex.html</a></p><p><strong>总结</strong></p><p>location目录后加”/“,只能匹配目录，不加“/”不仅可以匹配目录还对目录进行模糊匹配。而proxy_pass无论加不加“/”,代理跳转地址都直接拼接。</p><p>为了加深大家印象可以用下面的配置实验测试下：</p><pre><code class="hljs nginx"><span class="hljs-section">server</span> &#123;     <span class="hljs-attribute">listen</span>       <span class="hljs-number">80</span>;     <span class="hljs-attribute">server_name</span>  localhost;   <span class="hljs-comment"># http://localhost/bddd01/xxx -&gt; http://localhost:8080/bddd01/xxx</span>  <span class="hljs-attribute">location</span> /bddd01/ &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080;     &#125;  <span class="hljs-comment"># http://localhost/bddd02/xxx -&gt; http://localhost:8080/xxx   </span>  <span class="hljs-attribute">location</span> /bddd02/ &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080/;      &#125;  <span class="hljs-comment"># http://localhost/bddd03/xxx -&gt; http://localhost:8080/bddd03*/xxx   </span>  <span class="hljs-attribute">location</span> /bddd03 &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080;     &#125;    <span class="hljs-comment"># http://localhost/bddd04/xxx -&gt; http://localhost:8080//xxx，请注意这里的双斜线，好好分析一下。</span>  <span class="hljs-attribute">location</span> /bddd04 &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080/;     &#125;  <span class="hljs-comment"># http://localhost/bddd05/xxx -&gt; http://localhost:8080/hahaxxx，请注意这里的haha和xxx之间没有斜杠，分析一下原因。</span>  <span class="hljs-attribute">location</span> /bddd05/ &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080/haha;      &#125;  <span class="hljs-comment"># http://localhost/bddd06/xxx -&gt; http://localhost:8080/haha/xxx   </span>  <span class="hljs-attribute">location</span> /bddd06/ &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080/haha/;     &#125;  <span class="hljs-comment"># http://localhost/bddd07/xxx -&gt; http://localhost:8080/haha/xxx   </span>  <span class="hljs-attribute">location</span> /bddd07 &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080/haha;     &#125;   <span class="hljs-comment"># http://localhost/bddd08/xxx -&gt; http://localhost:8080/haha//xxx，请注意这里的双斜杠。</span>  <span class="hljs-attribute">location</span> /bddd08 &#123;               <span class="hljs-attribute">proxy_pass</span> http://localhost:8080/haha/;     &#125;&#125;</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> MySQL常见面试题（后续的面试题会更新） </title>
    <link href="/2020/05/25/2020-05-25-mysql-examination-questions/"/>
    <url>/2020/05/25/2020-05-25-mysql-examination-questions/</url>
    
    <content type="html"><![CDATA[<p><strong>题目一</strong></p><p>MyISAM和InnoDB的区别，什么时候选择MyISAM</p><p><strong>参考回答</strong></p><p>InnoDB是目前MySQL主流版本(5.6、5.7、8.0)默认的存储引擎，支持事务、外键、行级锁，对于并发条件下要求数据的一致性，适用于对数据准确性要求高的场景。</p><p>MyISAM只支持表级锁、数据排列是按照插入顺序，没有做规则排序。适合应用以查询和插入为主，只有很少量的更新和删除操作，对事务的完整性和并发性要求不是很高的场景。</p><p><strong>实际运用</strong></p><p>看到很多人在选择存储引擎的时候会无脑的选择InnoDB，这个选择合理的一点是如果对数据准确性要求没有那么高，直接用NoSQL就好了。用MySQL就是为了可靠啊。</p><p>但是实际工作中，我设计的数据库中通常都会有几张MyISAM的数据表，通常用来存储历史记录，与使用InnoDB存储实时记录信息的配合使用。</p><p>举个例子：比如一条物流信息，在实时的表里存着目前物流的状态：比如配送中。这条物流在历史上经过了：正在通知快递公司取件、XXX已收揽等，这张记录表基本只有插入和查询，并且丢失一个中间状态不影响当前结果，这就很合适用MyISAM。</p><p><strong>题目二</strong></p><p>简述MySQL的MVCC多版本并发控制</p><p><strong>参考回答</strong></p><p>MVCC是对于事务隔离级别的读已提交RC和可重复读RR，基于乐观锁的实现。在LBCC(基于锁的并发控制)RC、RR和串行化分别是通过加行锁、间隙锁和表锁来基于悲观锁实现。而乐观锁的原理就是在特定的时间点(RC是每次读时，RR是事务开始时)生成一个当前快照，读数据读取快照，只在提交时判断是否有冲突，类似于git的branch和commit。</p><p>MVCC会在新开启一个事务时，给事务里包含的每行记录添加一个当前事务ID和回滚指针。并包含一个Read View，Read View里保存了当前活跃的事务列表，小于这些列表的最近的事务ID才是可见的。这样保证了读到的都是已提交的事务。</p><p><strong>实际运用</strong></p><p>MVCC不仅可以用于数据库，也是很常见的一种并发控制手段。比如使用有限状态自动机来控制的订单状态，在更新订单状态的时候先查询当前状态，比如当前状态是订单未提交，则更新时update XXX set status=’订单已提交’ where status=’订单未提交’，如果执行这条语句时，status已经发生了改变，这条语句就执行失败了。这样不通过数据库自身事务的MVCC，在业务逻辑里也实现了MVCC思想的乐观锁设计。</p><p><strong>题目三</strong></p><p>分布式锁的实现方式</p><p><strong>参考回答</strong></p><p>主流有三种</p><p>1&gt;基于数据库</p><p>1.1&gt;基于数据库主键：插入一条数据，指定主键。如果有两条插入会主键冲突，并发执行失败</p><p>1.2&gt;基于数据库排他锁：提交一个update事务，如果这个事务不提交，其他也对锁定范围内执行update就会阻塞，解决并发问题</p><p>2&gt;基于缓存比如redis的setNX</p><p>3&gt;基于zookeeper</p><p><strong>实际运用</strong></p><p>相信很多人选择分布式锁都是选择第二种，第三种虽然并发性差一下，如果本来就引入了zk，而没有缓存，而分布式锁应用量又不那么大，为了减少引入新组件带来的风险和维护成本，也有可能选择zk。很多人大概认为自己没有用过基于数据库的分布式锁，实际上在不使用MVCC的时代并不是这样。</p><p>在使用spring进行业务开发的时候，常见的一种场景就是使用spring配置事务。默认级别是Repeatable Read可重复读。在这里面如果使用的是LBCC，一进入事务就加入一个排他锁，比如insert、update、delete或者select XXX for update。然后做其他的，比如进行一个RPC调用。这时候一旦出现并发，只有一个能顺利执行，其他都会被阻塞。实际上就相当于使用了分布式锁。</p><p><strong>题目四</strong></p><p>为什么采用B+树作为索引结构?</p><p><strong><em>\</em>参考回答**</strong></p><p>如果采用Hash表，范围查找需要全表扫描；如果采用二叉查找树，由于无法保证平衡，可能退化为链表；如果采用平衡二叉树，通过旋转解决了平衡的问题，但是旋转操作效率太低；如果采用红黑树，树太高，IO次数多；如果采用普通B树，节点要存数索引和数据，一个内存页可存储的数据还是少，另外范围查找也需要多次IO；</p><p>而B+Tree有三个特性：</p><p>1&gt;非叶子节点不存储data，只存储索引(冗余)，可以放更多的索引</p><p>2&gt;叶子节点包含所有索引字段</p><p>3&gt;叶子节点用指针链接，提高范围查询的性能</p><p><strong>实际运用</strong></p><p>在分布式场景下，我们的业务ID都是全局唯一的字符串。如果单纯从业务上来考虑，用业务ID作为数据库的主键就足够了。可以DBA往往要求使用整型的自增主键作为数据库主键，而这个主键对业务来说就是个浪费，没有任何业务含义。</p><p>如果了解了索引的底层结构就不难理解</p><p>1&gt;整型比字符串占用更少的空间</p><p>2&gt;同时大小比较也很快</p><p>3&gt;之所以要自增是每次插入新的记录，对于叶子节点来说：记录会顺序的添加到当前索引节点的后续位置，当一页写满，会自动开辟一个新的页。而如果使用非自增主键，就需要插入的时候移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要读回来。分页操作造成大量的碎片，必须通过优化操作重建表并优化填充页面。</p><p><strong>题目五</strong></p><p>什么叫做覆盖索引？</p><p><strong><em>\</em>参考回答**</strong></p><p>只需要在一棵辅助索引树上就可以获取SQL所需要的所有列数据，不需要回表。</p><p><strong>实际运用</strong></p><p>一些持久层框架比如mybatis的generator插件可以自动生成sql配置文件，这些配置文件往往效率很低。但是刚毕业的同学很多都不会去改这个文件，比如只需要个别列的时候会用java的lambda表达式等方式从逻辑上做处理。结果造成一些性能的问题。</p><p>我在根据一些条件进行范围查找的时候，如果只需要返回ID或者个别列，会自己去改mybatis的generator自动生成的文件，原因是尽量使用覆盖索引，较回表速度快。</p><p>想验证是否使用了覆盖索引，可以用explain执行计划，查看extra字段，如果只显示Using index说明正确使用了覆盖索引。如果extra为空或者除了using index还有filesort说明触发了回表。</p><p><strong>题目六</strong></p><p>查询在什么时候不走索引</p><p><strong><em>\</em>参考回答**</strong></p><p>主要三种情况</p><p>1&gt;不满足走索引的条件，常见的情况有</p><p>1.1&gt;不满足最左匹配原则</p><p>1.2&gt;查询条件使用了函数</p><p>1.3&gt;or操作有一个字段没有索引</p><p>1.4&gt;使用like条件以%开头</p><p>2&gt;走索引效率低于全表扫描，常见的情况有</p><p>2.1&gt;查询条件对null做判断，而null的值很多</p><p>2.2&gt;一个字段区分度很小，比如性别、状态</p><p>3&gt;需要回表的查询结果集过大，超过了配置的范围</p><p><strong>实际运用</strong></p><p>使用索引是为了对查询做优化，要衡量优化效果需要数据说话。所以需要一些工具来衡量，常用的有：</p><p>1&gt;慢查询日志</p><p>开启慢查询日志，可以针对慢SQL进行分析看看哪些可以用索引进行优化</p><p>2&gt;show processlist</p><p>show processlist 语句可以查看当前正在执行的SQL，如果一些SQL执行慢，block了其他的SQL，这是个很好的工具</p><p>3&gt;show profile分析SQL</p><p>支持的话，可以用select @@profiling 查看是否开启，如果结果为0说明未开启。需要先set @@profiling=1;</p><p>这时候就可以用show profiles查看每一条SQL语句耗费的时间</p><p>show profile for query XXID 可以查看具体耗费在哪个阶段</p><p>4&gt;Trace分析优化器的执行计划</p><p>使用set optimizer_trace=’enabled=on’,end_markers_in_json=on;可以打开trace分析，想查看具体的优化器执行计划，只要执行</p><p>select * from <code>information_schema</code>.optimizer_trace即可</p>]]></content>
    
    
    
    <tags>
      
      <tag>MYSQL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> HTTP 面试问题 </title>
    <link href="/2020/05/19/2020-05-19-http-interviews/"/>
    <url>/2020/05/19/2020-05-19-http-interviews/</url>
    
    <content type="html"><![CDATA[<h1 id="HTTP-和-HTTPS-的区别"><a href="#HTTP-和-HTTPS-的区别" class="headerlink" title="HTTP 和 HTTPS 的区别"></a>HTTP 和 HTTPS 的区别</h1><p>HTTP 是一种 <code>超文本传输协议(Hypertext Transfer Protocol)</code>，<strong>HTTP 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/http-content.png" srcset="/img/loading.gif" alt="http-content"></p><p>HTTP 主要内容分为三部分，<strong>超文本（Hypertext）、传输（Transfer）、协议（Protocol）</strong>。</p><ul><li>超文本就是不单单只是本文，它还可以传输图片、音频、视频，甚至点击文字或图片能够进行<code>超链接</code>的跳转。</li><li>上面这些概念可以统称为数据，传输就是数据需要经过一系列的物理介质从一个端系统传送到另外一个端系统的过程。通常我们把传输数据包的一方称为<code>请求方</code>，把接到二进制数据包的一方称为<code>应答方</code>。</li><li>而协议指的就是是网络中(包括互联网)传递、管理信息的一些规范。如同人与人之间相互交流是需要遵循一定的规矩一样，计算机之间的相互通信需要共同遵守一定的规则，这些规则就称为协议，只不过是网络协议。</li></ul><p>说到 HTTP，不得不提的就是 TCP/IP 网络模型，一般是五层模型。如下图所示</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tcp-five.png" srcset="/img/loading.gif" alt="tcp-five"></p><p>但是也可以分为四层，就是<strong>把链路层和物理层都表示为网络接口层</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tcp-four.png" srcset="/img/loading.gif" alt="tcp-four"></p><p>还有一种就是 OSI 七层网络模型，它就是在五层协议之上加了<strong>表示层和会话层</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/osi-seven.png" srcset="/img/loading.gif" alt="osi-seven"></p><p>而 HTTPS 的全称是 <code>Hypertext Transfer Protocol Secure</code>，从名称我们可以看出 HTTPS 要比 HTTPS 多了 secure 安全性这个概念，实际上， HTTPS 并不是一个新的应用层协议，它其实就是 HTTP + TLS/SSL 协议组合而成，而安全性的保证正是 TLS/SSL 所做的工作。</p><p>也就是说，<strong>HTTPS 就是身披了一层 SSL 的 HTTP</strong>。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/http-https-content.png" srcset="/img/loading.gif" alt="http-https-content"></p><p>那么，HTTP 和 HTTPS 的主要区别是什么呢？</p><ul><li>最简单的，HTTP 在地址栏上的协议是以 <code>http://</code> 开头，而 HTTPS 在地址栏上的协议是以 <code>https://</code> 开头</li></ul><pre><code class="hljs dts"><span class="hljs-symbol">http:</span><span class="hljs-comment">//www.baidu.com/</span><span class="hljs-symbol">https:</span><span class="hljs-comment">//www.baidu.com/</span></code></pre><ul><li>HTTP 是未经安全加密的协议，它的传输过程容易被攻击者监听、数据容易被窃取、发送方和接收方容易被伪造；而 HTTPS 是安全的协议，它通过 <strong>密钥交换算法 - 签名算法 - 对称加密算法 - 摘要算法</strong> 能够解决上面这些问题。</li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/chrome-ssl.png" srcset="/img/loading.gif" alt="chrome-ssl"></p><ul><li>HTTP 的默认端口是 80，而 HTTPS 的默认端口是 443。</li></ul><h1 id="HTTP-Get-和-Post-区别"><a href="#HTTP-Get-和-Post-区别" class="headerlink" title="HTTP Get 和 Post 区别"></a>HTTP Get 和 Post 区别</h1><p>HTTP 中包括许多方法，<strong>Get 和 Post 是 HTTP 中最常用的两个方法</strong>，基本上使用 HTTP 方法中有 99% 都是在使用 Get 方法和 Post 方法，所以有必要我们对这两个方法有更加深刻的认识。</p><ul><li><p>get 方法一般用于请求，比如你在浏览器地址栏输入 <code>www.cxuanblog.com</code> 其实就是发送了一个 get 请求，它的主要特征是请求服务器返回资源，而 post 方法一般用于``</p><p><code>表单</code>的提交，相当于是把信息提交给服务器，等待服务器作出响应，get 相当于一个是 pull/拉的操作，而 post 相当于是一个 push/推的操作。</p></li><li><p>get 方法是不安全的，因为你在发送请求的过程中，你的请求参数会拼在 URL 后面，从而导致容易被攻击者窃取，对你的信息造成破坏和伪造；</p></li></ul><pre><code class="hljs ceylon">/test/demo<span class="hljs-number">_f</span>orm.asp?name<span class="hljs-number">1</span>=<span class="hljs-keyword">value</span><span class="hljs-number">1</span>&amp;name<span class="hljs-number">2</span>=<span class="hljs-keyword">value</span><span class="hljs-number">2</span></code></pre><p>而 post 方法是把参数放在请求体 body 中的，这对用户来说不可见。</p><pre><code class="hljs dts">POST <span class="hljs-meta-keyword">/test/</span>demo_form.asp HTTP/<span class="hljs-number">1.1</span><span class="hljs-symbol">Host:</span> w3schools.comname1=value1<span class="hljs-variable">&amp;name2</span>=value2</code></pre><ul><li>get 请求的 URL 有长度限制，而 post 请求会把参数和值放在消息体中，对数据长度没有要求。</li><li>get 请求会被浏览器主动 cache，而 post 不会，除非手动设置。</li><li>get 请求在浏览器反复的 <code>回退/前进</code> 操作是无害的，而 post 操作会再次提交表单请求。</li><li>get 请求在发送过程中会产生一个 TCP 数据包；post 在发送过程中会产生两个 TCP 数据包。对于 get 方式的请求，浏览器会把 http header 和 data 一并发送出去，服务器响应 200（返回数据）；而对于 post，浏览器先发送 header，服务器响应 100 continue，浏览器再发送 data，服务器响应 200 ok（返回数据）。</li></ul><h1 id="什么是无状态协议，HTTP-是无状态协议吗，怎么解决"><a href="#什么是无状态协议，HTTP-是无状态协议吗，怎么解决" class="headerlink" title="什么是无状态协议，HTTP 是无状态协议吗，怎么解决"></a>什么是无状态协议，HTTP 是无状态协议吗，怎么解决</h1><p><code>无状态协议(Stateless Protocol)</code> 就是指<strong>浏览器对于事务的处理没有记忆能力</strong>。举个例子来说就是比如客户请求获得网页之后关闭浏览器，然后再次启动浏览器，登录该网站，但是服务器并不知道客户关闭了一次浏览器。</p><p>HTTP 就是一种无状态的协议，他对用户的操作没有记忆能力。可能大多数用户不相信，他可能觉得每次输入用户名和密码登陆一个网站后，下次登陆就不再重新输入用户名和密码了。这其实不是 HTTP 做的事情，起作用的是一个叫做 <code>小甜饼(Cookie)</code> 的机制。它能够让浏览器具有<code>记忆</code>能力。</p><p>如果你的浏览器允许 cookie 的话，查看方式 <strong>chrome://settings/content/cookies</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/chrome-cookies-set.png" srcset="/img/loading.gif" alt="chrome-cookies-set"></p><p>也就说明你的记忆芯片通电了…… 当你向服务端发送请求时，服务端会给你发送一个认证信息，服务器第一次接收到请求时，开辟了一块 Session 空间（创建了Session对象），同时生成一个 sessionId ，并通过响应头的 Set-Cookie：JSESSIONID=XXXXXXX 命令，向客户端发送要求设置 Cookie 的响应；客户端收到响应后，在本机客户端设置了一个 JSESSIONID=XXXXXXX 的 Cookie 信息，该 Cookie 的过期时间为浏览器会话结束；</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/cookies-procedure.png" srcset="/img/loading.gif" alt="cookies-procedure"></p><p>接下来客户端每次向同一个网站发送请求时，请求头都会带上该 Cookie信息（包含 sessionId ）， 然后，服务器通过读取请求头中的 Cookie 信息，获取名称为 JSESSIONID 的值，得到此次请求的 sessionId。这样，你的浏览器才具有了记忆能力。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/cookies-session.png" srcset="/img/loading.gif" alt="cookies-session"></p><p>还有一种方式是使用 JWT 机制，它也是能够让你的浏览器具有记忆能力的一种机制。与 Cookie 不同，JWT 是保存在客户端的信息，它广泛的应用于单点登录的情况。JWT 具有两个特点</p><ul><li>JWT 的 Cookie 信息存储在<code>客户端</code>，而不是服务端内存中。也就是说，JWT 直接本地进行验证就可以，验证完毕后，这个 Token 就会在 Session 中随请求一起发送到服务器，通过这种方式，可以节省服务器资源，并且 token 可以进行多次验证。</li><li>JWT 支持跨域认证，Cookies 只能用在<code>单个节点的域</code>或者它的<code>子域</code>中有效。如果它们尝试通过第三个节点访问，就会被禁止。使用 JWT 可以解决这个问题，使用 JWT 能够通过<code>多个节点</code>进行用户认证，也就是我们常说的<code>跨域认证</code>。</li></ul><h1 id="UDP-和-TCP-的区别"><a href="#UDP-和-TCP-的区别" class="headerlink" title="UDP 和 TCP 的区别"></a>UDP 和 TCP 的区别</h1><p>TCP 和 UDP 都位于计算机网络模型中的运输层，它们负责传输应用层产生的数据。下面我们就来聊一聊 TCP 和 UDP 分别的特征和他们的区别</p><h2 id="UDP-是什么"><a href="#UDP-是什么" class="headerlink" title="UDP 是什么"></a>UDP 是什么</h2><p>UDP 的全称是 <code>User Datagram Protocol</code>，用户数据报协议。它不需要所谓的<code>握手</code>操作，从而加快了通信速度，允许网络上的其他主机在接收方同意通信之前进行数据传输。</p><blockquote><p>数据报是与分组交换网络关联的传输单元。</p></blockquote><p>UDP 的特点主要有</p><ul><li>UDP 能够支持容忍数据包丢失的带宽密集型应用程序</li><li>UDP 具有低延迟的特点</li><li>UDP 能够发送大量的数据包</li><li>UDP 能够允许 DNS 查找，DNS 是建立在 UDP 之上的应用层协议。</li></ul><h2 id="TCP-是什么"><a href="#TCP-是什么" class="headerlink" title="TCP 是什么"></a>TCP 是什么</h2><p>TCP 的全称是<code>Transmission Control Protocol</code> ，传输控制协议。它能够帮助你确定计算机连接到 Internet 以及它们之间的数据传输。通过三次握手来建立 TCP 连接，三次握手就是用来启动和确认 TCP 连接的过程。一旦连接建立后，就可以发送数据了，当数据传输完成后，会通过关闭虚拟电路来断开连接。</p><p>TCP 的主要特点有</p><ul><li>TCP 能够确保连接的建立和数据包的发送</li><li>TCP 支持错误重传机制</li><li>TCP 支持拥塞控制，能够在网络拥堵的情况下延迟发送</li><li>TCP 能够提供错误校验和，甄别有害的数据包。</li></ul><h2 id="TCP-和-UDP-的不同"><a href="#TCP-和-UDP-的不同" class="headerlink" title="TCP 和 UDP 的不同"></a>TCP 和 UDP 的不同</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tcp-vs-udp.png" srcset="/img/loading.gif" alt="tcp-vs-udp"></p><p>下面为你罗列了一些 TCP 和 UDP 的不同点，方便理解，方便记忆。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tcp-vs-udp2.png" srcset="/img/loading.gif" alt="tcp-vs-udp2"></p><h2 id="TCP-三次握手和四次挥手"><a href="#TCP-三次握手和四次挥手" class="headerlink" title="TCP 三次握手和四次挥手"></a>TCP 三次握手和四次挥手</h2><p>TCP 三次握手和四次挥手也是面试题的热门考点，它们分别对应 TCP 的连接和释放过程。下面就来简单认识一下这两个过程</p><h3 id="TCP-三次握手"><a href="#TCP-三次握手" class="headerlink" title="TCP 三次握手"></a>TCP 三次握手</h3><p>在了解具体的流程前，我们需要先认识几个概念</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tcp-signals.png" srcset="/img/loading.gif" alt="tcp-signals"></p><ul><li>SYN：它的全称是 <code>Synchronize Sequence Numbers</code>，同步序列编号。是 TCP/IP 建立连接时使用的握手信号。在客户机和服务器之间建立 TCP 连接时，首先会发送的一个信号。客户端在接受到 SYN 消息时，就会在自己的段内生成一个随机值 X。</li><li>SYN-ACK：服务器收到 SYN 后，打开客户端连接，发送一个 SYN-ACK 作为答复。确认号设置为比接收到的序列号多一个，即 X + 1，服务器为数据包选择的序列号是另一个随机数 Y。</li><li>ACK：<code>Acknowledge character</code>, 确认字符，表示发来的数据已确认接收无误。最后，客户端将 ACK 发送给服务器。序列号被设置为所接收的确认值即 Y + 1。</li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tcp-three-hand.png" srcset="/img/loading.gif" alt="tcp-three-hand"></p><p>如果用现实生活来举例的话就是</p><p>小明 - 客户端 小红 - 服务端</p><ul><li>小明给小红打电话，接通了后，小明说<strong>喂，能听到吗</strong>，这就相当于是连接建立。</li><li>小红给小明回应，<strong>能听到，你能听到我说的话吗</strong>，这就相当于是请求响应。</li><li>小明听到小红的回应后，<strong>好的</strong>，这相当于是连接确认。在这之后小明和小红就可以通话/交换信息了。</li></ul><h3 id="TCP-四次挥手"><a href="#TCP-四次挥手" class="headerlink" title="TCP 四次挥手"></a>TCP 四次挥手</h3><p>在连接终止阶段使用四次挥手，连接的每一端都会独立的终止。下面我们来描述一下这个过程。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tcp-four-hand.png" srcset="/img/loading.gif" alt="tcp-four-hand"></p><ul><li>首先，客户端应用程序决定要终止连接(这里服务端也可以选择断开连接)。这会使客户端将 FIN 发送到服务器，并进入 <code>FIN_WAIT_1</code> 状态。当客户端处于 FIN_WAIT_1 状态时，它会等待来自服务器的 ACK 响应。</li><li>然后第二步，当服务器收到 FIN 消息时，服务器会立刻向客户端发送 ACK 确认消息。</li><li>当客户端收到服务器发送的 ACK 响应后，客户端就进入 <code>FIN_WAIT_2</code> 状态，然后等待来自服务器的 <code>FIN</code> 消息</li><li>服务器发送 ACK 确认消息后，一段时间（可以进行关闭后）会发送 FIN 消息给客户端，告知客户端可以进行关闭。</li><li>当客户端收到从服务端发送的 FIN 消息时，客户端就会由 FIN_WAIT_2 状态变为 <code>TIME_WAIT</code> 状态。处于 TIME_WAIT 状态的客户端允许重新发送 ACK 到服务器为了防止信息丢失。客户端在 TIME_WAIT 状态下花费的时间取决于它的实现，在等待一段时间后，连接关闭，客户端上所有的资源（包括端口号和缓冲区数据）都被释放。</li></ul><p>还是可以用上面那个通话的例子来进行描述</p><ul><li>小明对小红说，我所有的东西都说完了，我要挂电话了。</li><li>小红说，收到，我这边还有一些东西没说。</li><li>经过若干秒后，小红也说完了，小红说，我说完了，现在可以挂断了</li><li>小明收到消息后，又等了若干时间后，挂断了电话。</li></ul><h1 id="简述-HTTP1-0-1-1-2-0-的区别"><a href="#简述-HTTP1-0-1-1-2-0-的区别" class="headerlink" title="简述 HTTP1.0/1.1/2.0 的区别"></a>简述 HTTP1.0/1.1/2.0 的区别</h1><h2 id="HTTP-1-0"><a href="#HTTP-1-0" class="headerlink" title="HTTP 1.0"></a>HTTP 1.0</h2><p>HTTP 1.0 是在 1996 年引入的，从那时开始，它的普及率就达到了惊人的效果。</p><ul><li>HTTP 1.0 仅仅提供了最基本的认证，这时候用户名和密码还未经加密，因此很容易收到窥探。</li><li>HTTP 1.0 被设计用来使用短链接，即每次发送数据都会经过 TCP 的三次握手和四次挥手，效率比较低。</li><li>HTTP 1.0 只使用 header 中的 If-Modified-Since 和 Expires 作为缓存失效的标准。</li><li>HTTP 1.0 不支持断点续传，也就是说，每次都会传送全部的页面和数据。</li><li>HTTP 1.0 认为每台计算机只能绑定一个 IP，所以请求消息中的 URL 并没有传递主机名（hostname）。</li></ul><h2 id="HTTP-1-1"><a href="#HTTP-1-1" class="headerlink" title="HTTP 1.1"></a>HTTP 1.1</h2><p>HTTP 1.1 是 HTTP 1.0 开发三年后出现的，也就是 1999 年，它做出了以下方面的变化</p><ul><li>HTTP 1.1 使用了摘要算法来进行身份验证</li><li>HTTP 1.1 默认使用长连接，长连接就是只需一次建立就可以传输多次数据，传输完成后，只需要一次切断连接即可。长连接的连接时长可以通过请求头中的 <code>keep-alive</code> 来设置</li><li>HTTP 1.1 中新增加了 E-tag，If-Unmodified-Since, If-Match, If-None-Match 等缓存控制标头来控制缓存失效。</li><li>HTTP 1.1 支持断点续传，通过使用请求头中的 <code>Range</code> 来实现。</li><li>HTTP 1.1 使用了虚拟网络，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。</li></ul><h2 id="HTTP-2-0"><a href="#HTTP-2-0" class="headerlink" title="HTTP 2.0"></a>HTTP 2.0</h2><p>HTTP 2.0 是 2015 年开发出来的标准，它主要做的改变如下</p><ul><li><code>头部压缩</code>，由于 HTTP 1.1 经常会出现 <strong>User-Agent、Cookie、Accept、Server、Range</strong> 等字段可能会占用几百甚至几千字节，而 Body 却经常只有几十字节，所以导致头部偏重。HTTP 2.0 使用 <code>HPACK</code> 算法进行压缩。</li><li><code>二进制格式</code>，HTTP 2.0 使用了更加靠近 TCP/IP 的二进制格式，而抛弃了 ASCII 码，提升了解析效率</li><li><code>强化安全</code>，由于安全已经成为重中之重，所以 HTTP2.0 一般都跑在 HTTPS 上。</li><li><code>多路复用</code>，即每一个请求都是是用作连接共享。一个请求对应一个id，这样一个连接上可以有多个请求。</li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/http1-vs-http2.png" srcset="/img/loading.gif" alt="http1.1-vs-http2"></p><h1 id="请你说一下-HTTP-常见的请求头"><a href="#请你说一下-HTTP-常见的请求头" class="headerlink" title="请你说一下 HTTP 常见的请求头"></a>请你说一下 HTTP 常见的请求头</h1><p>这个问题比较开放，因为 HTTP 请求头有很多，这里只简单举出几个例子。</p><p>HTTP 标头会分为四种，分别是 <code>通用标头</code>、<code>实体标头</code>、<code>请求标头</code>、<code>响应标头</code>。分别介绍一下</p><h2 id="通用标头"><a href="#通用标头" class="headerlink" title="通用标头"></a>通用标头</h2><p>通用标头主要有三个，分别是 <code>Date</code>、<code>Cache-Control</code> 和 <code>Connection</code></p><p><strong>Date</strong></p><p>Date 是一个通用标头，它可以出现在请求标头和响应标头中，它的基本表示如下</p><pre><code class="hljs angelscript">Date: Wed, <span class="hljs-number">21</span> Oct <span class="hljs-number">2015</span> <span class="hljs-number">07</span>:<span class="hljs-number">28</span>:<span class="hljs-number">00</span> GMT</code></pre><p>表示的是格林威治标准时间，这个时间要比北京时间慢八个小时</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/timezone-transform.png" srcset="/img/loading.gif" alt="timezone-transform"></p><p><strong>Cache-Control</strong></p><p>Cache-Control 是一个通用标头，他可以出现在<code>请求标头</code>和<code>响应标头</code>中，Cache-Control 的种类比较多，虽然说这是一个通用标头，但是有一些特性是请求标头具有的，有一些是响应标头才有的。主要大类有 <code>可缓存性</code>、<code>阈值性</code>、 <code>重新验证并重新加载</code> 和<code>其他特性</code></p><p><strong>Connection</strong></p><p>Connection 决定当前事务（一次三次握手和四次挥手）完成后，是否会关闭网络连接。Connection 有两种，一种是<code>持久性连接</code>，即一次事务完成后不关闭网络连接</p><pre><code class="hljs armasm"><span class="hljs-symbol">Connection</span>: <span class="hljs-meta">keep</span>-alive</code></pre><p>另一种是<code>非持久性连接</code>，即一次事务完成后关闭网络连接</p><pre><code class="hljs pgsql"><span class="hljs-keyword">Connection</span>: <span class="hljs-keyword">close</span></code></pre><p>HTTP1.1 其他通用标头如下</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/http1-1-head-content.png" srcset="/img/loading.gif" alt="http1-1-head-content"></p><h2 id="实体标头"><a href="#实体标头" class="headerlink" title="实体标头"></a>实体标头</h2><p>实体标头是描述消息正文内容的 HTTP 标头。实体标头用于 HTTP 请求和响应中。头部<code>Content-Length</code>、 <code>Content-Language</code>、 <code>Content-Encoding</code> 是实体头。</p><ul><li><p>Content-Length 实体报头指示实体主体的大小，以字节为单位，发送到接收方。</p></li><li><p>Content-Language 实体报头描述了客户端或者服务端能够接受的语言。</p></li><li><p>Content-Encoding 这又是一个比较麻烦的属性，这个实体报头用来压缩媒体类型。Content-Encoding 指示对实体应用了何种编码。</p><p>常见的内容编码有这几种： <strong>gzip、compress、deflate、identity</strong> ，这个属性可以应用在请求报文和响应报文中</p></li></ul><pre><code class="hljs groovy">Accept-<span class="hljs-string">Encoding:</span> gzip, deflate <span class="hljs-comment">//请求头</span>Content-<span class="hljs-string">Encoding:</span> gzip  <span class="hljs-comment">//响应头</span></code></pre><p>下面是一些实体标头字段</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/entity-header-column.png" srcset="/img/loading.gif" alt="entity-header-column"></p><h2 id="请求标头"><a href="#请求标头" class="headerlink" title="请求标头"></a>请求标头</h2><p><strong>Host</strong></p><p>Host 请求头指明了服务器的域名（对于虚拟主机来说），以及（可选的）服务器监听的 TCP 端口号。如果没有给定端口号，会自动使用被请求服务的默认端口（比如请求一个 HTTP 的 URL 会自动使用 80 作为端口）。</p><pre><code class="hljs avrasm"><span class="hljs-symbol">Host:</span> developer.mozilla<span class="hljs-meta">.org</span></code></pre><p>上面的 <code>Accpet</code>、 <code>Accept-Language</code>、<code>Accept-Encoding</code> 都是属于内容协商的请求标头。</p><p><strong>Referer</strong></p><p>HTTP Referer 属性是请求标头的一部分，当浏览器向 web 服务器发送请求的时候，一般会带上 Referer，告诉服务器该网页是从哪个页面链接过来的，服务器因此可以获得一些信息用于处理。</p><pre><code class="hljs groovy"><span class="hljs-string">Referer:</span> <span class="hljs-string">https:</span><span class="hljs-comment">//developer.mozilla.org/testpage.html</span></code></pre><p><strong>If-Modified-Since</strong></p><p>If-Modified-Since 通常会与 If-None-Match 搭配使用，If-Modified-Since 用于确认代理或客户端拥有的本地资源的有效性。获取资源的更新日期时间，可通过确认首部字段 <code>Last-Modified</code> 来确定。</p><p>大白话说就是如果在 <code>Last-Modified</code> 之后更新了服务器资源，那么服务器会响应 200，如果在 <code>Last-Modified</code> 之后没有更新过资源，则返回 304。</p><pre><code class="hljs angelscript">If-Modified-Since: Mon, <span class="hljs-number">18</span> Jul <span class="hljs-number">2016</span> <span class="hljs-number">02</span>:<span class="hljs-number">36</span>:<span class="hljs-number">04</span> GMT</code></pre><p><strong>If-None-Match</strong></p><p>If-None-Match HTTP 请求标头使请求成为条件请求。对于 GET 和 HEAD 方法，仅当服务器没有与给定资源匹配的 <code>ETag</code> 时，服务器才会以 200 状态发送回请求的资源。对于其他方法，仅当最终现有资源的<code>ETag</code>与列出的任何值都不匹配时，才会处理请求。</p><pre><code class="hljs dart">If-None-<span class="hljs-built_in">Match</span>: <span class="hljs-string">"c561c68d0ba92bbeb8b0fff2a9199f722e3a621a"</span></code></pre><p><strong>Accept</strong></p><p>接受请求 HTTP 标头会通告客户端其能够理解的 MIME 类型</p><p><strong>Accept-Charset</strong></p><p>accept-charset 属性规定服务器处理表单数据所接受的字符集。</p><p>常用的字符集有：UTF-8 - Unicode 字符编码 ；ISO-8859-1 - 拉丁字母表的字符编码</p><p><strong>Accept-Language</strong></p><p>首部字段 Accept-Language 用来告知服务器用户代理能够处理的自然语言集（指中文或英文等），以及自然语言集的相对优先级。可一次指定多种自然语言集。</p><p>请求标头我们大概就介绍这几种，后面会有一篇文章详细深挖所有的响应头的，下面是一个响应头的汇总，基于 HTTP 1.1</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/http1-1-request-header.png" srcset="/img/loading.gif" alt="http1-1-request-header"></p><h2 id="响应标头"><a href="#响应标头" class="headerlink" title="响应标头"></a>响应标头</h2><p><strong>Access-Control-Allow-Origin</strong></p><p>一个返回的 HTTP 标头可能会具有 Access-Control-Allow-Origin ，<code>Access-Control-Allow-Origin</code> 指定一个来源，它告诉浏览器允许该来源进行资源访问。</p><p><strong>Keep-Alive</strong></p><p>Keep-Alive 表示的是 Connection 非持续连接的存活时间，可以进行指定。</p><p><strong>Server</strong></p><p>服务器标头包含有关原始服务器用来处理请求的软件的信息。</p><p>应该避免使用过于冗长和详细的 Server 值，因为它们可能会泄露内部实施细节，这可能会使攻击者容易地发现并利用已知的安全漏洞。例如下面这种写法</p><pre><code class="hljs angelscript">Server: Apache/<span class="hljs-number">2.4</span><span class="hljs-number">.1</span> (Unix)</code></pre><p><strong>Set-Cookie</strong></p><p>Set-Cookie 用于服务器向客户端发送 sessionID。</p><p><strong>Transfer-Encoding</strong></p><p>首部字段 Transfer-Encoding 规定了传输报文主体时采用的编码方式。</p><p>HTTP /1.1 的传输编码方式仅对分块传输编码有效。</p><p><strong>X-Frame-Options</strong></p><p>HTTP 首部字段是可以自行扩展的。所以在 Web 服务器和浏览器的应用上，会出现各种非标准的首部字段。</p><p>首部字段 <code>X-Frame-Options</code> 属于 HTTP 响应首部，用于控制网站内容在其他 Web 网站的 Frame 标签内的显示问题。其主要目的是为了防止点击劫持（clickjacking）攻击。</p><p>下面是一个响应头的汇总，基于 HTTP 1.1</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/http1-1-response-header.png" srcset="/img/loading.gif" alt="http1-1-response-header"></p><h1 id="地址栏输入-URL-发生了什么"><a href="#地址栏输入-URL-发生了什么" class="headerlink" title="地址栏输入 URL 发生了什么"></a>地址栏输入 URL 发生了什么</h1><p>这道题也是一道经常会考的面试题。那么下面我们就来探讨一下从你输入 URL 后到响应，都经历了哪些过程。</p><ul><li>首先，你需要在浏览器中的 URL 地址上，输入你想访问的地址，如下</li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/chrome-search.png" srcset="/img/loading.gif" alt="chrome-search"></p><p>你应该访问不到的，对不对~</p><ul><li>然后，浏览器会根据你输入的 URL 地址，去查找域名是否被本地 DNS 缓存，不同浏览器对 DNS 的设置不同，如果浏览器缓存了你想访问的 URL 地址，那就直接返回 ip。如果没有缓存你的 URL 地址，浏览器就会发起系统调用来查询本机 <code>hosts</code> 文件是否有配置 ip 地址，如果找到，直接返回。如果找不到，就向网络中发起一个 DNS 查询。</li></ul><blockquote><p>首先来看一下 DNS 是啥，互联网中识别主机的方式有两种，通过<code>主机名</code>和 <code>IP 地址</code>。我们人喜欢用名字的方式进行记忆，但是通信链路中的路由却喜欢定长、有层次结构的 IP 地址。所以就需要一种能够把主机名到 IP 地址的转换服务，这种服务就是由 DNS 提供的。DNS 的全称是 <code>Domain Name System</code> 域名系统。DNS 是一种由分层的 DNS 服务器实现的分布式数据库。DNS 运行在 UDP 上，使用 53 端口。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/dns1.png" srcset="/img/loading.gif" alt=""></p></blockquote><p>DNS 是一种分层数据库，它的主要层次结构如下</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/dns2.png" srcset="/img/loading.gif" alt="dns2"></p><p>一般域名服务器的层次结构主要是以上三种，除此之外，还有另一类重要的 DNS 服务器，它是 <code>本地 DNS 服务器(local DNS server)</code>。严格来说，本地 DNS 服务器并不属于上述层次结构，但是本地 DNS 服务器又是至关重要的。每个 <code>ISP(Internet Service Provider)</code> 比如居民区的 ISP 或者一个机构的 ISP 都有一台本地 DNS 服务器。当主机和 ISP 进行连接时，该 ISP 会提供一台主机的 IP 地址，该主机会具有一台或多台其本地 DNS 服务器的 IP地址。通过访问网络连接，用户能够容易的确定 DNS 服务器的 IP地址。当主机发出 DNS 请求后，该请求被发往本地 DNS 服务器，它起着代理的作用，并将该请求转发到 DNS 服务器层次系统中。</p><p>首先，查询请求会先找到本地 DNS 服务器来查询是否包含 IP 地址，如果本地 DNS 无法查询到目标 IP 地址，就会向根域名服务器发起一个 DNS 查询。</p><blockquote><p>注意：DNS 涉及两种查询方式：一种是<code>递归查询(Recursive query)</code> ，一种是<code>迭代查询(Iteration query)</code>。《计算机网络：自顶向下方法》竟然没有给出递归查询和迭代查询的区别，找了一下网上的资料大概明白了下。<br>如果根域名服务器无法告知本地 DNS 服务器下一步需要访问哪个顶级域名服务器，就会使用递归查询；<br>如果根域名服务器能够告知 DNS 服务器下一步需要访问的顶级域名服务器，就会使用迭代查询。</p></blockquote><p>在由根域名服务器 -&gt; 顶级域名服务器 -&gt; 权威 DNS 服务器后，由权威服务器告诉本地服务器目标 IP 地址，再有本地 DNS 服务器告诉用户需要访问的 IP 地址。</p><ul><li>第三步，浏览器需要和目标服务器建立 TCP 连接，需要经过三次握手的过程，具体的握手过程请参考上面的回答。</li><li>在建立连接后，浏览器会向目标服务器发起 <code>HTTP-GET</code> 请求，包括其中的 URL，HTTP 1.1 后默认使用长连接，只需要一次握手即可多次传输数据。</li><li>如果目标服务器只是一个简单的页面，就会直接返回。但是对于某些大型网站的站点，往往不会直接返回主机名所在的页面，而会直接重定向。返回的状态码就不是 200 ，而是 301,302 以 3 开头的重定向码，浏览器在获取了重定向响应后，在响应报文中 Location 项找到重定向地址，浏览器重新第一步访问即可。</li><li>然后浏览器重新发送请求，携带新的 URL，返回状态码 200 OK，表示服务器可以响应请求，返回报文。</li></ul><h1 id="HTTPS-的工作原理"><a href="#HTTPS-的工作原理" class="headerlink" title="HTTPS 的工作原理"></a>HTTPS 的工作原理</h1><p>我们上面描述了一下 HTTP 的工作原理，下面来讲述一下 HTTPS 的工作原理。因为我们知道 HTTPS 不是一种新出现的协议，而是</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/https-compose.png" srcset="/img/loading.gif" alt="https-compose"></p><p>所以，我们探讨 HTTPS 的握手过程，其实就是 SSL/TLS 的握手过程。</p><p>TLS 旨在为 Internet 提供通信安全的加密协议。TLS 握手是启动和使用 TLS 加密的通信会话的过程。在 TLS 握手期间，Internet 中的通信双方会彼此交换信息，验证密码套件，交换会话密钥。</p><p>每当用户通过 HTTPS 导航到具体的网站并发送请求时，就会进行 TLS 握手。除此之外，每当其他任何通信使用HTTPS（包括 API 调用和在 HTTPS 上查询 DNS）时，也会发生 TLS 握手。</p><p>TLS 具体的握手过程会根据所使用的<code>密钥交换算法的类型</code>和双方支持的<code>密码套件</code>而不同。我们以<code>RSA 非对称加密</code>来讨论这个过程。整个 TLS 通信流程图如下</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-05-19/tls-procedure.png" srcset="/img/loading.gif" alt="tls-procedure"></p><ul><li>在进行通信前，首先会进行 HTTP 的三次握手，握手完成后，再进行 TLS 的握手过程</li><li>ClientHello：客户端通过向服务器发送 <code>hello</code> 消息来发起握手过程。这个消息中会夹带着客户端支持的 <code>TLS 版本号(TLS1.0 、TLS1.2、TLS1.3)</code> 、客户端支持的密码套件、以及一串 <code>客户端随机数</code>。</li><li>ServerHello：在客户端发送 hello 消息后，服务器会发送一条消息，这条消息包含了服务器的 SSL 证书、服务器选择的密码套件和服务器生成的随机数。</li><li>认证(Authentication)：客户端的证书颁发机构会认证 SSL 证书，然后发送 <code>Certificate</code> 报文，报文中包含公开密钥证书。最后服务器发送 <code>ServerHelloDone</code> 作为 <code>hello</code> 请求的响应。第一部分握手阶段结束。</li><li><code>加密阶段</code>：在第一个阶段握手完成后，客户端会发送 <code>ClientKeyExchange</code> 作为响应，这个响应中包含了一种称为 <code>The premaster secret</code> 的密钥字符串，这个字符串就是使用上面公开密钥证书进行加密的字符串。随后客户端会发送 <code>ChangeCipherSpec</code>，告诉服务端使用私钥解密这个 <code>premaster secret</code> 的字符串，然后客户端发送 <code>Finished</code> 告诉服务端自己发送完成了。</li></ul><blockquote><p>Session key 其实就是用公钥证书加密的公钥。</p></blockquote><ul><li><code>实现了安全的非对称加密</code>：然后，服务器再发送 <code>ChangeCipherSpec</code> 和 <code>Finished</code> 告诉客户端解密完成，至此实现了 RSA 的非对称加密。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>HTTP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 详解 TCP 连接的“三次握手”与“四次挥手” </title>
    <link href="/2020/01/19/2020-01-19-tcp-second-talk/"/>
    <url>/2020/01/19/2020-01-19-tcp-second-talk/</url>
    
    <content type="html"><![CDATA[<h1 id="TCP-connection"><a href="#TCP-connection" class="headerlink" title="TCP connection"></a>TCP connection</h1><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/1.png" srcset="/img/loading.gif" alt="1.png"></p><p><img src="https://image.my-blog.wang/2020-01-19-tcp-second-talk/2.png" srcset="/img/loading.gif" alt="2.png"></p><p>客户端与服务器之间数据的发送和返回的过程当中需要创建一个叫TCP connection的东西；</p><p>由于TCP不存在连接的概念，只存在请求和响应，请求和响应都是数据包，它们之间都是经过由TCP创建的一个从客户端发起，服务器接收的类似连接的通道，这个连接可以一直保持，http请求是在这个连接的基础上发送的；</p><p>在一个TCP连接上是可以发送多个http请求的，不同的版本这个模式不一样。</p><p>在HTTP/1.0中这个TCP连接是在http请求创建的时候同步创建的，http请求发送到服务器端，服务器端响应了之后，这个TCP连接就关闭了；</p><p>HTTP/1.1中可以以某种方式声明这个连接一直保持，一个请求传输完之后，另一个请求可以接着传输。这样的好处是：在创建一个TCP连接的过程中需要“三次握手”的消耗，“三次握手”代表有三次网络传输。</p><p>如果TCP连接保持，第二个请求发送就没有这“三次握手”的消耗。HTTP/2中同一个TCP连接里还可以并发地传输http请求。</p><h1 id="TCP报文格式简介"><a href="#TCP报文格式简介" class="headerlink" title="TCP报文格式简介"></a>TCP报文格式简介</h1><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/3.png" srcset="/img/loading.gif" alt=""></p><p>其中比较重要的字段有：</p><p>（1）序号（sequence number）：Seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记。</p><p>（2）确认号（acknowledgement number）：Ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，Ack=Seq+1。</p><p>（3）标志位（Flags）：共6个，即URG、ACK、PSH、RST、SYN、FIN等。具体含义如下：</p><blockquote><ul><li>URG：紧急指针（urgent pointer）有效。</li><li>ACK：确认序号有效。</li><li>PSH：接收方应该尽快将这个报文交给应用层。</li><li>RST：重置连接。</li><li>SYN：发起一个新连接。</li><li>FIN：释放一个连接。</li></ul></blockquote><p>需要注意的是：</p><blockquote><p>不要将确认序号Ack与标志位中的ACK搞混了。<br>确认方Ack=发起方Seq+1，两端配对。</p></blockquote><h1 id="TCP的三次握手（Three-Way-Handshake）"><a href="#TCP的三次握手（Three-Way-Handshake）" class="headerlink" title="TCP的三次握手（Three-Way Handshake）"></a>TCP的三次握手（Three-Way Handshake）</h1><h2 id="1-“三次握手”的详解"><a href="#1-“三次握手”的详解" class="headerlink" title="1.“三次握手”的详解"></a>1.“三次握手”的详解</h2><p>所谓的三次握手即TCP连接的建立。这个连接必须是一方主动打开，另一方被动打开的。<br>以下为客户端主动发起连接的图解：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/4.png" srcset="/img/loading.gif" alt=""></p><p>握手之前主动打开连接的客户端结束CLOSED阶段，被动打开的服务器端也结束CLOSED阶段，并进入LISTEN阶段。随后开始“三次握手”：</p><p>（1）首先客户端向服务器端发送一段TCP报文，其中：</p><ul><li>标记位为SYN，表示“请求建立新连接”;</li><li>序号为Seq=X（X一般为1）；</li><li>随后客户端进入SYN-SENT阶段。</li></ul><p>（2）服务器端接收到来自客户端的TCP报文之后，结束LISTEN阶段。并返回一段TCP报文，其中：</p><ul><li>标志位为SYN和ACK，表示“确认客户端的报文Seq序号有效，服务器能正常接收客户端发送的数据，并同意创建新连接”（即告诉客户端，服务器收到了你的数据）；</li><li>序号为Seq=y；</li><li>确认号为Ack=x+1，表示收到客户端的序号Seq并将其值加1作为自己确认号Ack的值；随后服务器端进入SYN-RCVD阶段。</li></ul><p>（3）客户端接收到来自服务器端的确认收到数据的TCP报文之后，明确了从客户端到服务器的数据传输是正常的，结束SYN-SENT阶段。并返回最后一段TCP报文。其中：</p><ul><li>标志位为ACK，表示“确认收到服务器端同意连接的信号”（即告诉服务器，我知道你收到我发的数据了）；</li><li>序号为Seq=x+1，表示收到服务器端的确认号Ack，并将其值作为自己的序号值；</li><li>确认号为Ack=y+1，表示收到服务器端序号Seq，并将其值加1作为自己的确认号Ack的值；</li><li>随后客户端进入ESTABLISHED阶段。</li></ul><p>服务器收到来自客户端的“确认收到服务器数据”的TCP报文之后，明确了从服务器到客户端的数据传输是正常的。结束SYN-SENT阶段，进入ESTABLISHED阶段。</p><p>在客户端与服务器端传输的TCP报文中，双方的确认号Ack和序号Seq的值，都是在彼此Ack和Seq值的基础上进行计算的，这样做保证了TCP报文传输的连贯性。一旦出现某一方发出的TCP报文丢失，便无法继续”握手”，以此确保了”三次握手”的顺利完成。</p><p>此后客户端和服务器端进行正常的数据传输。这就是“三次握手”的过程。</p><h2 id="2-“三次握手”的动态过程"><a href="#2-“三次握手”的动态过程" class="headerlink" title="2.“三次握手”的动态过程"></a>2.“三次握手”的动态过程</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/5.gif" srcset="/img/loading.gif" alt=""></p><h2 id="3-“三次握手”的通俗理解"><a href="#3-“三次握手”的通俗理解" class="headerlink" title="3.“三次握手”的通俗理解"></a>3.“三次握手”的通俗理解</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/6.png" srcset="/img/loading.gif" alt=""></p><p>举个栗子：把客户端比作男孩，服务器比作女孩。用他们的交往来说明“三次握手”过程：</p><p>（1）男孩喜欢女孩，于是写了一封信告诉女孩：我爱你，请和我交往吧！;写完信之后，男孩焦急地等待，因为不知道信能否顺利传达给女孩。</p><p>（2）女孩收到男孩的情书后，心花怒放，原来我们是两情相悦呀！于是给男孩写了一封回信：我收到你的情书了，也明白了你的心意，其实，我也喜欢你！我愿意和你交往！;</p><p>写完信之后，女孩也焦急地等待，因为不知道回信能否能顺利传达给男孩。</p><p>（3）男孩收到回信之后很开心，因为发出的情书女孩收到了，并且从回信中知道了女孩喜欢自己，并且愿意和自己交往。然后男孩又写了一封信告诉女孩：你的心意和信我都收到了，谢谢你，还有我爱你！</p><p>女孩收到男孩的回信之后，也很开心，因为发出的情书男孩收到了。由此男孩女孩双方都知道了彼此的心意，之后就快乐地交流起来了~~</p><p>这就是通俗版的“三次握手”，期间一共往来了三封信也就是“三次握手”，以此确认两个方向上的数据传输通道是否正常。</p><h2 id="4-为什么要进行第三次握手？"><a href="#4-为什么要进行第三次握手？" class="headerlink" title="4.为什么要进行第三次握手？"></a>4.为什么要进行第三次握手？</h2><p>为了防止服务器端开启一些无用的连接增加服务器开销以及防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。</p><p>由于网络传输是有延时的(要通过网络光纤和各种中间代理服务器)，在传输的过程中，比如客户端发起了SYN=1创建连接的请求(第一次握手)。</p><p>如果服务器端就直接创建了这个连接并返回包含SYN、ACK和Seq等内容的数据包给客户端，这个数据包因为网络传输的原因丢失了，丢失之后客户端就一直没有接收到服务器返回的数据包。</p><p>客户端可能设置了一个超时时间，时间到了就关闭了连接创建的请求。再重新发出创建连接的请求，而服务器端是不知道的，如果没有第三次握手告诉服务器端客户端收的到服务器端传输的数据的话，</p><p>服务器端是不知道客户端有没有接收到服务器端返回的信息的。</p><p>这个过程可理解为：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/7.png" srcset="/img/loading.gif" alt=""></p><p>这样没有给服务器端一个创建还是关闭连接端口的请求，服务器端的端口就一直开着，等到客户端因超时重新发出请求时，服务器就会重新开启一个端口连接。那么服务器端上没有接收到请求数据的上一个端口就一直开着，长此以往，这样的端口多了，就会造成服务器端开销的严重浪费。</p><p>还有一种情况是已经失效的客户端发出的请求信息，由于某种原因传输到了服务器端，服务器端以为是客户端发出的有效请求，接收后产生错误。</p><p>所以我们需要“第三次握手”来确认这个过程，让客户端和服务器端能够及时地察觉到因为网络等一些问题导致的连接创建失败，这样服务器端的端口就可以关闭了不用一直等待。</p><p>也可以这样理解：“第三次握手”是客户端向服务器端发送数据，这个数据就是要告诉服务器，客户端有没有收到服务器“第二次握手”时传过去的数据。若发送的这个数据是“收到了”的信息，接收后服务器就正常建立TCP连接，否则建立TCP连接失败，服务器关闭连接端口。由此减少服务器开销和接收到失效请求发生的错误。</p><h2 id="5-抓包验证"><a href="#5-抓包验证" class="headerlink" title="5.抓包验证"></a>5.抓包验证</h2><p>下面是用抓包工具抓到的一些数据包，可用来分析TCP的三次握手：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/8.png" srcset="/img/loading.gif" alt=""></p><p>图中显示的就是完整的TCP连接的”三次握手”过程。在52528 -&gt; 80中，52528是本地(客户端)端口，80是服务器的端口。80端口和52528端口之间的三次来回就是”三次握手”过程。</p><p>注意到”第一次握手”客户端发送的TCP报文中以[SYN]作为标志位，并且客户端序号Seq=0；</p><p>接下来”第二次握手”服务器返回的TCP报文中以[SYN，ACK]作为标志位；并且服务器端序号Seq=0；确认号Ack=1(“第一次握手”中客户端序号Seq的值+1);</p><p>最后”第三次握手”客户端再向服务器端发送的TCP报文中以[ACK]作为标志位；</p><p>其中客户端序号Seq=1（“第二次握手”中服务器端确认号Ack的值）；确认号Ack=1(“第二次握手”中服务器端序号Seq的值+1)。</p><p>这就完成了”三次握手”的过程，符合前面分析的结果。</p><h1 id="TCP的四次挥手（Four-Way-Wavehand）"><a href="#TCP的四次挥手（Four-Way-Wavehand）" class="headerlink" title="TCP的四次挥手（Four-Way Wavehand）"></a>TCP的四次挥手（Four-Way Wavehand）</h1><h2 id="1、前言"><a href="#1、前言" class="headerlink" title="1、前言"></a>1、前言</h2><p>对于”三次握手”我们耳熟能详，因为其相对的简单。但是，我们却不常听见“四次挥手”，就算听过也未必能详细地说明白它的具体过程。下面就为大家详尽，直观，完整地介绍“四次挥手”的过程。</p><h2 id="2、“四次挥手”的详解"><a href="#2、“四次挥手”的详解" class="headerlink" title="2、“四次挥手”的详解"></a>2、“四次挥手”的详解</h2><p>所谓的四次挥手即TCP连接的释放(解除)。连接的释放必须是一方主动释放，另一方被动释放。以下为客户端主动发起释放连接的图解：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/9.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>挥手之前主动释放连接的客户端结束ESTABLISHED阶段。随后开始“四次挥手”：</p></blockquote><p>（1）首先客户端想要释放连接，向服务器端发送一段TCP报文，其中：</p><ul><li>标记位为FIN，表示“请求释放连接“；</li><li>序号为Seq=U；</li><li>随后客户端进入FIN-WAIT-1阶段，即半关闭阶段。并且停止在客户端到服务器端方向上发送数据，但是客户端仍然能接收从服务器端传输过来的数据。</li></ul><p>注意：这里不发送的是正常连接时传输的数据(非确认报文)，而不是一切数据，所以客户端仍然能发送ACK确认报文。</p><p>（2）服务器端接收到从客户端发出的TCP报文之后，确认了客户端想要释放连接，随后服务器端结束ESTABLISHED阶段，进入CLOSE-WAIT阶段（半关闭状态）并返回一段TCP报文，其中：</p><ul><li>标记位为ACK，表示“接收到客户端发送的释放连接的请求”；</li><li>序号为Seq=V；</li><li>确认号为Ack=U+1，表示是在收到客户端报文的基础上，将其序号Seq值加1作为本段报文确认号Ack的值；</li><li>随后服务器端开始准备释放服务器端到客户端方向上的连接。</li></ul><p>客户端收到从服务器端发出的TCP报文之后，确认了服务器收到了客户端发出的释放连接请求，随后客户端结束FIN-WAIT-1阶段，进入FIN-WAIT-2阶段</p><blockquote><p>前”两次挥手”既让服务器端知道了客户端想要释放连接，也让客户端知道了服务器端了解了自己想要释放连接的请求。于是，可以确认关闭客户端到服务器端方向上的连接了</p></blockquote><p>（3）服务器端自从发出ACK确认报文之后，经过CLOSED-WAIT阶段，做好了释放服务器端到客户端方向上的连接准备，再次向客户端发出一段TCP报文，其中：</p><ul><li>标记位为FIN，ACK，表示“已经准备好释放连接了”。注意：这里的ACK并不是确认收到服务器端报文的确认报文。</li><li>序号为Seq=W；</li><li>确认号为Ack=U+1；表示是在收到客户端报文的基础上，将其序号Seq值加1作为本段报文确认号Ack的值。</li></ul><p>随后服务器端结束CLOSE-WAIT阶段，进入LAST-ACK阶段。并且停止在服务器端到客户端的方向上发送数据，但是服务器端仍然能够接收从客户端传输过来的数据。</p><p>（4）客户端收到从服务器端发出的TCP报文，确认了服务器端已做好释放连接的准备，结束FIN-WAIT-2阶段，进入TIME-WAIT阶段，并向服务器端发送一段报文，其中：</p><ul><li>标记位为ACK，表示“接收到服务器准备好释放连接的信号”。</li><li>序号为Seq=U+1；表示是在收到了服务器端报文的基础上，将其确认号Ack值作为本段报文序号的值。</li><li>确认号为Ack=W+1；表示是在收到了服务器端报文的基础上，将其序号Seq值作为本段报文确认号的值。</li></ul><p>随后客户端开始在TIME-WAIT阶段等待2MSL</p><blockquote><p>为什么要客户端要等待2MSL呢？见后文。</p></blockquote><p>服务器端收到从客户端发出的TCP报文之后结束LAST-ACK阶段，进入CLOSED阶段。由此正式确认关闭服务器端到客户端方向上的连接。</p><p>客户端等待完2MSL之后，结束TIME-WAIT阶段，进入CLOSED阶段，由此完成“四次挥手”。</p><blockquote><p>后“两次挥手”既让客户端知道了服务器端准备好释放连接了，也让服务器端知道了客户端了解了自己准备好释放连接了。于是，可以确认关闭服务器端到客户端方向上的连接了，由此完成“四次挥手”。</p></blockquote><p>与“三次挥手”一样，在客户端与服务器端传输的TCP报文中，双方的确认号Ack和序号Seq的值，都是在彼此Ack和Seq值的基础上进行计算的，这样做保证了TCP报文传输的连贯性，一旦出现某一方发出的TCP报文丢失，便无法继续”挥手”，以此确保了”四次挥手”的顺利完成。</p><h2 id="3、“四次挥手”的通俗理解"><a href="#3、“四次挥手”的通俗理解" class="headerlink" title="3、“四次挥手”的通俗理解"></a>3、“四次挥手”的通俗理解</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/10.png" srcset="/img/loading.gif" alt=""></p><blockquote><p>举个栗子：把客户端比作男孩，服务器比作女孩。通过他们的分手来说明“四次挥手”过程。</p></blockquote><ul><li>“第一次挥手”：日久见人心，男孩发现女孩变成了自己讨厌的样子，忍无可忍，于是决定分手，随即写了一封信告诉女孩。</li><li>“第二次挥手”：女孩收到信之后，知道了男孩要和自己分手，怒火中烧，心中暗骂：你算什么东西，当初你可不是这个样子的！于是立马给男孩写了一封回信：分手就分手，给我点时间，我要把你的东西整理好，全部还给你！<br>男孩收到女孩的第一封信之后，明白了女孩知道自己要和她分手。随后等待女孩把自己的东西收拾好。</li><li>“第三次挥手”：过了几天，女孩把男孩送的东西都整理好了，于是再次写信给男孩：你的东西我整理好了，快把它们拿走，从此你我恩断义绝！</li><li>“第四次挥手”：男孩收到女孩第二封信之后，知道了女孩收拾好东西了，可以正式分手了，于是再次写信告诉女孩：我知道了，这就去拿回来！</li></ul><blockquote><p>这里双方都有各自的坚持。</p><ul><li>女孩自发出第二封信开始，限定一天内收不到男孩回信，就会再发一封信催促男孩来取东西！</li><li>男孩自发出第二封信开始，限定两天内没有再次收到女孩的信就认为，女孩收到了自己的第二封信；若两天内再次收到女孩的来信，就认为自己的第二封信女孩没收到，需要再写一封信，再等两天…..</li></ul></blockquote><p>倘若双方信都能正常收到，最少只用四封信就能彻底分手！这就是“四次挥手”。</p><h2 id="4-为什么“握手”是三次，“挥手”却要四次？"><a href="#4-为什么“握手”是三次，“挥手”却要四次？" class="headerlink" title="4.为什么“握手”是三次，“挥手”却要四次？"></a>4.为什么“握手”是三次，“挥手”却要四次？</h2><p>TCP建立连接时之所以只需要”三次握手”，是因为在第二次”握手”过程中，服务器端发送给客户端的TCP报文是以SYN与ACK作为标志位的。SYN是请求连接标志，表示服务器端同意建立连接；ACK是确认报文，表示告诉客户端，服务器端收到了它的请求报文。</p><p>即SYN建立连接报文与ACK确认接收报文是在同一次”握手”当中传输的，所以”三次握手”不多也不少，正好让双方明确彼此信息互通。</p><p>TCP释放连接时之所以需要“四次挥手”,是因为FIN释放连接报文与ACK确认接收报文是分别由第二次和第三次”握手”传输的。为何建立连接时一起传输，释放连接时却要分开传输？</p><blockquote><ul><li>建立连接时，被动方服务器端结束CLOSED阶段进入“握手”阶段并不需要任何准备，可以直接返回SYN和ACK报文，开始建立连接。</li><li>释放连接时，被动方服务器，突然收到主动方客户端释放连接的请求时并不能立即释放连接，因为还有必要的数据需要处理，所以服务器先返回ACK确认收到报文，经过CLOSE-WAIT阶段准备好释放连接之后，才能返回FIN释放连接报文。</li></ul></blockquote><p>所以是“三次握手”，“四次挥手”。</p><h2 id="5-为什么客户端在TIME-WAIT阶段要等2MSL"><a href="#5-为什么客户端在TIME-WAIT阶段要等2MSL" class="headerlink" title="5.为什么客户端在TIME-WAIT阶段要等2MSL?"></a>5.为什么客户端在TIME-WAIT阶段要等2MSL?</h2><p>为的是确认服务器端是否收到客户端发出的ACK确认报文</p><p>当客户端发出最后的ACK确认报文时，并不能确定服务器端能够收到该段报文。所以客户端在发送完ACK确认报文之后，会设置一个时长为2MSL的计时器。MSL指的是Maximum Segment Lifetime：一段TCP报文在传输过程中的最大生命周期。2MSL即是服务器端发出为FIN报文和客户端发出的ACK确认报文所能保持有效的最大时长。</p><p>服务器端在1MSL内没有收到客户端发出的ACK确认报文，就会再次向客户端发出FIN报文；</p><blockquote><ul><li>如果客户端在2MSL内，再次收到了来自服务器端的FIN报文，说明服务器端由于各种原因没有接收到客户端发出的ACK确认报文。客户端再次向服务器端发出ACK确认报文，计时器重置，重新开始2MSL的计时；</li><li>否则客户端在2MSL内没有再次收到来自服务器端的FIN报文，说明服务器端正常接收了ACK确认报文，客户端可以进入CLOSED阶段，完成“四次挥手”。</li></ul></blockquote><p>所以，客户端要经历时长为2SML的TIME-WAIT阶段；这也是为什么客户端比服务器端晚进入CLOSED阶段的原因</p><h2 id="6-抓包验证"><a href="#6-抓包验证" class="headerlink" title="6.抓包验证"></a>6.抓包验证</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-19/11.png" srcset="/img/loading.gif" alt=""></p><p>图中显示的就是完整的TCP连接释放的”四次挥手”过程。在 80 -&gt; 55389 中，假设80是本地(客户端)端口，55389是服务器端口。80端口与55389之间的四次来回就是”四次挥手”过程。</p><ul><li>”第一次挥手”客户端发送的FIN请求释放连接报文以[FIN，ACK]作为标志位，其中报文序号Seq=2445；确认号Ack=558；<br>注意：这里与“第三次握手”的ACK并不是表示确认的ACK报文。</li><li>”第二次挥手”服务器端返回的ACK确认报文以[ACK]作为标志位；其中报文序号Seq=558；确认号Ack=2246；</li><li>”第三次挥手”服务器端继续返回的FIN同意释放连接报文以[FIN，ACK]作为标志位；其中报文序号Seq=558；确认号Ack=2246；</li><li>”第四次挥手”客户端发出的ACK确认接收报文以[ACK]作为标志位；其中报文序号Seq=2446；确认号Ack=559。</li></ul><blockquote><p>后一次“挥手”传输报文中的序号Seq值等于前一次”握手”传输报文中的确认号Ack值；<br>后一次“挥手”传输报文中的确认号Ack值等于前一次”握手”传输报文中的序号Seq值；</p></blockquote><p>故这是连续的“四次挥手”过程，与前面的分析相符。</p>]]></content>
    
    
    
    <tags>
      
      <tag>TCP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 为什么 TCP 建立连接需要三次握手 </title>
    <link href="/2020/01/17/2020-01-17-why-tcp-third-hand/"/>
    <url>/2020/01/17/2020-01-17-why-tcp-third-hand/</url>
    
    <content type="html"><![CDATA[<blockquote><p>为什么这么设计（Why’s THE Design）是一系列关于计算机领域中程序设计决策的文章，我们在这个系列的每一篇文章中都会提出一个具体的问题并从不同的角度讨论这种设计的优缺点、对具体实现造成的影响。如果你有想要了解的问题，可以在文章下面留言。</p></blockquote><p>TCP 协议是我们几乎每天都会接触到的网络协议，绝大多数网络连接的建立都是基于 TCP 协议的，学过计算机网络或者对 TCP 协议稍有了解的人都知道 —— 使用 TCP 协议建立连接需要经过三次握手（three-way handshake）。</p><p>如果让我们简单说说 TCP 建立连接的过程，相信很多准备过面试的人都会非常了解，但是一旦想要深究『为什么 TCP 建立连接需要三次握手？』，作者相信大多数人都没有办法回答这个问题或者会给出错误的答案，这边文章就会讨论究竟为什么我们需要三次握手才能建立 TCP 连接？</p><blockquote><p>需要注意的是我们会将重点放到为什么需要 TCP 建立连接需要<strong>『三次握手』</strong>，而<em>不仅仅</em>是为什么需要<strong>『三次』</strong>握手。</p></blockquote><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在具体分析今天的问题之前，我们首先可以了解一下最常见的错误类比，这个对 TCP 连接过程的错误比喻误导了很多人，作者在比较长的一段时间内也认为它能够很好地描述 TCP 建立连接为什么需要三次握手：</p><ol><li>你听得到吗？</li><li>我能听到，你听得到？</li><li>我也能听到；</li></ol><p>这种用类比来解释问题往往就会面临『十个类比九个错』的尴尬局面，如果别人用类比回答你的为什么，你需要仔细想一想它的类比里究竟哪里有漏洞；类比带来的解释往往只能有片面的相似性，我们永远也无法找到绝对正确的类比，它只在我们想要通俗易懂地展示事物的特性时才能发挥较大的作用，我们在文章的后面会介绍为什么这里的类比有问题，各位读者也可以带着疑问来阅读剩下的内容。</p><p>很多人尝试回答或者思考这个问题的时候其实关注点都放在了三次握手中的<strong>三次</strong>上面，这确实很重要，但是如果重新审视这个问题，我们对于『什么是连接』真的清楚？只有知道<strong>连接的定义</strong>，我们才能去尝试回答为什么 TCP 建立连接需要三次握手。</p><blockquote><p>The reliability and flow control mechanisms described above require that TCPs initialize and maintain certain status information for each data stream. The combination of this information, including sockets, sequence numbers, and window sizes, is called a connection.</p></blockquote><p>RFC 793 - Transmission Control Protocol 文档中非常清楚地定义了 TCP 中的连接是什么，我们简单总结一下：用于保证可靠性和流控制机制的信息，包括 Socket、序列号以及窗口大小叫做连接。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-17/01.png" srcset="/img/loading.gif" alt=""></p><p>所以，建立 TCP 连接就是通信的双方需要对上述的三种信息达成共识，连接中的一对 Socket 是由互联网地址标志符和端口组成的，窗口大小主要用来做流控制，最后的序列号是用来追踪通信发起方发送的数据包序号，接收方可以通过序列号向发送方确认某个数据包的成功接收。</p><p>到这里，我们将原有的问题转换成了『为什么需要通过三次握手才可以初始化 Sockets、窗口大小和初始序列号？』，那么接下来我们就开始对这个细化的问题进行分析并寻找解释。</p><h1 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h1><p>这篇文章主要会从以下几个方面介绍为什么我们需要通过三次握手才可以初始化 Sockets、窗口大小、初始序列号并建立 TCP 连接：</p><ul><li>通过三次握手才能阻止重复历史连接的初始化；</li><li>通过三次握手才能对通信双方的初始序列号进行初始化；</li><li>讨论其他次数握手建立连接的可能性；</li></ul><p>这几个论点中的第一个是 TCP 选择使用三次握手的最主要原因，其他的几个原因相比之下都是次要的原因，我们在这里对它们的讨论只是为了让整个视角更加丰富，通过多方面理解这一有趣的设计决策。</p><h2 id="历史连接"><a href="#历史连接" class="headerlink" title="历史连接"></a>历史连接</h2><p>RFC 793 - Transmission Control Protocol 其实就指出了 TCP 连接使用三次握手的首要原因 —— 为了阻止历史的重复连接初始化造成的混乱问题，防止使用 TCP 协议通信的双方建立了错误的连接。</p><blockquote><p>The principle reason for the three-way handshake is to prevent old duplicate connection initiations from causing confusion.</p></blockquote><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-17/02.png" srcset="/img/loading.gif" alt=""></p><p>想象一下这个场景，如果通信双方的通信次数只有两次，那么发送方一旦发出建立连接的请求之后它就没有办法撤回这一次请求，如果在网络状况复杂或者较差的网络中，发送方连续发送多次建立连接的请求，如果 TCP 建立连接只能通信两次，那么接收方只能选择接受或者拒绝发送方发起的请求，它并不清楚这一次请求是不是由于网络拥堵而早早过期的连接。</p><p>所以，TCP 选择使用三次握手来建立连接并在连接引入了 <code>RST</code> 这一控制消息，接收方当收到请求时会将发送方发来的 <code>SEQ+1</code> 发送回接收方，这时由发送方来判断当前连接是否是历史连接：</p><ul><li>如果当前连接是历史连接，即 <code>SEQ</code> 过期或者超时，那么发送方就会直接发送 <code>RST</code> 控制消息中止这一次连接；</li><li>如果当前连接不是历史连接，那么发送方就会发送 <code>ACK</code> 控制消息，通信双方就会成功建立连接；</li></ul><p>使用三次握手和 <code>RST</code> 控制消息将是否建立连接的最终控制权交给了发送方，因为只有发送方有足够的上下文来判断当前连接是否是错误的或者过期的，这也是 TCP 使用三次握手建立连接的最主要原因。</p><h2 id="初始序列号"><a href="#初始序列号" class="headerlink" title="初始序列号"></a>初始序列号</h2><p>另一个使用三次握手的重要的原因就是通信双方都需要获得一个用于发送信息的初始化序列号，作为一个可靠的传输层协议，TCP 需要在不稳定的网络环境中构建一个可靠的传输层，网络的不确定性可能会导致数据包的缺失和顺序颠倒等问题，常见的问题可能包括：</p><ul><li>数据包被发送方多次发送造成数据的重复；</li><li>数据包在传输的过程中被路由或者其他节点丢失；</li><li>数据包到达接收方可能无法按照发送顺序；</li></ul><p>为了解决上述这些可能存在的问题，TCP 协议要求发送方在数据包中加入『序列号』字段，有了数据包对应的序列号，我们就可以：</p><ul><li>接收方可以通过序列号对重复的数据包进行去重；</li><li>发送方会在对应数据包未被 ACK 时进行重复发送；</li><li>接收方可以根据数据包的序列号对它们进行重新排序；</li></ul><p>序列号在 TCP 连接中有着非常重要的作用，初始序列号作为 TCP 连接的一部分也需要在三次握手期间进行初始化，由于 TCP 连接通信的双方都需要获得初始序列号，所以它们其实需要向对方发送 <code>SYN</code> 控制消息并携带自己期望的初始化序列号 <code>SEQ</code>，对方在收到 <code>SYN</code> 消息之后会通过 <code>ACK</code> 控制消息以及 <code>SEQ+1</code> 来进行确认。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-17/03.png" srcset="/img/loading.gif" alt=""></p><p>如上图所示，通信双方的两个 <code>TCP A/B</code> 分别向对方发送 <code>SYN</code> 和 <code>ACK</code> 控制消息，等待通信双方都获取到了自己期望的初始化序列号之后就可以开始通信了，由于 TCP 消息头的设计，我们可以将中间的两次通信合成一个，<code>TCP B</code> 可以向 <code>TCP A</code> 同时发送 <code>ACK</code> 和 <code>SYN</code> 控制消息，这也就帮助我们将四次通信减少至三次。</p><blockquote><p>A three way handshake is necessary because sequence numbers are not tied to a global clock in the network, and TCPs may have different mechanisms for picking the ISN’s. The receiver of the first SYN has no way of knowing whether the segment was an old delayed one or not, unless it remembers the last sequence number used on the connection (which is not always possible), and so it must ask the sender to verify this SYN. The three way handshake and the advantages of a clock-driven scheme are discussed in [3].</p></blockquote><p>除此之外，网络作为一个分布式的系统，其中并不存在一个用于计数的全局时钟，而 TCP 可以通过不同的机制来初始化序列号，作为 TCP 连接的接收方我们无法判断对方传来的初始化序列号是否过期，所以我们需要交由对方来判断，TCP 连接的发起方可以通过保存发出的序列号判断连接是否过期，如果让接收方来保存并判断序列号却是不现实的，这也再一次强化了我们在上一节中提出的观点 —— 避免历史错连接的初始化。</p><h2 id="通信次数"><a href="#通信次数" class="headerlink" title="通信次数"></a>通信次数</h2><p>当我们讨论 TCP 建立连接需要的通信次数时，我们经常会执着于为什么通信三次才可以建立连接，而不是两次或者四次；讨论使用更多的通信次数来建立连接往往是没有意义的，因为我们总可以<strong>使用更多的通信次数交换相同的信息</strong>，所以使用四次、五次或者更多次数建立连接在技术上都是完全可以实现的。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-17/04.png" srcset="/img/loading.gif" alt=""></p><p>这种增加 TCP 连接通信次数的问题往往没有讨论的必要性，我们追求的其实是用更少的通信次数（理论上的边界）完成信息的交换，也就是为什么我们在上两节中也一再强调使用『两次握手』没有办法建立 TCP 连接，使用三次握手是建立连接所需要的最小次数。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>我们在这篇文章中讨论了为什么 TCP 建立连接需要经过三次握手，在具体分析这个问题之前，我们首先重新思考了 TCP 连接究竟是什么，RFC 793 - Transmission Control Protocol - IETF Tools 对 TCP 连接有着非常清楚的定义 —— 用于保证可靠性和流控制机制的数据，包括 Socket、序列号以及窗口大小。</p><p>TCP 建立连接时通过三次握手可以有效地避免历史错误连接的建立，减少通信双方不必要的资源消耗，三次握手能够帮助通信双方获取初始化序列号，它们能够保证数据包传输的不重不丢，还能保证它们的传输顺序，不会因为网络传输的问题发生混乱，到这里不使用『两次握手』和『四次握手』的原因已经非常清楚了：</p><ul><li>『两次握手』：无法避免历史错误连接的初始化，浪费接收方的资源；</li><li>『四次握手』：TCP 协议的设计可以让我们同时传递 <code>ACK</code> 和 <code>SYN</code> 两个控制信息，减少了通信次数，所以不需要使用更多的通信次数传输相同的信息；</li></ul><p>我们重新回到在文章开头提的问题，为什么使用类比解释 TCP 使用三次握手是错误的？这主要还是因为，这个类比没有解释清楚核心问题 —— 避免历史上的重复连接。到最后，我们还是来看一些比较开放的相关问题，有兴趣的读者可以仔细想一下下面的问题：</p><ul><li>除了使用序列号是否还有其他方式保证消息的不重不丢？</li><li>UDP 协议有连接的概念么，它能保证数据传输的可靠么？</li></ul><p><strong>来源：</strong>微信公众号<strong>『真没什么逻辑』</strong>，作者 Draveness</p>]]></content>
    
    
    
    <tags>
      
      <tag>TCP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 听上去超酷的 Service Mesh 到底是什么？ </title>
    <link href="/2020/01/16/2020-01-16-what-is-servicemesh/"/>
    <url>/2020/01/16/2020-01-16-what-is-servicemesh/</url>
    
    <content type="html"><![CDATA[<p>Service Mesh 作为下一代微服务技术的代名词，初出茅庐却深得人心一鸣惊人，大有一统微服务时代的趋势。那么到底什么是Service Mesh？</p><p>一言以蔽之：<strong>Service Mesh是微服务时代的TCP协议。</strong></p><p>有了这样一个感性的初步认知，我们再来看到底什么是Service Mesh。提到Service Mesh，就不得不提微服务。根据维基百科的定义：</p><blockquote><p>微服务 (Microservices) 是一种软件架构风格，它是以专注于单一责任与功能的小型功能区块 (Small Building Blocks) 为基础，利用模块化的方式组合出复杂的大型应用程序，各功能区块使用与语言无关 (Language-Independent/Language agnostic) 的 API 集相互通信。</p></blockquote><p>目前业界跟微服务相关的开发平台和框架更是不胜枚举：Spring Cloud， Service Fabric，Linkerd，Envoy，Istio … 这些纷繁的产品和Sevice Mesh有什么样的关联？哪些属于Service Mesh的范畴？</p><p>为了理清这些繁复的产品和概念，我们先来了解下微服务和Service Mesh技术的历史发展脉络。了解清楚了技术的主要脉络，就能清晰的知道上述的各个平台、框架属于技术脉络中的哪个结点，其间的关系也就一目了然。</p><p>Phil Calçado的文章《Pattern: Service Mesh》详细的介绍了从开发者视角来看，服务开发模式和Service Mesh技术的演化过程，个人认为是非常经典的学习Service Mesh的资料。</p><p>这里借用文章的脉络，结合自己的理解并予以简化，试图说清楚ServiceMesh的概念和这项技术诞生的历史必然性。</p><p>时代0：开发人员想象中，不同服务间通信的方式，抽象表示如下：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/developer.png" srcset="/img/loading.gif" alt=""></p><p><strong>时代1：原始通信时代</strong><br>然而现实远比想象的复杂，在实际情况中，通信需要底层能够传输字节码和电子信号的物理层来完成，在TCP协议出现之前，服务需要自己处理网络通信所面临的丢包、乱序、重试等一系列流控问题，因此服务实现中，除了业务逻辑外，还夹杂着对网络传输问题的处理逻辑。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time1.png" srcset="/img/loading.gif" alt=""></p><p><strong>时代2：TCP时代</strong><br>为了避免每个服务都需要自己实现一套相似的网络传输处理逻辑，TCP协议出现了，它解决了网络传输中通用的流量控制问题，将技术栈下移，从服务的实现中抽离出来，成为操作系统网络层的一部分。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time2.png" srcset="/img/loading.gif" alt=""></p><p><strong>时代3：第一代微服务</strong><br>在TCP出现之后，机器之间的网络通信不再是一个难题，以GFS/BigTable/MapReduce为代表的分布式系统得以蓬勃发展。这时，分布式系统特有的通信语义又出现了，如熔断策略、负载均衡、服务发现、认证和授权、quota限制、trace和监控等等，于是服务根据业务需求来实现一部分所需的通信语义。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time3.png" srcset="/img/loading.gif" alt=""></p><p><strong>时代4：第二代微服务</strong><br>为了避免每个服务都需要自己实现一套分布式系统通信的语义功能，随着技术的发展，一些面向微服务架构的开发框架出现了，如Twitter的Finagle、Facebook的Proxygen以及Spring Cloud等等，这些框架实现了分布式系统通信需要的各种通用语义功能：如负载均衡和服务发现等，因此一定程度上屏蔽了这些通信细节，使得开发人员使用较少的框架代码就能开发出健壮的分布式系统。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time4.png" srcset="/img/loading.gif" alt=""></p><p><strong>时代5：第一代Service Mesh</strong><br>第二代微服务模式看似完美，但开发人员很快又发现，它也存在一些本质问题：</p><ul><li>其一，虽然框架本身屏蔽了分布式系统通信的一些通用功能实现细节，但开发者却要花更多精力去掌握和管理复杂的框架本身，在实际应用中，去追踪和解决框架出现的问题也绝非易事；</li><li>其二，开发框架通常只支持一种或几种特定的语言，回过头来看文章最开始对微服务的定义，一个重要的特性就是语言无关，但那些没有框架支持的语言编写的服务，很难融入面向微服务的架构体系，想因地制宜的用多种语言实现架构体系中的不同模块也很难做到；</li><li>其三，框架以lib库的形式和服务联编，复杂项目依赖时的库版本兼容问题非常棘手，同时，框架库的升级也无法对服务透明，服务会因为和业务无关的lib库升级而被迫升级；</li></ul><p>因此以Linkerd，Envoy，Ngixmesh为代表的代理模式（边车模式）应运而生，这就是第一代Service Mesh，它将分布式服务的通信抽象为单独一层，在这一层中实现负载均衡、服务发现、认证授权、监控追踪、流量控制等等分布式系统所需要的功能，作为一个和服务对等的代理服务，和服务部署在一起，接管服务的流量，通过代理之间的通信间接完成服务之间的通信请求，这样上边所说的三个问题也迎刃而解。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time5-1.png" srcset="/img/loading.gif" alt=""></p><p>如果我们从一个全局视角来看，就会得到如下部署图：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time5-2.png" srcset="/img/loading.gif" alt=""></p><p>如果我们暂时略去服务，只看Service Mesh的单机组件组成的网络：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time5-3.png" srcset="/img/loading.gif" alt=""></p><p>相信现在，大家已经理解何所谓Service Mesh，也就是服务网格了。<strong>它看起来确实就像是一个由若干服务代理所组成的错综复杂的网格。</strong></p><p><strong>时代6：第二代Service Mesh</strong><br>第一代Service Mesh由一系列独立运行的单机代理服务构成，为了提供统一的上层运维入口，演化出了集中式的控制面板，所有的单机代理组件通过和控制面板交互进行网络拓扑策略的更新和单机数据的汇报。这就是以Istio为代表的第二代Service Mesh。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time6-1.png" srcset="/img/loading.gif" alt=""></p><p>只看单机代理组件(数据面板)和控制面板的Service Mesh全局部署视图如下：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-16/time6-2.png" srcset="/img/loading.gif" alt=""></p><p>至此，见证了6个时代的变迁，大家一定清楚了Service Mesh技术到底是什么，以及是如何一步步演化到今天这样一个形态。</p><p>现在，我们再回过头来看Buoyant的CEO William Morgan，也就是Service Mesh这个词的发明人，对Service Mesh的定义：</p><blockquote><p>服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但对应用程序透明。</p></blockquote><p>这个定义中，有四个关键词：</p><p><strong>基础设施层+请求在这些拓扑中可靠穿梭</strong>：这两个词加起来描述了Service Mesh的定位和功能，是不是似曾相识？没错，你一定想到了TCP；</p><p><strong>网络代理</strong>：这描述了Service Mesh的实现形态；</p><p><strong>对应用透明</strong>：这描述了Service Mesh的关键特点，正是由于这个特点，Service Mesh能够解决以Spring Cloud为代表的第二代微服务框架所面临的三个本质问题；</p><p>总结一下，Service Mesh具有如下优点：</p><ul><li>屏蔽分布式系统通信的复杂性(负载均衡、服务发现、认证授权、监控追踪、流量控制等等)，服务只用关注业务逻辑；</li><li>真正的语言无关，服务可以用任何语言编写，只需和Service Mesh通信即可；</li><li>对应用透明，Service Mesh组件可以单独升级；</li></ul><p>当然，Service Mesh目前也面临一些挑战：</p><ul><li>Service Mesh组件以代理模式计算并转发请求，一定程度上会降低通信系统性能，并增加系统资源开销；</li><li>Service Mesh组件接管了网络流量，因此服务的整体稳定性依赖于Service Mesh，同时额外引入的大量Service Mesh服务实例的运维和管理也是一个挑战；</li></ul><p>历史总是惊人的相似。为了解决端到端的字节码通信问题，TCP协议诞生，让多机通信变得简单可靠；微服务时代，Service Mesh 应运而生，屏蔽了分布式系统的诸多复杂性，让开发者可以回归业务，聚焦真正的价值。</p><p>原文链接：<a href="https://smwyzi.github.io/post/what-is-service-mesh/" target="_blank" rel="noopener">https://smwyzi.github.io/post/what-is-service-mesh/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>ServerMesh</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 为什么执行自己的程序要在前面加./ </title>
    <link href="/2020/01/15/2020-01-15-why-exec-program/"/>
    <url>/2020/01/15/2020-01-15-why-exec-program/</url>
    
    <content type="html"><![CDATA[<p>在Linux中，我们执行内置命令时，直接输入命令名称即可，如：</p><pre><code class="hljs bash">$ mv a b <span class="hljs-comment">#将a重命名为b</span></code></pre><p>而在执行自己写好的程序时，却要带上./，例如：</p><pre><code class="hljs bash">$ hellohello: <span class="hljs-built_in">command</span> not found$ ./hellohello world</code></pre><p>这是为什么呢？它们有什么区别呢？</p><h1 id="shell是如何运行程序的"><a href="#shell是如何运行程序的" class="headerlink" title="shell是如何运行程序的"></a>shell是如何运行程序的</h1><p>在说明清楚问题之前，我们必须了解shell是如何运行程序的。首先我们必须要清楚的是，执行一条Linux命令，本质是在运行一个程序，如执行ls命令，它执行的是ls程序。那么在shell中输入一条命令，到底发生了什么？如果没有给出当前路径或绝对路径，它会经历哪几个查找过程？</p><h2 id="alias中查找"><a href="#alias中查找" class="headerlink" title="alias中查找"></a>alias中查找</h2><p>alias命令可用来设置命令别名，而单独输入alias可以查看到已设置的别名：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">alias</span><span class="hljs-built_in">alias</span> egrep=<span class="hljs-string">'egrep --color=auto'</span><span class="hljs-built_in">alias</span> fgrep=<span class="hljs-string">'fgrep --color=auto'</span><span class="hljs-built_in">alias</span> grep=<span class="hljs-string">'grep --color=auto'</span><span class="hljs-built_in">alias</span> l=<span class="hljs-string">'ls -CF'</span><span class="hljs-built_in">alias</span> la=<span class="hljs-string">'ls -A'</span><span class="hljs-built_in">alias</span> ll=<span class="hljs-string">'ls -alF'</span><span class="hljs-built_in">alias</span> ls=<span class="hljs-string">'ls --color=auto'</span></code></pre><p>如果这里没有找到你执行的命令，那么就会接下去查找。如果找到了，那么就会执行下去。</p><h2 id="内置命令中查找"><a href="#内置命令中查找" class="headerlink" title="内置命令中查找"></a>内置命令中查找</h2><p>不同的shell包含一些不同的内置命令，通常不需要shell到磁盘中去搜索。通过help命令可以看到有哪些内置命令：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">help</span></code></pre><p>通过type 命令可以查看命令类型：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">type</span> <span class="hljs-built_in">echo</span><span class="hljs-built_in">echo</span> is a shell <span class="hljs-built_in">builtin</span></code></pre><p>如果是内置命令，则会直接执行，否则继续查找。</p><h2 id="PATH中查找"><a href="#PATH中查找" class="headerlink" title="PATH中查找"></a>PATH中查找</h2><p>以ls为例，在shell输入ls时，首先它会从PATH环境变量中查找，PATH内容是什么呢，我们看看：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span>/usr/<span class="hljs-built_in">local</span>/sbin:/usr/<span class="hljs-built_in">local</span>/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/<span class="hljs-built_in">local</span>/games</code></pre><p>所以它会在这些路径下去寻找ls程序，按照路径找到的第一个ls程序就会被执行。使用whereis也能确定ls的位置：</p><pre><code class="hljs bash">$ whereis lsls: /bin/ls /usr/share/man/man1/ls.1.g</code></pre><p>既然它是在bin目录下，那么我把ls从bin目录下移走是不是就找不到了呢？是的。</p><pre><code class="hljs bash">$ mv /bin/ls /temp/ls_bak  <span class="hljs-comment">#测试完后记得改回来奥</span></code></pre><p>现在再来执行ls命令看看：</p><pre><code class="hljs bash">$ ls The program <span class="hljs-string">'ls'</span> is currently not installed. You can install it by typing:apt install coreutils</code></pre><p>没错，它会提示你没有安装这个程序或者命令没有找到。</p><p>所以你现在明白为什么你第一次安装jdk或者python的时候要设置环境变量了吧？不设置的话行不行？</p><p>行。这个时候你就需要指定路径了。怎么指定路径？无非就是那么几种，相对路径，绝对路径等等。<br>比如：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">cd</span> /temp$ ./ls_bak</code></pre><p>或者：</p><pre><code class="hljs bash">$ /temp/ls_bak</code></pre><p>是不是发现和运行自己的普通程序方式没什么差别呢？</p><p>到这里，如果还没有找到你要执行的命令，那么就会报错。</p><h2 id="确定解释程序"><a href="#确定解释程序" class="headerlink" title="确定解释程序"></a>确定解释程序</h2><p>在找到程序之后呢，需要确定解释程序。什么意思呢？<br>shell通常可以执行两种程序，一种是二进制程序，一种是脚本程序。</p><p>而一旦发现要执行的程序文件是文本文件，且文本未指定解释程序，那么就会默认当成shell脚本来执行。例如，假设有test.txt内容如下：</p><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> -e <span class="hljs-string">"hello world"</span></code></pre><p>赋予执行权限并执行：</p><pre><code class="hljs bash">$ chmod +x test.txt$ ./test.txthello world</code></pre><p>当然了，我们通常会在shell脚本程序的来头带上下面这句：</p><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span></code></pre><p>这是告诉shell，你要用bash程序来解释执行test.txt。作为一位调皮的开发者，如果开头改成下面这样呢？</p><pre><code class="hljs bash"><span class="hljs-comment">#!/usr/bin/python</span></code></pre><p>再次执行之后结果如下：</p><pre><code class="hljs bash">$ ./test.txt  File <span class="hljs-string">"./test.txt"</span>, line 2    <span class="hljs-built_in">echo</span> -e <span class="hljs-string">"hello world"</span>                        ^SyntaxError: invalid syntax</code></pre><p>是的，它被当成python脚本来执行了，自然就会报错了。</p><p>那么如果是二进制程序呢？就会使用execl族函数去创建一个新的进程来运行新的程序了。</p><p>小结一下前面的内容，就是说，如果是文本程序，且开头没有指定解释程序，则按照shell脚本处理，如果指定了解释程序，则使用解释程序来解释运行；对于二进制程序，则直接创建新的进程即可。</p><h1 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h1><p>前面我们也已经看到了运行方式，设置环境变量或者使用相对路径，绝对路径即可。不过对于shell脚本，你还可以像下面这样执行：</p><pre><code class="hljs bash">$ sh test.txt$ . test.txt</code></pre><p>即便test.txt没有执行权限，也能够正常执行。</p><p>什么？你说为什么txt也能执行？注意，Linux下的文件后缀不过是为了方便识别文件类型罢了，以.txt结尾，并不代表一定是文本。当然在这里它确实是，而且还是ASCII text executable：</p><pre><code class="hljs bash">$ file test.txttest.txt: Bourne-Again shell script, ASCII text executable$ file hellohello: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/l, <span class="hljs-keyword">for</span> GNU/Linux 2.6.32, BuildID[sha1]=8ae48f0f84912dec98511581c876aa042824efdb, not stripped</code></pre><h1 id="扩展一下"><a href="#扩展一下" class="headerlink" title="扩展一下"></a>扩展一下</h1><p>那么如果让我们自己的程序也能够像Linux内置命令一样输入即可被识别呢？</p><h2 id="将程序放到PATH路径下"><a href="#将程序放到PATH路径下" class="headerlink" title="将程序放到PATH路径下"></a>将程序放到PATH路径下</h2><p>第一种方法就是将我们自己的程序放到PATH中的路径中去，这样在shell输入hello时，也能找到，例如我们将其放在/bin目录下：</p><pre><code class="hljs bash">$ hellohello world$ whereis hellohello: /bin/hello</code></pre><p>也就是说，如果你的程序安装在了PATH指定的路径，就需要配置PATH环境变量，在命令行输入就可以直接找到了。</p><h2 id="设置PATH环境变量"><a href="#设置PATH环境变量" class="headerlink" title="设置PATH环境变量"></a>设置PATH环境变量</h2><p>那么如果想在指定的目录能够直接运行呢？很简单，那就是添加环境变量，例如将当前路径加入到PATH中：</p><pre><code class="hljs bash">$ PATH=<span class="hljs-variable">$PATH</span>:./   <span class="hljs-comment">#这种方式只在当前shell有效，所有shell生效可修改/etc/profile文件</span>$ hellohello world</code></pre><h2 id="设置别名"><a href="#设置别名" class="headerlink" title="设置别名"></a>设置别名</h2><p>例如：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">alias</span> hello=<span class="hljs-string">"/temp/hello"</span>$ hellohello world</code></pre><p>以上三种方法都可以达到目的。</p><h1 id="执行顺序"><a href="#执行顺序" class="headerlink" title="执行顺序"></a>执行顺序</h1><p>那么假设我写了一个自己的printf程序，当执行printf的时候，到底执行的是哪一个呢？</p><p>实际上它的查找顺序可以可以通过type -a来查看：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">type</span> -a <span class="hljs-built_in">printf</span><span class="hljs-built_in">printf</span> is aliased to `<span class="hljs-built_in">printf</span> <span class="hljs-string">"hello</span><span class="hljs-string">"</span><span class="hljs-string">'</span><span class="hljs-string">printf is a shell builtin</span><span class="hljs-string">printf is /usr/bin/printf</span><span class="hljs-string">printf is ./printf</span></code></pre><p>这里就可以很清楚地看到查找顺序了。也就是说，如果你输入printf，它执行的是：</p><pre><code class="hljs bash">$ <span class="hljs-built_in">printf</span>hello</code></pre><p>而如果删除别名：</p><pre><code class="hljs bash"><span class="hljs-built_in">unalias</span> <span class="hljs-built_in">printf</span></code></pre><p>它执行的将会是内置命令printf。<br>以此类推。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>说到这里，想必标题的问题以及下面的问题你都清楚了：</p><ul><li>安装Python或者Jdk程序为什么要设置PATH环境变量？如果不设置，该如何运行？</li><li>除了./方式运行自己的程序还有什么方式？</li><li>如果让自己的程序能够像内置命令一样被识别？</li><li>如何查看文件类型？</li><li>执行一条命令，如何确定是哪里的命令被执行</li></ul><p>本文涉及命令：</p><ul><li>mv 移动/重命名</li><li>file 查看文件信息</li><li>whereis 查看命令或者手册位置</li><li>type 查看命令类别</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 分布式和集群的区别 </title>
    <link href="/2020/01/14/2020-01-14-distributed-and-cluster-difference/"/>
    <url>/2020/01/14/2020-01-14-distributed-and-cluster-difference/</url>
    
    <content type="html"><![CDATA[<p>分布式开发的时代实际上早已悄悄地成为了时代的主流，吵得很热的云计算实际上只是包装在分布式之外的商业概念，很多开发者（包括我）都想加入研究云计算这个潮流，在 Google 上通过 “云计算” 这个关键词来查询资料，查到的都是些概念性或商业性的宣传资料，其实真正需要深入的还是那个早以被人熟知的概念——分布式。</p><p>分布式可繁也可以简，最简单的分布式就是大家最常用的，在负载均衡服务器后加一堆 Web 服务器，然后在上面搞一个缓存服务器来保存临时状态，后面共享一个数据库，其实很多号称分布式专家的人也就停留于此，大致结构如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-14/1.png" srcset="/img/loading.gif" alt=""></p><p>这种环境下真正进行分布式的只是 Web Server 而已，并且 Web Server 之间没有任何联系，所以结构和实现都非常简单。</p><p>有些情况下，对分布式的需求就没这么简单，在每个环节上都有分布式的需求，比如 Load Balance、DB、Cache 和文件等等，并且当分布式节点之间有关联时，还得考虑之间的通讯，另外，节点非常多的时候，得有监控和管理来支撑。这样看起来，分布式是一个非常庞大的体系，只不过你可以根据具体需求进行适当地裁剪。按照最完备的分布式体系来看，可以由以下模块组成：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-14/2.png" srcset="/img/loading.gif" alt=""></p><ul><li>分布式任务处理服务：负责具体的业务逻辑处理</li><li>分布式节点注册和查询：负责管理所有分布式节点的命名和物理信息的注册与查询，是节点之间联系的桥梁</li><li>分布式 DB：分布式结构化数据存取</li><li>分布式 Cache：分布式缓存数据（非持久化）存取</li><li>分布式文件：分布式文件存取</li><li>网络通信：节点之间的网络数据通信</li><li>监控管理：搜集、监控和诊断所有节点运行状态</li><li>分布式编程语言：用于分布式环境下的专有编程语言，比如 Elang、Scala</li><li>分布式算法：为解决分布式环境下一些特有问题的算法，比如解决一致性问题的 Paxos 算法</li></ul><p>因此，若要深入研究云计算和分布式，就得深入研究以上领域，而这些领域每一块的水都很深，都需要很底层的知识和技术来支撑，所以说，对于想提升技术的开发者来说，以分布式来作为切入点是非常好的，可以以此为线索，探索计算机世界的各个角落。</p><p>集群是个物理形态，分布式是个工作方式。</p><p>只要是一堆机器，就可以叫集群，他们是不是一起协作着干活，这个谁也不知道；一个程序或系统，只要运行在不同的机器上，就可以叫分布式，嗯，C/S 架构也可以叫分布式。</p><p>集群一般是物理集中、统一管理的，而分布式系统则不强调这一点。</p><p>所以，集群可能运行着一个或多个分布式系统，也可能根本没有运行分布式系统；分布式系统可能运行在一个集群上，也可能运行在不属于一个集群的多台（2 台也算多台）机器上。</p><p>布式是相对中心化而来，强调的是任务在多个物理隔离的节点上进行。中心化带来的主要问题是可靠性，若中心节点宕机则整个系统不可用，分布式除了解决部分中心化问题，也倾向于分散负载，但分布式会带来很多的其他问题，最主要的就是一致性。</p><p>集群就是逻辑上处理同一任务的机器集合，可以属于同一机房，也可分属不同的机房。分布式这个概念可以运行在某个集群里面，某个集群也可作为分布式概念的一个节点。</p><p>一句话，就是：“分头做事” 与 “一堆人” 的区别。</p><p>分布式是指将不同的业务分布在不同的地方。而集群指的是将几台服务器集中在一起，实现同一业务。</p><p>分布式中的每一个节点，都可以做集群。而集群并不一定就是分布式的。</p><p>举例：就比如新浪网，访问的人多了，他可以做一个群集，前面放一个响应服务器，后面几台服务器完成同一业务，如果有业务访问的时候，响应服务器看哪台服务器的负载不是很重，就将给哪一台去完成。</p><p>而分布式，从窄意上理解，也跟集群差不多， 但是它的组织比较松散，不像集群，有一个组织性，一台服务器垮了，其它的服务器可以顶上来。</p><p>分布式的每一个节点，都完成不同的业务，一个节点垮了，哪这个业务就不可访问了。</p><p>简单说，分布式是以缩短单个任务的执行时间来提升效率的，而集群则是通过提高单位时间内执行的任务数来提升效率。</p><p>例如：如果一个任务由 10 个子任务组成，每个子任务单独执行需 1 小时，则在一台服务器上执行该任务需 10 小时。</p><p>采用分布式方案，提供 10 台服务器，每台服务器只负责处理一个子任务，不考虑子任务间的依赖关系，执行完这个任务只需一个小时。(这种工作模式的一个典型代表就是 Hadoop 的 Map/Reduce 分布式计算模型）</p><p>而采用集群方案，同样提供 10 台服务器，每台服务器都能独立处理这个任务。假设有 10 个任务同时到达，10 个服务器将同时工作，1 小时后，10 个任务同时完成，这样，整身来看，还是 1 小时内完成一个任务！</p><p>集群一般被分为三种类型，高可用集群如 RHCS、LifeKeeper 等，负载均衡集群如 LVS 等、高性能运算集群;分布式应该是高性能运算集群范畴内。</p><ul><li>分布式：不同的业务模块部署在不同的服务器上或者同一个业务模块分拆多个子业务，部署在不同的服务器上，解决高并发的问题</li><li>集群：同一个业务部署在多台机器上，提高系统可用性</li></ul><p>大白话讲：</p><p>小饭店原来只有一个厨师，切菜洗菜备料炒菜全干。后来客人多了，厨房一个厨师忙不过来，又请了个厨师，两个厨师都能炒一样的菜，这两个厨师的关系是集群。为了让厨师专心炒菜，把菜做到极致，又请了个配菜师负责切菜，备菜，备料，厨师和配菜师的关系是分布式，一个配菜师也忙不过来了，又请了个配菜师，两个配菜师关系是集群。</p>]]></content>
    
    
    
    <tags>
      
      <tag>架构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为什么需要微服务网关</title>
    <link href="/2020/01/13/2020-01-13-why-microservice-gateway/"/>
    <url>/2020/01/13/2020-01-13-why-microservice-gateway/</url>
    
    <content type="html"><![CDATA[<h1 id="一、什么是服务网关"><a href="#一、什么是服务网关" class="headerlink" title="一、什么是服务网关"></a>一、什么是服务网关</h1><p>服务网关 = 路由转发 + 过滤器</p><p>1、路由转发：接收一切外界请求，转发到后端的微服务上去；</p><p>2、过滤器：在服务网关中可以完成一系列的横切功能，例如权限校验、限流以及监控等，这些都可以通过过滤器完成（其实路由转发也是通过过滤器实现的）。</p><h1 id="二、为什么需要服务网关"><a href="#二、为什么需要服务网关" class="headerlink" title="二、为什么需要服务网关"></a>二、为什么需要服务网关</h1><p>上述所说的横切功能（以权限校验为例）可以写在三个位置：</p><ul><li>每个服务自己实现一遍</li><li>写到一个公共的服务中，然后其他所有服务都依赖这个服务</li><li>写到服务网关的前置过滤器中，所有请求过来进行权限校验</li></ul><p>第一种，缺点太明显，基本不用；第二种，相较于第一点好很多，代码开发不会冗余，但是有两个缺点：</p><ul><li>由于每个服务引入了这个公共服务，那么相当于在每个服务中都引入了相同的权限校验的代码，使得每个服务的jar包大小无故增加了一些，尤其是对于使用docker镜像进行部署的场景，jar越小越好；</li><li>由于每个服务都引入了这个公共服务，那么我们后续升级这个服务可能就比较困难，而且公共服务的功能越多，升级就越难，而且假设我们改变了公共服务中的权限校验的方式，想让所有的服务都去使用新的权限校验方式，我们就需要将之前所有的服务都重新引包，编译部署。</li></ul><p>而服务网关恰好可以解决这样的问题：</p><ul><li>将权限校验的逻辑写在网关的过滤器中，后端服务不需要关注权限校验的代码，所以服务的jar包中也不会引入权限校验的逻辑，不会增加jar包大小；</li><li>如果想修改权限校验的逻辑，只需要修改网关中的权限校验过滤器即可，而不需要升级所有已存在的微服务。</li></ul><p>所以，需要服务网关！！！</p><h1 id="三、服务网关技术选型"><a href="#三、服务网关技术选型" class="headerlink" title="三、服务网关技术选型"></a>三、服务网关技术选型</h1><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-13/1.png" srcset="/img/loading.gif" alt=""></p><p>引入服务网关后的微服务架构如上，总体包含三部分：服务网关、open-service和service。</p><h2 id="1、总体流程"><a href="#1、总体流程" class="headerlink" title="1、总体流程"></a>1、总体流程</h2><ul><li>服务网关、open-service和service启动时注册到注册中心上去；</li><li>用户请求时直接请求网关，网关做智能路由转发（包括服务发现，负载均衡）到open-service，这其中包含权限校验、监控、限流等操作</li><li>open-service聚合内部service响应，返回给网关，网关再返回给用户</li></ul><h2 id="2、引入网关的注意点"><a href="#2、引入网关的注意点" class="headerlink" title="2、引入网关的注意点"></a>2、引入网关的注意点</h2><ul><li>增加了网关，多了一层转发（原本用户请求直接访问open-service即可），性能会下降一些（但是下降不大，通常，网关机器性能会很好，而且网关与open-service的访问通常是内网访问，速度很快）；</li><li>网关的单点问题：在整个网络调用过程中，一定会有一个单点，可能是网关、nginx、dns服务器等。防止网关单点，可以在网关层前边再挂一台nginx，nginx的性能极高，基本不会挂，这样之后，网关服务就可以不断的添加机器。但是这样一个请求就转发了两次，所以最好的方式是网关单点服务部署在一台牛逼的机器上（通过压测来估算机器的配置），而且nginx与zuul的性能比较，根据国外的一个哥们儿做的实验来看，其实相差不大，zuul是netflix开源的一个用来做网关的开源框架；</li><li>网关要尽量轻。</li></ul><h2 id="3、服务网关基本功能"><a href="#3、服务网关基本功能" class="headerlink" title="3、服务网关基本功能"></a>3、服务网关基本功能</h2><ul><li><p>智能路由：接收</p><p>外部一切请求，并转发到后端的对外服务open-service上去；</p></li><li><p>注意：我们只转发外部请求，服务之间的请求不走网关，这就表示全链路追踪、内部服务API监控、内部服务之间调用的容错、智能路由不能在网关完成；当然，也可以将所有的服务调用都走网关，那么几乎所有的功能都可以集成到网关中，但是这样的话，网关的压力会很大，不堪重负。</p></li><li><p>权限校验：只校验用户向open-service服务的请求，不校验服务内部的请求。服务内部的请求有必要校验吗？</p></li><li><p>API监控：只监控经过网关的请求，以及网关本身的一些性能指标（例如，gc等）；</p></li><li><p>限流：与监控配合，进行限流操作；</p></li><li><p>API日志统一收集：类似于一个aspect切面，记录接口的进入和出去时的相关日志</p></li><li><p>。。。后续补充</p></li></ul><p>上述功能是网关的基本功能，网关还可以实现以下功能：</p><ul><li>A|B测试：A|B测试时一块比较大的东西，包含后台实验配置、数据埋点（看转化率）以及分流引擎，在服务网关中，可以实现分流引擎，但是实际上分流引擎会调用内部服务，所以如果是按照上图的架构，分流引擎最好做在open-service中，不要做在服务网关中。</li><li>。。。后续补充</li></ul><h2 id="4、技术选型"><a href="#4、技术选型" class="headerlink" title="4、技术选型"></a>4、技术选型</h2><p>笔者准备自建一个轻量级的服务网关，技术选型如下：</p><ul><li>开发语言：java + groovy，groovy的好处是网关服务不需要重启就可以动态的添加filter来实现一些功能；</li><li>微服务基础框架：springboot；</li><li>网关基础组件：netflix zuul；</li><li>服务注册中心：consul；</li><li>权限校验：jwt；</li><li>API监控：prometheus + grafana；</li><li>API统一日志收集：logback + ELK；</li><li>压力测试：Jmeter；</li><li>。。。后续补充</li></ul><p>在后续的介绍中，会逐渐介绍各个知识点，并完成一个轻量级的服务网关！！！</p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 迄今为止把同步/异步/阻塞/非阻塞/BIO/NIO/AIO讲的这么清楚的好文章 </title>
    <link href="/2020/01/10/2020-01-10-sync-async/"/>
    <url>/2020/01/10/2020-01-10-sync-async/</url>
    
    <content type="html"><![CDATA[<p><strong>常规的误区</strong></p><p>假设有一个展示用户详情的需求，分两步，先调用一个HTTP接口拿到详情数据，然后使用适合的视图展示详情数据。</p><p>如果网速很慢，代码发起一个HTTP请求后，就卡住不动了，直到十几秒后才拿到HTTP响应，然后继续往下执行。</p><p>这个时候你问别人，刚刚代码发起的这个请求是不是一个同步请求，对方一定回答是。这是对的，它确实是。</p><p>但你要问它为什么是呢？对方一定是这样回答的，“因为发起请求后，代码就卡住不动了，直到拿到响应后才可以继续往下执行”。</p><p>我相信很多人也都是这样认为的，其实这是不对的，是把因果关系搞反了：</p><p>不是因为代码卡住不动了才叫同步请求，而是因为它是同步请求所以代码才卡住不动了。</p><p>至于为什么能卡住不动，这是由操作系统和CPU决定的：</p><p>因为内核空间里的对应函数会卡住不动，造成用户空间发起的系统调用卡住不动，继而使程序里的用户代码卡住不动了。</p><p>因此卡住不动了只是同步请求的一个副作用，并不能用它来定义同步请求，那该如何定义呢？</p><p><strong>同步和异步</strong></p><p>所谓同步，指的是协同步调。既然叫协同，所以至少要有2个以上的事物存在。协同的结果就是：</p><p>多个事物不能同时进行，必须一个一个的来，上一个事物结束后，下一个事物才开始。</p><p>那当一个事物正在进行时，其它事物都在干嘛呢？</p><p>严格来讲这个并没有要求，但一般都是处于一种“等待”的状态，因为通常后面事物的正常进行都需要依赖前面事物的结果或前面事物正在使用的资源。</p><p>因此，可以认为，同步更希望关注的是从宏观整体来看，多个事物是一种逐个逐个的串行化关系，绝对不会出现交叉的情况。</p><p>所以，自然也不太会去关注某个瞬间某个具体事物是处于一个什么状态。</p><p>把这个理论应用的出神入化的非“排队”莫属。凡是在资源少需求多的场景下都会用到排队。</p><p>比如排队买火车票这件事：</p><p>其实售票大厅更在意的是旅客一个一个的到窗口去买票，因为一次只能卖一张票。</p><p>即使大家一窝蜂的都围上去，还是一次只能卖一张票，何必呢？挤在一起又不安全。</p><p>只是有些人素质太差，非要往上挤，售票大厅迫不得已，采用排队这种形式来达到自己的目的，即一个一个的买票。</p><p>至于每个旅客排队时的状态，是看手机呀还是说话呀，根本不用去在意。</p><p>除了这种由于资源导致的同步外，还存在一种由于逻辑上的先后顺序导致的同步。</p><p>比如，先更新代码，然后再编译，接着再打包。这些操作由于后一步要使用上一步的结果，所以只能按照这种顺序一个一个的执行。</p><p>关于同步还需知道两个小的点：</p><p>一是范围，并不需要在全局范围内都去同步，只需要在某些关键的点执行同步即可。</p><p>比如食堂只有一个卖饭窗口，肯定是同步的，一个人买完，下一个人再买。但吃饭的时候也是一个人吃完，下一个人才开始吃吗？当然不是啦。</p><p>二是粒度，并不是只有大粒度的事物才有同步，小粒度的事物也有同步。</p><p>只不过小粒度的事物同步通常是天然支持的，而大粒度的事物同步往往需要手工处理。</p><p>比如两个线程的同步就需要手工处理，但一个线程里的两个语句天然就是同步的。</p><p>所谓异步，就是步调各异。既然是各异，那就是都不相同。所以结果就是：</p><p>多个事物可以你进行你的、我进行我的，谁都不用管谁，所有的事物都在同时进行中。</p><p>一言以蔽之，同步就是多个事物不能同时开工，异步就是多个事物可以同时开工。</p><p>注：一定要去体会“多个事物”，多个线程是多个事物，多个方法是多个事物，多个语句是多个事物，多个CPU指令是多个事物。等等等等。</p><p><strong>阻塞和非阻塞</strong></p><p>所谓阻塞，指的是阻碍堵塞。它的本意可以理解为由于遇到了障碍而造成的动弹不得。</p><p>所谓非阻塞，自然是和阻塞相对，可以理解为由于没有遇到障碍而继续畅通无阻。</p><p>对这两个词最好的诠释就是，当今中国一大交通难题，堵车：</p><p>汽车可以正常通行时，就是非阻塞。一旦堵上了，全部趴窝，一动不动，就是阻塞。</p><p>因此阻塞关注的是不能动，非阻塞关注的是可以动。</p><p>不能动的结果就是只能等待，可以动的结果就是继续前行。</p><p>因此和阻塞搭配的词一定是等待，和非阻塞搭配的词一定是进行。</p><p>回到程序里，阻塞同样意味着停下来等待，非阻塞表明可以继续向下执行。</p><p><strong>阻塞和等待</strong></p><p>等待只是阻塞的一个副作用而已，表明随着时间的流逝，没有任何有意义的事物发生或进行。</p><p>阻塞的真正含义是你关心的事物由于某些原因无法继续进行，因此让你等待。但没必要干等，你可以做一些其它无关的事物，因为这并不影响你对相关事物的等待。</p><p>在堵车时，你可以干等。也可以玩手机、和别人聊天，或者打牌、甚至先去吃饭都行。因为这些事物并不影响你对堵车的等待。不过你的车必须呆在原地。</p><p>在计算机里，是没有人这么灵活的，一般在阻塞时，选在干等，因为这最容易实现，只需要挂起线程，让出CPU即可。在条件满足时，会重新调度该线程。</p><p><strong>两两组合</strong></p><p>所谓同步/异步，关注的是能不能同时开工。</p><p>所谓阻塞/非阻塞，关注的是能不能动。</p><p>通过推理进行组合：</p><p>同步阻塞，不能同时开工，也不能动。只有一条小道，一次只能过一辆车，可悲的是还TMD的堵上了。</p><p>同步非阻塞，不能同时开工，但可以动。只有一条小道，一次只能过一辆车，幸运的是可以正常通行。</p><p>异步阻塞，可以同时开工，但不可以动。有多条路，每条路都可以跑车，可气的是全都TMD的堵上了。</p><p>异步非阻塞，可以工时开工，也可以动。有多条路，每条路都可以跑车，很爽的是全都可以正常通行。</p><p>是不是很容易理解啊。其实它们的关注点是不同的，只要搞明白了这点，组合起来也不是事儿。</p><p>回到程序里，把它们和线程关联起来：</p><p>同步阻塞，相当于一个线程在等待。</p><p>同步非阻塞，相当于一个线程在正常运行。</p><p>异步阻塞，相当于多个线程都在等待。</p><p>异步非阻塞，相当于多个线程都在正常运行。</p><p><strong>I/O</strong></p><p>IO指的就是读入/写出数据的过程，和<strong>等待</strong>读入/写出数据的过程。一旦拿到数据后就变成了数据操作了，就不是IO了。</p><p>拿网络IO来说，等待的过程就是数据从网络到网卡再到内核空间。读写的过程就是内核空间和用户空间的相互拷贝。</p><p>所以IO就包括两个过程，一个是等待数据的过程，一个是读写（拷贝）数据的过程。而且还要明白，一定<strong>不</strong>能包括操作数据的过程。</p><p><strong>阻塞IO和非阻塞IO</strong></p><p>应用程序都是运行在用户空间的，所以它们能操作的数据也都在用户空间。按照这样子来理解，只要数据没有到达用户空间，用户线程就操作不了。</p><p>如果此时用户线程已经参与，那它一定会被阻塞在IO上。这就是常说的阻塞IO。用户线程被阻塞在等待数据上或拷贝数据上。</p><p>非阻塞IO就是用户线程不参与以上两个过程，即数据已经拷贝到用户空间后，才去通知用户线程，一上来就可以直接操作数据了。</p><p>用户线程没有因为IO的事情出现阻塞，这就是常说的非阻塞IO。</p><p><strong>同步IO和同步阻塞IO</strong></p><p>按照上文中对同步的理解，同步IO是指发起IO请求后，必须拿到IO的数据才可以继续执行。</p><p>按照程序的表现形式又分为两种：</p><p>在等待数据的过程中，和拷贝数据的过程中，线程都在阻塞，这就是同步阻塞IO。</p><p>在等待数据的过程中，线程采用死循环式轮询，在拷贝数据的过程中，线程在阻塞，这其实还是同步阻塞IO。</p><p>网上很多文章把第二种归为同步非阻塞IO，这肯定是<strong>错误</strong>的，它一定是阻塞IO，因为拷贝数据的过程，线程是阻塞的。</p><p>严格来讲，在IO的概念上，同步和非阻塞是不可能搭配的，因为它们是一对相悖的概念。</p><p>同步IO意味着必须拿到IO的数据，才可以继续执行。因为后续操作依赖IO数据，所以它必须是阻塞的。</p><p>非阻塞IO意味着发起IO请求后，可以继续往下执行。说明后续执行不依赖于IO数据，所以它肯定不是同步的。</p><p>因此，在IO上，同步和非阻塞是互斥的，所以不存在同步非阻塞IO。但同步非阻塞是存在的，那不叫IO，叫操作数据了。</p><p>所以，同步IO一定是阻塞IO，同步IO也就是同步阻塞IO。</p><p><strong>异步IO和异步阻塞/非阻塞IO</strong></p><p>按照上文中对异步的理解，异步IO是指发起IO请求后，不用拿到IO的数据就可以继续执行。</p><p>用户线程的继续执行，和操作系统准备IO数据的过程是同时进行的，因此才叫做异步IO。</p><p>按照IO数据的两个过程，又可以分为两种：</p><p>在等待数据的过程中，用户线程继续执行，在拷贝数据的过程中，线程在阻塞，这就是异步阻塞IO。</p><p>在等待数据的过程中，和拷贝数据的过程中，用户线程都在继续执行，这就是异步非阻塞IO。</p><p>第一种情况是，用户线程没有参与数据等待的过程，所以它是异步的。但用户线程参与了数据拷贝的过程，所以它又是阻塞的。合起来就是异步阻塞IO。</p><p>第二种情况是，用户线程既没有参与等待过程也没有参与拷贝过程，所以它是异步的。当它接到通知时，数据已经准备好了，它没有因为IO数据而阻塞过，所以它又是非阻塞的。合起来就是异步非阻塞IO。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Network</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 阿里云terway源码分析 </title>
    <link href="/2020/01/09/2020-01-09-aliyun-terway/"/>
    <url>/2020/01/09/2020-01-09-aliyun-terway/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>随着公司业务的发展，底层容器环境也需要在各个区域部署，实现多云架构， 使用各个云厂商提供的CNI插件是k8s多云环境下网络架构的一种高效的解法。我们在阿里云的方案中，便用到了阿里云提供的CNI插件terway。terway所提供的VPC互通的网络方案，方便对接已有的基础设施，同时没有overlay网络封包解包的性能损耗，简单易用，出现网络问题方便诊断。本文对该插件做简单的代码分析，理解其原理，以便后期诊断问题和维护。</p><h1 id="功能划分"><a href="#功能划分" class="headerlink" title="功能划分"></a>功能划分</h1><p>阿里云开源的terway代码有三部分组成:</p><ul><li>CNI plugin： 即CNI插件，实现<code>ADD、DEL、VERSION</code>三个接口来供kubelet调用， 该插件将kubelet传递的参数进行简单处理后，会通过gRPC调用terwayBackendServer来实现具体的逻辑，例如申请网络设备等。同步调用terwayBackendServer将网络设备分配完毕之后，会通过<code>ipvlanDriver.Driver</code>进行pod sandbox network namespace的<code>Setup</code>操作，同时还会通过TC进行流控。该插件会通过daemonSet中initContainer安装到所有node上。</li><li>backend server： terway中主要的执行逻辑， 会进行IPAM管理，并申请对应的网络设备， 这部分是本次着重分析的对象。该程序以daemonSet的方式运行在每个节点上。</li><li>networkPolicy： 该部分是借助calico felix实现， 完全与上面两部分解耦。我们看到terway创建的网络设备是以cali为前缀的， 其实就是为了兼容calico的schema。</li></ul><h1 id="TerwayBackendServer"><a href="#TerwayBackendServer" class="headerlink" title="TerwayBackendServer"></a>TerwayBackendServer</h1><p>在terway的main函数中会启动gRPC server监听请求，同时会创建一个<code>TerwayBackendServer</code>， TerwayBackendServer封装全部操作逻辑，在<code>newNetworkService</code>函数中会依次初始化各个子模块实例，具体包括：</p><ul><li>ECS client　用来操作ECS client, 所有创建删除更新操作最后都会通过该client进行处理，简单封装了一层alicloud的SKD</li><li>kubernetes pod 管理模块，用来同步kubernetes pod信息</li><li>resouceDB 用来存储状态信息，便于重启等操作后恢复状态</li><li>resourceManager 管理资源分配的实例，terway会根据不同的配置生成不同的resourceManager，此处我们使用的是<code>ENIMultiIP</code>这种模式，对应的就是<code>newENIIPResourceManager</code></li></ul><p>ENIMultiIP模式会申请阿里云弹性网卡并配置多个辅助VPC的IP地址，将这些辅助IP地址映射和分配到Pod中，这些Pod的网段和宿主机网段是一致的，能够实现VPC网络互通。</p><p>整个架构如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-09/terway.png" srcset="/img/loading.gif" alt="twrway.png"></p><p>首先我们理解一下kubernetes pod管理模块，该模块用于获取kubernetes pod状态。terway为了支持一些高级的特性，例如流控等，有一些信息无法通过CNI调用传递过来，　还是得去kubernetes中去查询这些信息。此外CNI调用在一些异常情况下可能无法准确回调CNI插件， 例如用户直接<code>kubectl delete pod --force --graceperiod=0</code>，此时就需要kubernetes作为唯一的<code>single source of truth</code>， 保证最后网络设备在pod删除时肯定能够被释放掉。 它内部主要的方法就是<code>GetPod</code>与<code>GetLocalPod</code>。<code>GetPod</code>方法会请求apiserver返回pod信息，如果该pod已经在apiserver中删除，就会从本地的storage中获取。该storage是用boltDB做为底层存储的一个本地文件，每个被处理过的pod都会在该storage中保存一份信息，且该pod副本并不会随着apiserver中pod的删除而删除，这样后面程序如果需要该pod信息可以从该storage中获取。同时该pod副本会通过异步清理goroutine在pod删除一小时后删除。<code>GetLocalPod</code>是从apiserver获取该node上所有的pod信息，该过程是调用kubernetes最多的地方，目前两个清理goroutine会每5min调用一次，调用量相对较小，对apiserver的负载影响不大。该模块也会在本地DB里缓存一份数据，便于在kubernetes pod删除后还可以拿到用户信息。</p><p>其次是resourceDB模块，该模块是用来持久化状态信息，该DB中记录了当前已分配的pod及其网络设备(networkResource)信息。每次请求/释放设备都会更新该DB。程序重新启动初始化完成之后，也会从resouceDB中恢复上次运行的数据。<br>除了基本的分配删除操作会更新该DB, terway还启动异步goroutine定期清理，保证异常情况下的最终一致性，该goroutine会从apiserve中获取所有pod信息和当前DB中的信息进行对比，如果对应的pod已经删除会先释放对应的网络设备，然后从DB中删除该记录。同时延迟清理可以实现Statefulset的Pod在更新过程中IP地址保持不变，</p><p>最重要的是<code>resouceManager</code>模块，该iterface封装了具体网络设备的操作，如下所示:</p><pre><code class="hljs java"><span class="hljs-comment">// ResourceManager Allocate/Release/Pool/Stick/GC pod resource</span><span class="hljs-comment">// managed pod and resource relationship</span>type ResourceManager <span class="hljs-class"><span class="hljs-keyword">interface</span> </span>&#123;    Allocate(context *networkContext, prefer string) (types.NetworkResource, error)    Release(context *networkContext, resID string) error    GarbageCollection(inUseResList map[string]<span class="hljs-class"><span class="hljs-keyword">interface</span></span>&#123;&#125;, expireResList map[string]<span class="hljs-class"><span class="hljs-keyword">interface</span></span>&#123;&#125;) error&#125;</code></pre><p>从其中三个method可以很明显的看出可以执行的的动作，每次CNI插件调用backendServer时， 就会调用ResoueceManager进行具体的分配释放操作。对于<code>ENIMultiIP</code>这种模式来说，具体的实现类是<code>eniIPResourceManager</code>：</p><pre><code class="hljs java">type eniIPResourceManager struct &#123;    pool pool.ObjectPool&#125;</code></pre><p>其中只有pool一个成员函数，具体的实现类型是<code>simpleObjectPool</code>, 该pool维护了当前所有的ENI信息。当resouceManager进行分配释放网络设备的时候其实是从该pool中进行存取即可：</p><pre><code class="hljs java">func (m *eniIPResourceManager) Allocate(ctx *networkContext, prefer string) (types.NetworkResource, error) &#123;    <span class="hljs-keyword">return</span> m.pool.Acquire(ctx, prefer)&#125;func (m *eniIPResourceManager) Release(context *networkContext, resID string) error &#123;    <span class="hljs-keyword">if</span> context != nil &amp;&amp; context.pod != nil &#123;        <span class="hljs-keyword">return</span> m.pool.ReleaseWithReverse(resID, context.pod.IPStickTime)    &#125;    <span class="hljs-keyword">return</span> m.pool.Release(resID)&#125;func (m *eniIPResourceManager) GarbageCollection(inUseSet map[string]<span class="hljs-class"><span class="hljs-keyword">interface</span></span>&#123;&#125;, expireResSet map[string]<span class="hljs-class"><span class="hljs-keyword">interface</span></span>&#123;&#125;) error &#123;    <span class="hljs-keyword">for</span> expireRes := range expireResSet &#123;        <span class="hljs-keyword">if</span> err := m.pool.Stat(expireRes); err == nil &#123;            err = m.Release(nil, expireRes)            <span class="hljs-keyword">if</span> err != nil &#123;                <span class="hljs-keyword">return</span> err            &#125;        &#125;    &#125;    <span class="hljs-keyword">return</span> nil&#125;</code></pre><p>由上述代码可见，resouceManager实际操作的都是simpleObjectPool这个对象。　我们看看这个pool到底做了那些操作。首先初始化该pool:</p><pre><code class="hljs java"><span class="hljs-comment">// NewSimpleObjectPool return an object pool implement</span><span class="hljs-function">func <span class="hljs-title">NewSimpleObjectPool</span><span class="hljs-params">(cfg Config)</span> <span class="hljs-params">(ObjectPool, error)</span> </span>&#123;    <span class="hljs-keyword">if</span> cfg.MinIdle &gt; cfg.MaxIdle &#123;        <span class="hljs-keyword">return</span> nil, ErrInvalidArguments    &#125;    <span class="hljs-keyword">if</span> cfg.MaxIdle &gt; cfg.Capacity &#123;        <span class="hljs-keyword">return</span> nil, ErrInvalidArguments    &#125;    pool := &amp;simpleObjectPool&#123;        factory:  cfg.Factory,        inuse:    make(map[string]types.NetworkResource),        idle:     newPriorityQueue(),        maxIdle:  cfg.MaxIdle,        minIdle:  cfg.MinIdle,        capacity: cfg.Capacity,        notifyCh: make(chan <span class="hljs-class"><span class="hljs-keyword">interface</span></span>&#123;&#125;),        tokenCh:  make(chan struct&#123;&#125;, cfg.Capacity),    &#125;    <span class="hljs-keyword">if</span> cfg.Initializer != nil &#123;        <span class="hljs-keyword">if</span> err := cfg.Initializer(pool); err != nil &#123;            <span class="hljs-keyword">return</span> nil, err        &#125;    &#125;    <span class="hljs-keyword">if</span> err := pool.preload(); err != nil &#123;        <span class="hljs-keyword">return</span> nil, err    &#125;    log.Infof(<span class="hljs-string">"pool initial state, capacity %d, maxIdle: %d, minIdle %d, idle: %s, inuse: %s"</span>,        pool.capacity,        pool.maxIdle,        pool.minIdle,        queueKeys(pool.idle),        mapKeys(pool.inuse))    go pool.startCheckIdleTicker()    <span class="hljs-keyword">return</span> pool, nil&#125;</code></pre><p>可以看到在创建的时候会根据传入的config依次初始化各成员变量，　其中</p><ul><li>factory 成员用来分配网络设备，会调用ECS SDK进行分配资源，分配之后将信息存储在pool之中，具体的实现是<code>eniIPFactory</code>。</li><li>inuse 存储了当前所有正在使用的networkResource</li><li>idle 存储了当前所有空闲的networkResource, 即已经通过factory分配好，但是还未被某个pod实际使用。如果某个network resouce不再使用，也会归还到该idle之中。　通过这种方式，pool具备一定的缓充能力，避免频繁调用factory进行分配释放。idle为<code>priorityQeueu</code>类型，即所有空闲的networkResouce通过优先级队列排列，优先级队列的比较函数会比较<code>reverse</code>字段，<code>reverse</code>默认是入队时间，也就是该networkResouce的释放的时间，这样做能够尽量使一个IP释放之后不会被立马被复用。<code>reverse</code>字段对于一些statueSet的resouce也会进行一些特殊处理，因为statufulSet是有状态workload, 对于IP的释放也会特殊处理，保证其尽可能复用。</li><li>maxIdle, minIdle 分别表示上述idle队列中允许的最大和最小个数，　minIdle是为了提供有一定的缓冲能力，但该值并不保证，最大是为了防止缓存过多，如果空闲的networkResouce太多没有被使用就会释放一部分，IP地址不止是节点级别的资源，也会占用整个vpc/vswitch/安全组的资源，太多的空闲可能会导致其他节点或者云产品分配不出IP。</li><li>capacity 是该pool的容量，最大能分配的networkResouce的个数。该值可以自己指定， 但如果超过该ECS能允许的最大个数就会被设置成允许的最大个数。</li><li>tokenCh 是个buffered channel, 容量大小即为上面capacity的值，被做token bucket。 pool初始化的时候会将其中放满元素，后面运行过程中中，只要能从该channel中读取到元素则意味着该pool还没有满。每次调用factory申请networkResouce之前会从该channel中读取一个元素，　每次调用factory释放networkDevice会从该channel中放入一个元素。</li></ul><p>成员变量初始化完成之后会调用<code>Initializer</code>, 该函数会回调一个闭包函数，定义在<code>newENIIPResourceManager</code>中： 当程序启动时，resouceManager通过读取存储在本地磁盘也就是resouceDB中的信息获取当前正在使用的networkResouce，然后通过ecs获取当前所有eni设备及其ip, 依次遍历所有ip判断当前是否在使用，分别来初始化inuse和idle。这样可以保证程序重启之后可以重构内存中的pool数据信息。</p><p>然后会调用<code>preload</code>,该函数确保pool(idle)中有minIdle个空闲元素, 防止启动时大量调用factory。<br>最后会进行<code>go pool.startCheckIdleTicker()</code>　异步来goroutine中调用<code>checkIdle</code>定期查询pool(idle)中的元素是否超过maxIdle个元素，　如果超过则会调用factory进行释放。同时每次调用factory也会通过<code>notifyCh</code>来通知该goroutine执行检查操作。</p><p>pool结构初始化完成之后，resouceManager中所有对于networkResource的操作都会通过该pool进行，该pool在必要条件下再调用factory进行分配释放。</p><p>factory的具体实现是<code>eniIPFactory</code>, 用来调用ecs SDK进行申请释放eniIP, 并维护对应的数据结构。不同于直接使用eni设备，<code>ENIMultiIP</code>模式会为每个eni设备会有多个eniIP。eni设备是通过<code>ENI</code>结构体标识， eniIP通过<code>ENIIP</code>结构体标识。terway会为每个<code>ENI</code>创建一个goroutine, 该ENI上所有eniIP的分配释放都会在goroutine内进行，factory通过channel与该groutine通信， 每个goroutine对应一个接受channel <code>ipBacklog</code>，用于传递分配请求到该goroutine。 每次factory 需要创建(eniIPFactory.Create)一个eniIP时， 会一次遍历当前已经存在的<code>ENI</code>设备，如果该设备还有空闲的eniIP，就会通过该<code>ipBacklog</code> channel发送一个元素到该ENI设备的goroutine进行请求分配， 当goroutine将eniIP分配完毕之后通过factory 的<code>resultChan</code>通知factory, 这样factory就成功完成一次分配。 如果所有的ENI的eniIP都分配完毕，会首先创建ENI设备及其对应goroutine。因为每个ENI设备会有个主IP， 所以首次分配ENI不需要发送请求到<code>ipBacklog</code>, 直接将该主ip返回即可。对应的释放(Dispose)就是先释放eniIP， 等到只剩最后一个eniIP(主eniIP)时会释放整个ENI设备。对于所有ecs调用都会通过buffer channel进行流控，防止瞬间调用过大。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总之，terway的整个实现，逻辑比较清晰，并且扩展性也较高。后期，可以比较方便地在此基础上做一些定制和运维支持，从而很好地融入公司的基础架构设施。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阿里云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> sonarqube 搭配gitlab-ci </title>
    <link href="/2020/01/07/2020-01-07-sonarqube-and-gitlab/"/>
    <url>/2020/01/07/2020-01-07-sonarqube-and-gitlab/</url>
    
    <content type="html"><![CDATA[<p>1、项目文件中创建sonar-project.properties 文件</p><pre><code class="hljs bash"><span class="hljs-comment">#项目的key</span>sonar.projectKey=admin<span class="hljs-comment">#sonarqube的主机地址</span>sonar.host.url=https://192.168.1.6:9000<span class="hljs-comment">#项目的名字（这个名字在sonar界面显示的,此处根据项目名称修改）</span>sonar.projectName=sonar-test<span class="hljs-comment">#项目的版本（这个名字在sonar界面显示的,根据项目版本修改）</span>sonar.projectVersion=1.0<span class="hljs-comment">#需要分析源码的目录，多个目录用英文逗号隔开（根据需要修改）</span>sonar.sources=./<span class="hljs-comment">#编码格式</span><span class="hljs-comment">#sonar.sourceEncoding=UTF-8</span><span class="hljs-comment">#项目所用语言</span>sonar.language=java<span class="hljs-comment">#登录账号</span>sonar.login=admin<span class="hljs-comment">#登录密码</span>sonar.password=admin<span class="hljs-comment">#包含与源文件对应的已编译字节码文件 (如果没编译可以不写,默认创建target/sonar目录)</span>sonar.java.binaries=target/sonar</code></pre><p>例如：如下图</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-07/1.png" srcset="/img/loading.gif" alt=""></p><p>2、修改项目.gitlab-ci.yml文件，在首部添加如下：</p><pre><code class="hljs yaml"><span class="hljs-attr">sonar_preview:</span>  <span class="hljs-attr">stage:</span> <span class="hljs-string">test</span>  <span class="hljs-attr">script:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">/usr/local/sonar-scanner/bin/sonar-scanner</span>  <span class="hljs-attr">except:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">master</span>  <span class="hljs-attr">tags:</span>    <span class="hljs-bullet">-</span> <span class="hljs-string">sonar</span></code></pre><p>例如：如下图</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-07/2.png" srcset="/img/loading.gif" alt=""></p><p>3、修改完成后，分支用户每次commit都会触发进行代码检测（没有触发请手动）</p>]]></content>
    
    
    
    <tags>
      
      <tag>Gitlab</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Docker安装sonarqube </title>
    <link href="/2020/01/06/2020-01-06-sonarqube-install/"/>
    <url>/2020/01/06/2020-01-06-sonarqube-install/</url>
    
    <content type="html"><![CDATA[<p><strong>Sonarqube</strong>，是一种自动代码审查工具，可检测代码中的错误，漏洞和代码异常。它可以与您现有的工作流程集成，以实现跨项目分支和请求请求的连续代码检查。</p><p>1、拉取镜像</p><pre><code class="hljs bash">docker pull soanrqube docker pull postgres</code></pre><p>2、启动postgres</p><pre><code class="hljs bash">docker run -d --name postgresql --restart=always \-p 5432:5432 \-e POSTGRES_USER=sonarqube \-e POSTGRES_PASSWORD=sonarqube \-e POSTGRES_DB=sonarqube \postgres</code></pre><p>POSTGRES_DB：如果未指定此参数，那么第一次启动容器时创建的默认数据库将使用POSTGRES_USER的值</p><p>3、启动soanrqube</p><pre><code class="hljs bash">docker run -d --name sonarqube --restart=always \-p 9000:9000 \-v /opt/sonarqube/conf:/opt/sonarqube/conf \-v /opt/sonarqube/data:/opt/sonarqube/data \-v /opt/sonarqube/logs:/opt/sonarqube/logs \-v /opt/sonarqube/extensions:/opt/sonarqube/extensions \-e sonar.jdbc.username=sonarqube \-e sonar.jdbc.password=sonarqube \-e sonar.jdbc.url=jdbc:postgresql://192.168.1.6:5432/sonarqube \sonarqube</code></pre><p>4、浏览器打开192.168.1.6:9000 账号:admin 密码:admin</p><p>5、安装中文插件，configuration–market–搜索Chinese Pack</p><p>6、安装soanr-scanner（安装在gitlab-runner服务器上，可以搭配gitlab-ci）</p><p><a href="https://docs.sonarqube.org/latest/analysis/scan/sonarscanner/" target="_blank" rel="noopener">官网下载</a></p><p>解压放入/usr/local/目录下</p><p>7、项目文件根目录中创建sonar-project.properties 文件</p><pre><code class="hljs bash"><span class="hljs-comment">#项目的key</span>sonar.projectKey=admin<span class="hljs-comment">#sonarqube的主机地址</span>sonar.host.url=https://192.168.1.6:9000<span class="hljs-comment">#项目的名字（这个名字在sonar界面显示的,此处根据项目名称修改）</span>sonar.projectName=sonar-test<span class="hljs-comment">#项目的版本（这个名字在sonar界面显示的,根据项目版本修改）</span>sonar.projectVersion=1.0<span class="hljs-comment">#需要分析源码的目录，多个目录用英文逗号隔开（根据需要修改）</span>sonar.sources=./<span class="hljs-comment">#编码格式</span><span class="hljs-comment">#sonar.sourceEncoding=UTF-8</span><span class="hljs-comment">#项目所用语言</span>sonar.language=java<span class="hljs-comment">#登录账号</span>sonar.login=admin<span class="hljs-comment">#登录密码</span>sonar.password=admin<span class="hljs-comment">#包含与源文件对应的已编译字节码文件 (如果没编译可以不写,默认创建target/sonar目录)</span>sonar.java.binaries=target/sonar</code></pre><p>8、cd 到项目文件中，执行/usr/local/sonar-scanner/bin/sonar-scanner</p><p>9、再次进入web界面，可以看到分析结果</p>]]></content>
    
    
    
    <tags>
      
      <tag>Gitlab</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Gitlab-CI 流程 </title>
    <link href="/2020/01/03/2020-01-03-gitlab-ci/"/>
    <url>/2020/01/03/2020-01-03-gitlab-ci/</url>
    
    <content type="html"><![CDATA[<p><strong>持续集成(Continuous Integration)</strong></p><p>持续集成指的是频繁的将代码集成到主干，每次集成都通过自动化的构建（包括编译、发布、自动化测试）来验证，它的好处主要有两个：</p><ul><li>快速发现错误。每完成一点更新，就集成到主干，可以快速发现错误，定位错误也比较容易；</li><li>防止分支大幅偏离主干。如果不经常集成，很容易导致集成难度变大，以至于难以集成。</li></ul><h2 id="一、GitLab-CI-CD"><a href="#一、GitLab-CI-CD" class="headerlink" title="一、GitLab CI/CD"></a><strong>一、GitLab CI/CD</strong></h2><p>从<code>8.0</code>版开始，<code>GitLab</code>持续集成(CI)完全集成到<code>GitLab</code>本身，它还具有持续部署和持续交付功能，可用于构建、测试和部署你的应用程序。下面是<code>GitLab CI/CD</code>流程图。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-03/1.png" srcset="/img/loading.gif" alt=""></p><p>那么怎样让<code>GitLab CI</code>工作起来呢？总结起来就两条：</p><ol><li>将<code>.gitlab-ci.yml</code>文件添加到远程仓库的根目录；</li><li>将<code>GitLab</code>项目配置为使用<code>Runner</code></li></ol><p>设置好这些后，你每次<code>push</code>代码到<code>Git</code>仓库，<code>Runner</code>都会自动触发<code>CI pipeline</code>，你可以在项目的<code>Pipelines</code>页面下。如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-03/2.png" srcset="/img/loading.gif" alt=""></p><p>如果一切运行正常，你可以看到绿色复选标记，这样你就可以在查看代码之前轻松查看任何提交是否导致测试失败。</p><h2 id="二、什么是-gitlab-ci-yml"><a href="#二、什么是-gitlab-ci-yml" class="headerlink" title="二、什么是 .gitlab-ci.yml"></a><strong>二、什么是 .gitlab-ci.yml</strong></h2><p>1、.gitlab-ci.yml文件配置CI对项目执行的操作，它告诉GitLab runner该做什么。</p><p>2、它位于存储库的根目录中，你代码的每次提交，GitLab都会查找.gitlab-ci.yml这个文件，并根据这个文件的内容，在Runner上启动你提交的工作。</p><p>3、默认情况下，它运行一个包含三个stage的管道：build，test，deploy。你不需要使用所有三个stage，没有工作的stage将会被忽略。</p><p>注意： .gitlab-ci.yml是一个YAML文件，因此您必须特别注意缩进。始终使用空格键，而不是Tab键。</p><p>你需要在你仓库的根目录下创建一个名为<code>.gitlab-ci.yml</code>的文件，下面是一个工程示例。它是最简单的配置。</p><pre><code class="hljs yaml"><span class="hljs-string">`before_script:``</span> <span class="hljs-string">``-</span> <span class="hljs-string">hostname``</span> <span class="hljs-string">``-</span> <span class="hljs-string">ip</span> <span class="hljs-string">addr`</span> <span class="hljs-string">`stages:``</span> <span class="hljs-string">``-</span> <span class="hljs-string">test``</span> <span class="hljs-string">``-</span> <span class="hljs-string">deploy-app``</span> <span class="hljs-string">`</span> <span class="hljs-string">`sonar_analyze:``</span> <span class="hljs-string">``stage:</span> <span class="hljs-string">test``</span> <span class="hljs-string">``script:``</span>   <span class="hljs-string">``-</span> <span class="hljs-string">/usr/local/sonar-scanner/bin/sonar-scanner``</span> <span class="hljs-string">``except:``</span>  <span class="hljs-string">``-</span> <span class="hljs-string">master``</span> <span class="hljs-string">``tags:``</span>  <span class="hljs-string">``-</span> <span class="hljs-string">sonar`</span> <span class="hljs-string">`deploy-app-to-test:``</span> <span class="hljs-string">``stage:</span> <span class="hljs-string">deploy-app``</span> <span class="hljs-string">``only:``</span>  <span class="hljs-string">``-</span> <span class="hljs-string">master``</span> <span class="hljs-string">``script:``</span>  <span class="hljs-string">``-</span> <span class="hljs-string">hostname``</span>  <span class="hljs-string">``-</span> <span class="hljs-string">ls``</span> <span class="hljs-string">``tags:``</span>  <span class="hljs-string">``-</span> <span class="hljs-string">sonar`</span></code></pre><p>上面的配置主要做了两件事：</p><ol><li>执行了两个job（名称是任意的）；</li><li>在每个<code>job</code>之前，执行<code>before_script</code>定义的命令。 </li></ol><p>关于<code>.gitlab-ci.yml</code>的语法讲解，可以查看<a href="https://docs.gitlab.com/ee/ci/yaml/" target="_blank" rel="noopener">官网的介绍</a>，然后根据项目的具体需求，使用这些语法，创建自己的脚本。</p><h2 id="三、配置Runner"><a href="#三、配置Runner" class="headerlink" title="三、配置Runner"></a><strong>三、配置Runner</strong></h2><p><strong>runner简单介绍</strong></p><p>GitLab Runner是一个开源项目，用于运行您的作业并将结果发送回GitLab。它与GitLab CI一起使用，GitLab CI是GitLab随附的开源持续集成服务，用于协调作业。</p><p>要求</p><ul><li>GitLab Runner是用Go编写的，可以作为单个二进制文件运行，不需要语言特定的要求。</li><li>它运行在GNU / Linux，macOS和Windows操作系统上。只要您可以在其上编译Go二进制文件，它就可以正常工作。</li><li>如果要使用Docker，请安装最新版本。GitLab Runner至少需要的Docker v1.13.0。</li><li>建议使用和gitlab相同版本</li></ul><p><strong>1、Runner安装</strong></p><p>请参考官方文档，这里不再详细说明   <a href="https://docs.gitlab.com/runner/" target="_blank" rel="noopener">https://docs.gitlab.com/runner/</a></p><p><strong>2、Runner注册</strong></p><p>要求<br>在注册Runner之前，您需要先：</p><ul><li>将其安装在与安装GitLab的位置不同的服务器上</li><li>通过GitLab的界面获取共享或特定Runner的令牌</li></ul><p>如下图：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-03/3.png" srcset="/img/loading.gif" alt=""></p><p><strong>GNU / Linux</strong></p><p>1、运行以下命令：<br>gitlab-runner register</p><p>2、输入您的GitLab实例URL：<br>Please enter the gitlab-ci coordinator URL (e.g. <a href="https://gitlab.com" target="_blank" rel="noopener">https://gitlab.com</a> ):</p><p><a href="https://gitlab.xxx.xxx" target="_blank" rel="noopener">https://gitlab.xxx.xxx</a></p><p>3、输入您获得的令牌以注册Runner：<br>Please enter the gitlab-ci token for this runner:</p><p>xxx</p><p>4、输入Runner的描述，您可以稍后在GitLab的UI中更改：（根据需求更改）<br>Please enter the gitlab-ci description for this runner:</p><p>my-runner</p><p>5、输入与Runner关联的标签，您可以稍后在GitLab的UI中更改：（根据需求更改）<br>Please enter the gitlab-ci tags for this runner (comma separated):</p><p>my-tag</p><p>6、输入Runner执行程序：(每个执行程序的作用，详情请点击<a href="https://docs.gitlab.com/runner/executors/README.html" target="_blank" rel="noopener">runner执行程序</a>，请根据需要选择执行器)<br>Please enter the executor: ssh, docker+machine, docker-ssh+machine, kubernetes, docker, parallels, virtualbox, docker-ssh, shell:</p><p>docker</p><p>7、如果您选择Docker作为执行程序，您将被要求为没有在.gitlab-ci.yml中定义映像的项目使用默认映像（根据需要设置默认镜像）</p><p>Please enter the Docker image (eg. ruby:2.1):</p><p>maven:latest</p><p><strong>3、查看注册是否成功</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-03/4.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>Gitlab</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 阿里云搭建shadowsocks-vpn </title>
    <link href="/2020/01/02/2020-01-02-aliyun-vpn/"/>
    <url>/2020/01/02/2020-01-02-aliyun-vpn/</url>
    
    <content type="html"><![CDATA[<p><strong>脚本可能已不可用，未测试</strong> 2020-5-29</p><p>阿里云服务器购买国外节点，建议香港、日本、新加坡</p><p>系统选择linux，脚本支持centos、debian、ubuntu</p><p>1、下载脚本文件，并执行</p><pre><code class="hljs bash">$ wget --no-check-certificate -O shadowsocks-all.shhttps://github.com/ILIKETWICE/shadowsocks-install/blob/master/shadowsocks-all.shchmod +x shadowsocks-all.sh$ ./shadowsocks-all.sh 2&gt;&amp;1 | tee shadowsocks-all.log</code></pre><p>2、选择脚本（Python、R、Go、libev），任选一个：(这里我选择的是Go)</p><pre><code class="hljs bahs">Which Shadowsocks server you&#39;d select:1.Shadowsocks-Python2.ShadowsocksR3.Shadowsocks-Go4.Shadowsocks-libevPlease enter a number (default 1):3</code></pre><p>3、我选择的是<code>Shadowsocks-Go</code>，输入3……然后，输入密码和端口，笔者直接回车用默认：</p><pre><code class="hljs bash">You choose = Shadowsocks-Go Please enter password <span class="hljs-keyword">for</span> Shadowsocks-Go(default password: teddysun.com):  <span class="hljs-comment"># 输入你的密码</span> password = teddysun.com Please enter a port <span class="hljs-keyword">for</span> Shadowsocks-Go [1-65535](default port: 8989):   <span class="hljs-comment"># 输入端口</span> port = 8989  Press any key to start...or Press Ctrl+C to cancel  <span class="hljs-comment"># 回车</span></code></pre><p>4、安装成功后，命令行出现：</p><pre><code class="hljs bash">Congratulations, Shadowsocks-Go server install completed!Your Server IP        :  xx.xx.xx.xxYour Server Port      :  8989Your Password         :  teddysun.comYour Encryption Method:  aes-256-cfb Welcome to visit: https://teddysun.com/486.htmlEnjoy it!</code></pre><p>5、注意阿里云ECS主机要打开“安全组”，添加入口规则</p>]]></content>
    
    
    
    <tags>
      
      <tag>阿里云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> mesos 以容器方式启动,拉取镜像失败问题 </title>
    <link href="/2020/01/01/2020-01-01-mesos-pull-images-error/"/>
    <url>/2020/01/01/2020-01-01-mesos-pull-images-error/</url>
    
    <content type="html"><![CDATA[<p>为了方便快速部署,将 mesos、marathon进行了容器化部署, 但是容器化完后发现在<code>marathon</code> 上创建应用一直创建不成功</p><p><strong>分析过程</strong></p><p>因为是容器化部署的mesos-slave和marathon,是不是因为没有找到证书和登录信息导致的,随后在mesos-salve容器手动的进行<code>docker login</code> 操作并将证书进行挂载到对应目录,手工执行命令docker -H unix:///var/run/docker.sock pull registry.cn-beijing.aliyuncs.com/xxxx/xxxxx-service:12984 能正常下载仓库镜像,但通过marathon创建应用问题依旧,此时我们查看了一下日志发现一些踪迹</p><p><strong>mesos-slave 错误信息</strong></p><pre><code class="hljs bash">Failed to launch container: Failed to run <span class="hljs-string">'docker -H unix:///var/run/docker.sock pull registry.cn-beijing.aliyuncs.com/xxxx/xxxxx-service:12984'</span>: exited with status 1; stderr=<span class="hljs-string">'Error response from daemon: pull access denied for registry.cn-beijing.aliyuncs.com/xxxxx/xxxxx-service, repository does not exist or may require '</span>docker login<span class="hljs-string">': denied: requested access to the resource is denied '</span></code></pre><p>镜像仓库报错很明显,认证不通过,通过查找<code>marathon</code> 官方文档,我们发现官方说明在使用<code>Private Docker Registry</code> 时候需要额外做一些配置,参考<a href="https://mesosphere.github.io/marathon/docs/native-docker-private-registry.html" target="_blank" rel="noopener">marathon使用私有仓库配置</a> ,配置简单的步骤如下,所有<code>slave</code>节点和<code>marathon</code>节点都需要配置</p><ul><li>宿主机手工进行docker login</li></ul><pre><code class="hljs bash">docker login</code></pre><ul><li>登录信息会保存在/root/.docker 目录下,将其进行打包</li></ul><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /roottar -cvf docker.tar.gz .docker/</code></pre><ul><li>将打包的文件复制到所有节点目录,<strong>注意:</strong> 放置在一个公共目录,因为marathon和mesos-slave都需要调用到</li></ul><pre><code class="hljs bash">scp  docker.tar.gz  所有节点:/一个公共目录</code></pre><ul><li>容器启动marathon 和mesos-slave时候进行挂载对应公共目录</li></ul><pre><code class="hljs bash"><span class="hljs-comment">#mesos-slave</span>docker run -v /tmp/docker.tar.gz:/tmp/docker.tar.gz  .....  mesos-salve<span class="hljs-comment">#marathon</span>docker run -v /tmp/docker.tar.gz:/tmp/docker.tar.gz  .....  marathon</code></pre><ul><li>创建应用时候,配置urls参数</li></ul><pre><code class="hljs json">"fetch": [  &#123;    <span class="hljs-attr">"uri"</span>: <span class="hljs-string">"file:///etc/docker.tar.gz"</span>  &#125;]</code></pre><p>应用启动后,我们登录到mesos-slave 容器上查看资源stderr日志可以发现,资源节点如何下载和使用docker.tar.gz 包，这里就不贴出日志信息啦。</p><p>因为我使用的图形化界面配置，mesos版本为1.5.0，配置图如下</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-01/1.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-01/2.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2020-01-01/3.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>Mesos</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> TCP 超时与重传 </title>
    <link href="/2019/12/31/2019-12-31-tcp-timeout-and-retransmission/"/>
    <url>/2019/12/31/2019-12-31-tcp-timeout-and-retransmission/</url>
    
    <content type="html"><![CDATA[<p>我们都知道 TCP 协议具有重传机制，也就是说，如果发送方认为发生了丢包现象，就重发这些数据包。很显然，我们需要一个方法来「<strong>猜测</strong>」是否发生了丢包。最简单的想法就是，接收方每收到一个包，就向发送方返回一个 <strong>ACK</strong>，表示自己已经收到了这段数据，反过来，如果发送方一段时间内没有收到 ACK，就知道<strong>很可能</strong>是数据包丢失了，紧接着就重发该数据包，直到收到 ACK 为止。</p><p>你可能注意到我用的是「猜测」，因为即使是超时了，这个数据包也可能并没有丢，它只是绕了一条远路，来的很晚而已。毕竟 TCP 协议是位于<strong>传输层</strong>的协议，不可能明确知道数据链路层和物理层发生了什么。但这并不妨碍我们的超时重传机制，因为接收方会自动忽略重复的包。</p><p>超时和重传的概念其实就是这么简单，但内部的细节却是很多，我们最先想到的一个问题就是，<strong>到底多长时间才能算超时呢</strong>？</p><h2 id="超时是怎么确定的？"><a href="#超时是怎么确定的？" class="headerlink" title="超时是怎么确定的？"></a>超时是怎么确定的？</h2><p>一刀切的办法就是，我<strong>直接把超时时间设成一个固定值</strong>，比如说 200ms，但这样肯定是有问题的，我们的电脑和很多服务器都有交互，这些服务器位于天南海北，国内国外，延迟差异巨大，打个比方：</p><ul><li>我的个人博客搭在国内，延迟大概 30ms，也就是说正常情况下的数据包，60ms 左右就已经能收到 ACK 了，但是按照我们的方法，200ms 才能确定丢包（正常可能是 90 到 120 ms），这<strong>效率实在是有点低</strong>。</li><li>假设你访问某国外网站，延迟有 130 ms，这就麻烦了，<strong>正常的数据包都可能被认为是超时，导致大量数据包被重发，可以想象，重发的数据包也很容易被误判为超时。。。雪崩效应的感觉</strong></li></ul><p>所以设置固定值是很不可靠的，<strong>我们要根据网络延迟，动态调整超时时间</strong>，延迟越大，超时时间越长。</p><p>在这里先引入两个概念：</p><ul><li>RTT（Round Trip Time）：往返时延，也就是<strong>数据包从发出去到收到对应 ACK 的时间。</strong>RTT 是针对连接的，每一个连接都有各自独立的 RTT。</li><li>RTO（Retransmission Time Out）：重传超时，也就是前面说的超时时间。</li></ul><p>比较标准的 RTT 定义：</p><blockquote><p>Measure the elapsed time between sending a data octet with a particular sequence number and <strong>receiving an acknowledgment that covers that sequence number</strong> (segments sent do not have to match segments received). This measured elapsed time is the Round Trip Time (RTT).</p></blockquote><h3 id="经典方法"><a href="#经典方法" class="headerlink" title="经典方法"></a>经典方法</h3><p>最初的规范「RFC0793」采用了下面的公式来得到平滑的 RTT 估计值（称作 SRTT）：</p><p><strong>SRTT  &lt;-  α·SRTT +（1 - α）·RTT</strong></p><p>RTT 是指最新的样本值，这种估算方法叫做「指数加权移动平均」，名字听起来比较高大上，但整个公式比较好理解，就是利用现存的 SRTT 值和最新测量到的 RTT 值取一个加权平均。</p><p>有了 SRTT，就该设置对应的 RTO 的值了，「RFC0793」是这么算的：</p><p><strong>RTO = min(ubound, max(lbound, (SRTT)·β))</strong></p><p>这里面的 <strong>ubound</strong> 是 RTO 的<strong>上边界</strong>，<strong>lbound</strong> 为 RTO 的<strong>下边界</strong>，β 称为<strong>时延离散因子</strong>，推荐值为 1.3 ~ 2.0。这个计算公式就是将  (SRTT)·β 的值作为 RTO，只不过另外<strong>限制了 RTO 的上下限</strong>。</p><p>这个计算方法，初看是没有什么问题（至少我是这么感觉的），但是实际应用起来，有两个缺陷：</p><blockquote><p>There were two known problems with the RTO calculations specified in RFC-793. First, the accurate measurement of RTTs is difficult <strong>when there are retransmissions</strong>. Second, the algorithm to compute the smoothed round-trip time is inadequate [TCP:7], <strong>because it incorrectly assumed that the variance in RTT values would be small and constant</strong>. These problems were solved by <strong>Karn’s and Jacobson’s algorithm</strong>, respectively.</p></blockquote><p>这段话摘自「RFC1122」，我来解释一下：</p><ul><li>当<strong>出现数据包重传的情况</strong>下，RTT 的计算就会很“麻烦”，我画了张图来说明这些情况：</li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-31/rtt.png" srcset="/img/loading.gif" alt=""></p><ul><li><p>图上列了两种情况，这两种情况下计算 RTT 的方法是不一样的（这就是所谓的重传二义性）：</p><p>但是对于客户端来说，它不知道发生了哪种情况，选错情况的结果就是 RTT 偏大/偏小，影响到 RTO 的计算。（最简单粗暴的解决方法就是<strong>忽略有重传的数据包，只计算那些没重传过的</strong>，但这样会导致其他问题。。详见 Karn’s algorithm）</p></li><li><ul><li>情况一：RTT = t2 - t0</li><li>情况二：RTT = t2 - t1</li></ul></li><li><p>另一个问题是，<strong>这个算法假设 RTT 波动比较小</strong>，因为这个加权平均的算法又叫<strong>低通滤波器</strong>，对突然的网络波动不敏感。如果网络时延突然增大导致实际 RTT 值远大于估计值，会导致不必要的重传，增大网络负担。（ RTT 增大已经表明网络出现了过载，这些不必要的重传会进一步加重网络负担）。</p></li></ul><h3 id="标准方法"><a href="#标准方法" class="headerlink" title="标准方法"></a>标准方法</h3><p>说实话这个标准方法比较，，，麻烦，我就直接贴公式了：</p><p><strong>SRTT  &lt;-  (1 - α)·SRTT  + α·RTT</strong>  //跟基本方法一样，<strong>求 SRTT 的加权平均</strong></p><p><strong>rttvar  &lt;- (1 - h)·rttvar + h·(|RTT - SRTT |)</strong>  //计算 <strong>SRTT 与真实值的差距</strong>（称之为绝对误差|Err|），同样用到<strong>加权平均</strong></p><p><strong>RTO = SRTT  + 4·rttvar</strong> //估算出来的新的 RTO，rttvar 的系数 4 是调参调出来的</p><p>这个算法的整体思想就是结合<strong>平均值</strong>（就是基本方法）和<strong>平均偏差</strong>来进行估算，一波玄学调参得到不错的效果。如果想更深入了解这个算法，参考「RFC6298」。</p><h2 id="重传——TCP的重要事件"><a href="#重传——TCP的重要事件" class="headerlink" title="重传——TCP的重要事件"></a>重传——TCP的重要事件</h2><h3 id="基于计时器的重传"><a href="#基于计时器的重传" class="headerlink" title="基于计时器的重传"></a>基于计时器的重传</h3><p>这种机制下，<strong>每个数据包都有相应的计时器</strong>，一旦超过 RTO 而没有收到 ACK，就重发该数据包。没收到 ACK 的数据包都会存在重传缓冲区里，等到 ACK 后，就从缓冲区里删除。</p><p>首先明确一点，对 TCP 来说，超时重传是<strong>相当重要</strong>的事件（RTO 往往大于两倍的 RTT，超时往往意味着拥塞），一旦发生这种情况，<strong>TCP 不仅会重传对应数据段，还会降低当前的数据发送速率</strong>，因为TCP 会认为当前网络发生了拥塞。</p><p>简单的超时重传机制往往比较低效，如下面这种情况：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-31/transfer.png" srcset="/img/loading.gif" alt=""></p><p>假设数据包5丢失，数据包 6,7,8,9 都已经到达接收方，这个时候客户端就只能等服务器发送 ACK，注意对于包 6,7,8,9，服务器都不能发送 ACK，这是滑动窗口机制决定的，因此对于客户端来说，他完全不知道丢了几个包，可能就悲观的认为，5 后面的数据包也都丢了，就重传这 5 个数据包，这就比较浪费了。</p><h3 id="快速重传"><a href="#快速重传" class="headerlink" title="快速重传"></a>快速重传</h3><p>快速重传机制「RFC5681」基于接收端的反馈信息来引发重传，而非重传计时器超时。</p><p>刚刚提到过，基于计时器的重传往往要等待很长时间，而快速重传使用了很巧妙的方法来解决这个问题：<strong>服务器如果收到乱序的包，也给客户端回复 ACK</strong>，只不过是重复的 ACK。就拿刚刚的例子来说，收到乱序的包 6,7,8,9 时，服务器全都发 ACK = 5。这样，客户端就知道 5 发生了空缺。一般来说，如果客户端连续三次收到重复的 ACK，就会重传对应包，而不需要等到计时器超时。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-31/fast-transfer.png" srcset="/img/loading.gif" alt=""></p><p>但快速重传仍然没有解决第二个问题：到底该重传多少个包？</p><h3 id="带选择确认的重传"><a href="#带选择确认的重传" class="headerlink" title="带选择确认的重传"></a>带选择确认的重传</h3><p>改进的方法就是 SACK（Selective Acknowledgment），简单来讲就是在快速重传的基础上，<strong>返回最近收到的报文段的序列号范围</strong>，这样客户端就知道，哪些数据包已经到达服务器了。</p><p>来几个简单的示例：</p><ul><li><p>case 1：第一个包丢失，剩下的 7 个包都被收到了。</p><p>当收到 7 个包的<strong>任何一个</strong>的时候，接收方会返回一个带 SACK 选项的 ACK，告知发送方自己收到了哪些乱序包。注：<strong>Left Edge，Right Edge 就是这些乱序包的左右边界</strong>。</p></li></ul><pre><code class="hljs bash">Triggering    ACK      Left Edge   Right EdgeSegment5000         (lost)5500         5000     5500       60006000         5000     5500       65006500         5000     5500       70007000         5000     5500       75007500         5000     5500       80008000         5000     5500       85008500         5000     5500       9000</code></pre><ul><li><p>case 2：第 2, 4, 6, 8 个数据包丢失。</p></li><li><ul><li>收到第一个包时，没有乱序的情况，正常回复 ACK。</li><li>收到第 3, 5, 7 个包时，由于出现了乱序包，回复带 SACK 的 ACK。</li><li>因为这种情况下有很多碎片段，所以相应的 Block 段也有很多组，当然，因为选项字段大小限制， Block 也有上限。</li></ul></li></ul><pre><code class="hljs bash">Triggering  ACK    First Block   2nd Block     3rd BlockSegment            Left   Right  Left   Right  Left   Right                   Edge   Edge   Edge   Edge   Edge   Edge5000       55005500       (lost)6000       5500    6000   65006500       (lost)7000       5500    7000   7500   6000   65007500       (lost)8000       5500    8000   8500   7000   7500   6000   65008500       (lost)</code></pre><p>不过 SACK 的规范「RFC2018」有点坑爹，接收方可能会在提供一个 SACK 告诉发送方这些信息后，又「食言」，也就是说，接收方可能把这些（乱序的）数据包删除掉，然后再通知发送方。以下摘自「RFC2018」：</p><blockquote><p>Note that the data receiver is permitted to discard data in its queue that has not been acknowledged to the data sender, even if the data has already been reported in a SACK option. <strong>Such discarding of SACKed packets is discouraged, but may be used if the receiver runs out of buffer space.</strong></p></blockquote><p>最后一句是说，<strong>当接收方缓冲区快被耗尽时</strong>，可以采取这种措施，当然并不建议这种行为。。。</p><p>由于这个操作，发送方在收到 SACK 以后，也不能直接清空重传缓冲区里的数据，一直到接收方发送普通的，ACK 号大于其最大序列号的值的时候才能清除。另外，重传计时器也收到影响，重传计时器应该忽略 SACK 的影响，毕竟接收方把数据删了跟丢包没啥区别。</p><h3 id="DSACK-扩展"><a href="#DSACK-扩展" class="headerlink" title="DSACK 扩展"></a>DSACK 扩展</h3><p>DSACK，即重复 SACK，这个机制是在 SACK 的基础上，额外携带信息，<strong>告知发送方有哪些数据包自己重复接收了</strong>。DSACK 的目的是帮助发送方判断，是否发生了包失序、ACK 丢失、包重复或伪重传。让 TCP 可以更好的做网络流控。</p><p>关于 DSACK，「RFC2883」里举了很多例子，有兴趣的读者可以去阅读一下，我这里就不讲那么细了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> TCP 协议，握手挥手不是你想的那么简单 </title>
    <link href="/2019/12/30/2019-12-30-tcp-handshake/"/>
    <url>/2019/12/30/2019-12-30-tcp-handshake/</url>
    
    <content type="html"><![CDATA[<h2 id="TCP-报文段结构"><a href="#TCP-报文段结构" class="headerlink" title="TCP 报文段结构"></a>TCP 报文段结构</h2><p>一谈到 TCP 协议，大家最先想到的词就是「<strong>面向连接</strong>」和「<strong>可靠</strong>」。没错，TCP 协议的设计就是为了能够在客户端和服务器之间建立起一个可靠连接。</p><p>在讲连接过程之前，我们先来看看 TCP 的报文段结构，通过这个结构，我们可以知道 TCP 能够提供什么信息：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-30/tcp-message.png" srcset="/img/loading.gif" alt="TCP报文结构"></p><p>这里有几点是需要注意的：</p><ul><li>TCP 协议需要一个<strong>四元组</strong>（源IP，源端口，目的IP，目的端口）来确定连接，这要和 UDP 协议区分开。多说一句，IP 地址位于 IP 报文段，TCP 报文段是不含 IP 地址信息的。</li><li><strong>基本 TCP 头部</strong>的长度是 20 字节，但是由于「<strong>选项</strong>」的长度是不确定的，所以需要「<strong>首部长度</strong>」字段明确给出头部长度。这里要注意的是，首部长度字段的单位是 32bit，也就是 4 字节，所以该字段的最小值是 5。</li><li>标橙色的字段（<strong>确认序号，接收窗口大小，ECE，ACK</strong>）用于「回复」对方，举个例子，服务器收到对方的数据包后，不单独发一个数据包来回应，而是稍微等一下，把确认信息附在<strong>下一个</strong>发往<strong>客户端</strong>的数据帧上，也就是<strong>捎带</strong>技术。</li><li>窗口大小是一个 16 位无符号数，也就是说窗口被限制在了 65535 字节，也就限制了 TCP 的吞吐量性能，这对一些高速以及高延迟的网络不太友好（可以想想为什么）。所幸 TCP 额外提供了<strong>窗口缩放</strong>（Window Scale）选项，允许对这个值进行缩放。</li></ul><p>下面是 8 个标志位的含义，有的协议比较旧，可能没有前两个标志位：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-30/8flag.png" srcset="/img/loading.gif" alt="8个标志位的含义"></p><p>标志位虽然很多，但是如果放到具体场景里来看的话，就很容易理解他们的作用了。</p><h2 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h2><p>三次握手就是为了在客户端和服务器间建立连接，这个过程并不复杂，但里面有很多细节需要注意。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-30/three-handshake.png" srcset="/img/loading.gif" alt="三次握手"></p><p>这张图就是握手的过程，可以看到客户端与服务器之间一共传递了三次消息，这三次握手其实就是两台机器之间互相确认状态，我们来一点一点看。</p><p><strong>第一次握手</strong></p><p>首先是<strong>客户端发起连接</strong>，第一个数据包将 SYN 置位（也就是 SYN = 1），表明这个数据包是 SYN 报文段（也被称为<strong>段 1</strong>）。这一次发送的目的是告诉服务器，自己的<strong>初始序列号</strong>是 <code>client_isn</code> ，还有一个隐含的信息在图里没有表现出来，那就是告知服务端自己想连接的<strong>端口号</strong>。除了这些，客户端还会发送一些<strong>选项</strong>，不过这跟三次握手没多大关系，暂且按下不表。</p><p>段 1 里最需要注意的就是这个<code>client_isn</code> ，也就是初始序列号。「RFC0793^1」指出:</p><blockquote><p>When new connections are created, an initial sequence number (ISN) generator is employed which selects a new 32 bit ISN. The generator is bound to a (possibly fictitious) 32 bit clock whose low order bit is incremented roughly every 4 microseconds.  Thus, the ISN cycles approximately every 4.55 hours.</p></blockquote><p>翻译过来就是，初始序列号是一个 32 位的（虚拟）计数器，而且这个计数器每 4 微秒加 1，也就是说，ISN 的值<strong>每 4.55 小时循环一次</strong>。这个举措是为了<strong>防止序列号重叠</strong>。</p><p>但即使这样还是会有安全隐患——因为初始 ISN 仍然是可预测的，恶意程序可能会分析 ISN ，然后根据先前使用的 ISN <strong>预测</strong>后续 TCP 连接的 ISN，然后进行攻击，一个著名的例子就是「The Mitnick attack^2」 。这里摘一段原文：</p><blockquote><p>Mitnick sent SYN request to X-Terminal and received SYN/ACK response.  Then he sent RESET response to keep the X-Terminal from being filled up. He repeated this for twenty times. He found there is a pattern between  two successive TCP sequence numbers. It turned out that the numbers were not random at all. The latter number was greater than the previous one  by 128000.</p></blockquote><p>所以为了让初始序列号<strong>更难预测</strong>，现代系统常常使用<strong>半随机</strong>的方法选择初始序列号，详细的方法就不在这里展开了。</p><p><strong>第二次握手</strong></p><p>当服务器接收到客户端的连接请求后，就会向客户端发送 <strong>ACK</strong> 表示自己收到了连接请求，而且，服务器还得<strong>把自己的初始序列号告诉客户端</strong>，这其实是两个步骤，但是发送<strong>一个数据包</strong>就可以完成，用的就是前面说的<strong>捎带</strong>技术。图里的 <code>ACK = client_isn + 1</code> 是指<strong>确认号字段</strong>的值，要注意和 <strong>ACK 标志位</strong>区分开。</p><p>ACK 字段其实也有不少需要注意的点，不过这个跟滑动窗口一块讲比较直观，这里就先不提了。</p><p>这里重点强调一下，当一个 SYN 报文段到达的时候，<strong>服务器会检查处于 SYN_RCVD 状态的连接数目是否超过了 <code>tcp_max_syn_backlog</code> 这个参数，如果超过了，服务器就会拒绝连接</strong>。当然，这个也会被黑客所利用，「SYN Flood」就是个很好的例子。因为服务器在回复 SYN-ACK 后，会等待客户端的 ACK ，如果一定时间内没有收到，认为是丢包了，就重发 SYN-ACK，重复几次后才会断开这个连接，linux 可能要一分钟才会断开，所以攻击者如果制造一大批 SYN 请求而不回复，服务器的 SYN 队列很快就被耗尽，这一段时间里，正常的连接也会得不到响应。</p><p>服务器的这种状态称为<strong>静默</strong>（muted）。为了抵御 SYN Flood 攻击，服务器可以采用「SYN cookies」，这种思想是，当 SYN 到达时，<strong>并不直接为其分配内存</strong>，而是把这条连接的信息编码并保存在 SYN-ACK 报文段的<strong>序列号</strong>字段，如果客户端回复了，服务器再<strong>从 ACK 字段里解算出 SYN 报文的重要信息</strong>（有点黑魔法的感觉了），验证成功后才为该连接分配内存。这样，服务器不会响应攻击者的请求，正常连接则不会受到影响。</p><p>但 SYN cookies 本身有一些限制，并不适合作为默认选项，有兴趣可以自行 Google。</p><p><strong>第三次握手</strong></p><p>这是建立 TCP 连接的最后一步，经过前两次握手，客户端（服务器）已经知道对方的<strong>滑动窗口大小</strong>，<strong>初始序列号</strong>等信息了，这不就完了吗？为什么还要第三次握手？</p><p>这是因为服务器虽然把数据包发出去了，但他<strong>还不知道客户端是否收到了这个包</strong>，所以服务器需要等待客户端返回一个 ACK，表明客户端收到了数据，至此，连接完成。</p><h2 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h2><p>有了三次握手的基础，四次挥手就比较容易理解了：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-30/four-breakup.png" srcset="/img/loading.gif" alt="四次挥手"></p><p>四次挥手的过程其实很简单，就是服务器和客户端互相发送 FIN 和 ACK 报文段，告知对方要断开连接。</p><p>四次挥手里值得关注的一点就是 <strong>TIME_WAIT</strong> 状态，也就是说主动关闭连接的一方，即使收到了对方的 FIN 报文，也还要等待 2<strong>MSL</strong> 的时间才会彻底关闭这条连接。（这里面的 MSL 指的是<strong>最大段生成期</strong>，指的是报文段<strong>在网络中</strong>被允许存在的最长时间。）可<strong>为什么不直接关闭连接呢</strong>？</p><p>一个原因是，<strong>第四次挥手的 ACK 报文段不一定到达了服务器</strong>，为了不让服务器一直处于 LAST_ACK 状态（服务器会重发 FIN，<strong>直到收到 ACK</strong>），客户端还得等一会儿，看看是否需要重发。假如真的丢包了，服务器发送 FIN ，这个 FIN 报文到达客户端时不会超过 2MSL（一来一回最多 2MSL），这时候客户端这边的 TCP 还没关掉，还能重发 ACK。</p><p>另一个原因是，<strong>经过 2MSL 之后，网络中与该连接相关的包都已经消失</strong>了，不会干扰新连接。我们来看一个例子：假如客户端向服务器建立了<strong>新的连接</strong>，<strong>旧连接中某些延迟的数据坚持到了新连接建立完毕，而且序列号刚好还在滑动窗口内，服务器就误把它当成新连接的数据包接收</strong>，如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-30/example.png" srcset="/img/loading.gif" alt="四次挥手"></p><p>2MSL 机制就避免了这种情况。</p>]]></content>
    
    
    
    <tags>
      
      <tag>TCP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 浅谈 TCP 的三次握手和四次挥手 </title>
    <link href="/2019/12/27/2019-12-27-talk-tcp/"/>
    <url>/2019/12/27/2019-12-27-talk-tcp/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是OSI-七层模型"><a href="#什么是OSI-七层模型" class="headerlink" title="什么是OSI 七层模型?"></a>什么是OSI 七层模型?</h2><blockquote><p>开放式系统互联通信参考模型（英语：Open System Interconnection Reference Model，缩写为 OSI），简称为 OSI 模型（OSI model），一种概念模型，由国际标准化组织（ISO）提出，一个试图使各种计算机在世界范围内互连为网络的标准框架。定义于 ISO/IEC 7498-1。</p></blockquote><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-27/osi7.png" srcset="/img/loading.gif" alt="OSI七层模型"></p><ul><li>第 7 层应用层 ( Application Layer )</li></ul><p><strong>主要功能：</strong> 为应用软件提供接口，使应用程序能够使用网络服务<br><strong>典型设备：</strong> 网关<br><strong>典型协议、标准和应用：</strong> http(80)、ftp(20/21)、smtp(25)、pop3(110)、telnet(23)、dns(53)</p><ul><li>第 6 层表示层 ( Presentation Layer )</li></ul><p><strong>主要功能：</strong> 数据的解码和编码，数据的加密和解密，数据的压缩和解压缩<br><strong>典型设备：</strong> 网关<br><strong>典型协议、标准和应用：</strong> ASCLL、PICT、TIFF、JPEG、 MIDI、MPEG</p><ul><li>第 5 层会话层 ( Session Layer )</li></ul><p><strong>主要功能：</strong> 建立、维护、管理应用程序之间的会话<br><strong>典型设备：</strong> 网关<br><strong>典型协议、标准和应用：</strong> RPC、SQL、NFS 、X WINDOWS、ASP</p><ul><li>第 4 层传输层 (Transport Layer)</li></ul><p><strong>主要功能：</strong> 负责建立端到端的链接，保证保温在端到端之间的传输<br><strong>典型设备：</strong> 网关<br><strong>典型协议、标准和应用：</strong> TCP、UDP、SPX</p><ul><li>第 3 层网络层 (Network Layer)</li></ul><p><strong>主要功能：</strong> 负责将分组数据从源端传输到目的端，网络层的主要作用就是路由和寻址<br><strong>典型设备：</strong> 路由器<br><strong>典型协议、标准和应用：</strong> IP、IPX、APPLETALK、ICMP</p><ul><li>第 2 层数据链接层 (Data Link Layer)</li></ul><p><strong>主要功能：</strong> 在不可靠的物理链路上，提供可靠的数据传输服务<br><strong>典型设备：</strong> 交换机、网桥、网卡<br><strong>典型协议、标准和应用：</strong> 802.2、802.3ATM、HDLC、FRAME RELAY</p><ul><li>第 1 层物理层 (Physical Layer)</li></ul><p><strong>主要功能：</strong> 利用传输介质为数据链路层提供物理连接，实现比特流的透明传输<br><strong>典型设备：</strong> 集线器、中继器<br><strong>典型协议、标准和应用：</strong> V.35、EIA/TIA-232</p><ul><li>TCP/IP 协议族常用协议</li></ul><p><strong>应用层：</strong> TFTP，HTTP，SNMP，FTP，SMTP，DNS，Telnet 等等<br><strong>传输层：</strong> TCP，UDP<br><strong>网络层：</strong> IP，ICMP，OSPF，EIGRP，IGMP<br><strong>数据链路层：</strong> SLIP，CSLIP，PPP，MTU</p><h2 id="什么是-TCP-IP"><a href="#什么是-TCP-IP" class="headerlink" title="什么是  TCP/IP ?"></a>什么是  TCP/IP ?</h2><blockquote><p>互联网协议族（英语：Internet Protocol Suite，缩写为 IPS），是一个网络通信模型，以及一整个网络传输协议家族，为互联网的基础通信架构。它常被通称为 TCP/IP 协议族（英语：TCP/IP Protocol Suite，或 TCP/IP Protocols），简称 TCP/IP 。</p><p>因为这个协议家族的两个核心协议，包括TCP（传输控制协议）和 IP（网际协议），为这个家族中最早通过的标准。由于在网络通讯协议普遍采用分层的结构，当多个层次的协议共同工作时，类似计算机科学中的堆栈，因此又被称为 TCP/IP 协议栈（英语：TCP/IP Protocol Stack） 。</p><p>这些协议最早发源于美国国防部（缩写为 DoD）的ARPA 网项目，因此也被称作 DoD 模型（DoD Model）。这个协议套组由互联网工程任务组负责维护。</p><p>TCP/IP 提供点对点的链接机制，将数据应该如何封装、定址、传输、路由以及在目的地如何接收，都加以标准化。它将软件通信过程抽象化为四个抽象层，采取协议堆栈的方式，分别实现出不同通信协议。协议套组下的各种协议，依其功能不同，被分别归属到这四个层次结构之中，常被视为是简化的七层 OSI 模型。</p></blockquote><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-27/7-4-model.png" srcset="/img/loading.gif" alt="七层/四层模型"></p><p>在建立 TCP 连接之前需要进行三次握手，以便于链接到服务器，如果要断开服务器需要进行四次挥手，具体流程如下。</p><h2 id="TCP-IP-三次握手"><a href="#TCP-IP-三次握手" class="headerlink" title="TCP/IP 三次握手"></a>TCP/IP 三次握手</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-27/handshake.png" srcset="/img/loading.gif" alt="三次握手"></p><ol><li><p><strong>第一次握手：</strong> Client 将标志位 SYN 设置为 1，随机产生一个 Number 值 SEQ=100，并将数据发送给 Server，Client 进入 SYN_SENT 状态，等待 Server 确认；</p></li><li><p><strong>第二次握手：</strong> Server 收到数据包后 Client 设置的标志位 SYN=1 知道 Client 要求建立连接，Server 将标志位 SYN 和 ACK 都置为 1，并且发送一个确认序号 ACK=100+1，然后随机产生一个值 SEQ=130，并将该数据包发送给 Client 以确认连接请求，Server 进入 SYN_RCVD 状态。</p></li><li><p><strong>第三次握手：</strong> Client 收到确认后，检查 ACK 状态是否为 100+1，ACK 是否为 1，如果正确则将标志位 ACK 置为 1，ACK=130+1，并将该数据包发送给 Server，Server 检查 ACK 是否为 130+1，ACK 是否为1，如果正确则连接建立成功，Client 和 Server 进入 ESTABLISHED 状态，完成三次握手，随后 Client 与 Server 之间可以开始传输数据了。</p></li></ol><p>一个完整的三次握手也就是<strong>请求—应答—再次确认</strong></p><h2 id="TCP-IP-四次挥手"><a href="#TCP-IP-四次挥手" class="headerlink" title="TCP/IP 四次挥手"></a>TCP/IP 四次挥手</h2><p>为什么要挥手，简单点来说就是既然建立了链接，那么肯定还要断开连接吖，连接总不能一直占用吧，这样多浪费系统该资源，下面让我们来看看四次挥手的流程。</p><ol><li><p><strong>第一次挥手：</strong> Client 发送一个 FIN，用来关闭 Client 到 Server 的数据传送，Client 进入 FIN_WAIT_1 状态。</p></li><li><p><strong>第二次挥手：</strong> Server 收到 FIN 后，发送一个 ACK 给 Client，确认序号为 ACK=100+1（与 SYN 相同，一个 FIN 占用一个序号），Server 进入 CLOSE_WAIT 状态。</p></li><li><p><strong>第三次挥手：</strong> Server 发送一个 FIN，用来关闭 Server 到 Client 的数据传送，Server 进入 LAST_ACK 状态。</p></li><li><p><strong>第四次挥手：</strong> Client 收到 FIN 后，Client 进入 TIME_WAIT 状态，接着发送一个 ACK 给 Server，确认序号为 131+1，Server 进入 CLOSED 状态，完成四次挥手。</p></li></ol><h2 id="Q-A"><a href="#Q-A" class="headerlink" title="Q/A"></a>Q/A</h2><ul><li>为什么建立连接是三次握手，而关闭连接却是四次挥手呢？</li></ul><p>这是因为服务端在 LISTEN 状态下，收到建立连接请求的 SYN 报文后，把 ACK 和 SYN 放在一个报文里发送给客户端。而关闭连接时，当收到对方的 FIN 报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即 Close，也可以发送一些数据给对方后，再发送 FIN 报文给对方来表示同意现在关闭连接，因此，己方 ACK 和 FIN一般都会分开发送。</p><ul><li>为什么建立连接要三次握手？</li></ul><p><strong>目的：</strong> 防止已经失效的连接请求到达服务端，创建无效的连接，浪费资源。</p><p><strong>说明：</strong> 当客户端发出的第一个连接请求在网络上的某个节点被滞留了（网络会存在许多不可靠的因素），过一段时间后突然又到达了服务端，服务端误以为这是一个新的建立连接的请求，于是就会向客户端发出确认包并建立连接。</p><p>实际上客户端当前并没有发出创建连接的请求，就会丢弃服务端的确认包。而服务端却创建了连接并等待客户端发送数据，浪费了相关的资源。</p><ul><li>SYN 攻击</li></ul><p>在三次握手过程中，服务器<code>发送 SYN-ACK 之后，收到客户端的 ACK 之前</code>的 TCP 连接称为半连接 (half-open connect)。此时服务器处于 SYN_RECV 状态，当收到 ACK 后，服务器转入 ESTABLISHED 状态.</p><p>SYN 攻击就是：攻击客户端在短时间内伪造大量不存在的 IP 地址，向服务器不断地发送 SYN 包，服务器回复 ACK 确认包，并等待客户的确认从而建立连接。由于源地址是不存在的，不会再发送 ACK 确认包，所以服务器需要不断的重发直至超时，这些伪造的 SYN 包将长时间占用未连接队列，正常的 SYN 请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。</p><p>SYN 攻击是一个典型的 DDOS 攻击。检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源 IP 地址是随机的，基本上可以断定这是一次 SYN 攻击</p><ul><li>为什么 TIME_WAIT 状态需要经过 2MSL (最大报文段生存时间)才能返回到 CLOSE 状态？</li></ul><p>虽然按道理，四个报文都发送完毕，我们可以直接进入 CLOSE 状态了，但是我们必须假象网络是不可靠的，有可以最后一个 ACK 丢失。所以 TIME_WAIT 状态就是用来重发可能丢失的 ACK 报文。</p>]]></content>
    
    
    
    <tags>
      
      <tag>TCP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Nginx 中 reload 流程 </title>
    <link href="/2019/12/26/2019-12-26-nginx-reload/"/>
    <url>/2019/12/26/2019-12-26-nginx-reload/</url>
    
    <content type="html"><![CDATA[<p>今天这篇文章主要来介绍下 Nginx 的 reload 流程。实际上在之前文章中，在更改了 nginx 配置文件时，我们都会执行 nginx -s reload 命令，我们执行这条命令的原因是希望 nginx 不停止服务始终在处理新的请求的同时把 nginx 的配置文件平滑的把旧的 nginx.conf 配置更新为新的 nginx.conf 配置。</p><p>这样一个功能对于 nginx 非常有必要，但是有时候我们会发现在执行 <code>nginx -s reload</code> 命令后，worker 子进程的数量会变多了，这是因为老的配置运行的 worker 进程长时间没有退出，当使用 stream 做四层反向代理的时候，可能这种场景会更多。</p><p>那么下面我们通过分析 nginx 的 reload 流程，来探究下 nginx 到底做了些什么？所谓优雅的退出和立即退出有什么区别？</p><h2 id="reload-流程"><a href="#reload-流程" class="headerlink" title="reload 流程"></a>reload 流程</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-26/reload1.png" srcset="/img/loading.gif" alt=""></p><p>第一步在修改好 nginx 的配置文件 nginx.conf 后，向 master 进程发送 HUP 信号，这实际上和我们在命令行执行 <code>nginx -s reload</code> 命令效果是一样的。</p><p>那么 master 进程在收到 HUP 信号以后，会在第二步检查我们的配置文件语法是否正确，也就是说我们并不一定非要在 nginx -s reload 前执行 nginx -t 检验下语法是否正确，因为在第二步 nginx 的 master 进程一定会执行这个步骤。</p><p>在 nginx 的配置语法全部正确以后，master 进程会打开新的监听端口，为什么要在 master 进程中打开新的监听端口？因为我们可能在 nginx.conf 中会引入新的例如 443 或者之前我们没有打开的的监听端口，而所有 worker 进程是 master 进程 的子进程，子进程会继承父进程所有已经打开的端口，这是 linux 操作系统定义的，所以第三步，我们 master 进程打开了可能引入的新的监听端口。</p><p>接下来 mster 进程会用新的 nginx.conf 配置文件来启动新的 worker 子进程，那么老的 worker 子进程会怎么样呢？</p><p>我们会在第五步在启动新的 worker 子进程以后，由 master 进程再向老 worker 子进程发送 QUIT 信号，QUIT 信号和 TERM，INT 信号是不一样的，QUIT 信号是请优雅地关闭子进程，这时候需要关注顺序，因为 nginx 需要保证平滑，所以要先启动新的 worker 子进程，再向老的 worker 子进程发送 QUIT 信号。</p><p>那么老的 master 子进程收到 QUIT 信号后，首先关闭监听句柄，也就是说这个时候新的连接只会到新的 worker 子进程，所以虽然他们之间有时间差，但是时间是非常快速的，那么关闭监听句柄后，处理完当前连接后就结束进程。</p><p>下面看 reload 不停机载入新配置的图示。</p><h2 id="reload-不停机载入新配置"><a href="#reload-不停机载入新配置" class="headerlink" title="reload 不停机载入新配置"></a>reload 不停机载入新配置</h2><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-26/reload2.png" srcset="/img/loading.gif" alt=""></p><p>master 进程上原先有四个绿色的 worker 子进程，它们使用了老的配置，当我们更改了 nginx.conf 配置文件后，向 master 发送 SIGHUP 信号或者执行 reload 命令， 然后 master 会用新的配置文件启动四个新的黄色 worker 子进程，此时是四个老的绿色 worker 子进程和四个新的黄色的 worker 子进程是并存的。那么老的 worker 子进程在正常的情况下会在处理已经建立好的连接上的请求之后关闭这个连接，哪怕这个连接是 keeplive 请求也会正常关闭。</p><p>但是异常情况，如果有一些请求出现问题，客户端长时间无法处理，那么就会导致这个请求长时间停留在这个 worker 子进程当中，那么这个 worker 子进程会长时间存在，因为新的连接已经跑在黄色的 worker 子进程中，所以影响并不会很大，唯一会影响的就是绿色的 worker 子进程会长时间存在，但也只影响已存在的连接，不会影响新的连接。</p><p>我们有什么办法处理呢？在新版本中提供了一个新的配置 worker_shutdown_timeout，也就是说最长等待多长时间，这样 master 进程启动新的黄色 worker 进程之后，如果老的 worker 进程一直没有退出，时间到了之后会强制把老的 worker 进程退出掉。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> linux中inode包含什么内容？ </title>
    <link href="/2019/12/25/2019-12-25-linux-inode/"/>
    <url>/2019/12/25/2019-12-25-linux-inode/</url>
    
    <content type="html"><![CDATA[<h2 id="1、inode是什么"><a href="#1、inode是什么" class="headerlink" title="1、inode是什么"></a>1、inode是什么</h2><p>理解inode，要从文件储存说起。</p><p>文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。</p><p>操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。</p><p>文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。</p><h2 id="2、inode内容"><a href="#2、inode内容" class="headerlink" title="2、inode内容"></a>2、inode内容</h2><p>inode包含文件的元信息，具体来说有以下内容：</p><p>* 文件的字节数</p><p>* 文件拥有者的User ID</p><p>* 文件的Group ID</p><p>* 文件的读、写、执行权限</p><p>* 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。</p><p>* 链接数，即有多少文件名指向这个inode</p><p>* 文件数据block的位置</p><p>可以用stat命令，查看某个文件的inode信息：</p><p>stat example.txt</p><p>总之，除了文件名以外的所有文件信息，都存在inode之中。至于为什么没有文件名，下文会有详细解释。</p><h2 id="3、inode的大小"><a href="#3、inode的大小" class="headerlink" title="3、inode的大小"></a>3、inode的大小</h2><p>inode也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是inode区（inode table），存放inode所包含的信息。</p><p>每个inode节点的大小，一般是128字节或256字节。inode节点的总数，在格式化时就给定，一般是每1KB或每2KB就设置一个inode。假定在一块1GB的硬盘中，每个inode节点的大小为128字节，每1KB就设置一个inode，那么inode table的大小就会达到128MB，占整块硬盘的12.8%。</p><p>查看每个硬盘分区的inode总数和已经使用的数量，可以使用df命令。</p><pre><code class="hljs bash">df -i</code></pre><p>查看每个inode节点的大小，可以用如下命令：</p><pre><code class="hljs bash">sudo dumpe2fs -h /dev/hda | grep <span class="hljs-string">"Inode size"</span></code></pre><p>由于每个文件都必须有一个inode，因此有可能发生inode已经用光，但是硬盘还未存满的情况。这时，就无法在硬盘上创建新文件。</p><h2 id="4、inode号码"><a href="#4、inode号码" class="headerlink" title="4、inode号码"></a>4、inode号码</h2><p>每个inode都有一个号码，操作系统用inode号码来识别不同的文件。</p><p>这里值得重复一遍，Unix/Linux系统内部不使用文件名，而使用inode号码来识别文件。对于系统来说，文件名只是inode号码便于识别的别称或者绰号。表面上，用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：首先，系统找到这个文件名对应的inode号码；其次，通过inode号码，获取inode信息；最后，根据inode信息，找到文件数据所在的block，读出数据。</p><p>使用ls -i命令，可以看到文件名对应的inode号码：</p><pre><code class="hljs bash">ls -i example.txt</code></pre><h2 id="5、目录文件"><a href="#5、目录文件" class="headerlink" title="5、目录文件"></a>5、目录文件</h2><p>Unix/Linux系统中，目录（directory）也是一种文件。打开目录，实际上就是打开目录文件。</p><p>目录文件的结构非常简单，就是一系列目录项（dirent）的列表。每个目录项，由两部分组成：所包含文件的文件名，以及该文件名对应的inode号码。</p><p>ls命令只列出目录文件中的所有文件名：</p><pre><code class="hljs bash">ls /etc</code></pre><p>ls -i命令列出整个目录文件，即文件名和inode号码：</p><pre><code class="hljs bash">ls -i /etc</code></pre><p>如果要查看文件的详细信息，就必须根据inode号码，访问inode节点，读取信息。ls -l命令列出文件的详细信息。</p><pre><code class="hljs bash">ls -l /etc</code></pre><h2 id="6、硬链接"><a href="#6、硬链接" class="headerlink" title="6、硬链接"></a>6、硬链接</h2><p>一般情况下，文件名和inode号码是”一一对应”关系，每个inode号码对应一个文件名。但是，Unix/Linux系统允许，多个文件名指向同一个inode号码。这意味着，可以用不同的文件名访问同样的内容；对文件内容进行修改，会影响到所有文件名；但是，删除一个文件名，不影响另一个文件名的访问。这种情况就被称为”硬链接”（hard link）。</p><p>ln命令可以创建硬链接：</p><p>ln 源文件 目标文件</p><p>运行上面这条命令以后，源文件与目标文件的inode号码相同，都指向同一个inode。inode信息中有一项叫做”链接数”，记录指向该inode的文件名总数，这时就会增加1。反过来，删除一个文件名，就会使得inode节点中的”链接数”减1。当这个值减到0，表明没有文件名指向这个inode，系统就会回收这个inode号码，以及其所对应block区域。</p><p>这里顺便说一下目录文件的”链接数”。创建目录时，默认会生成两个目录项：”.”和”..”。前者的inode号码就是当前目录的inode号码，等同于当前目录的”硬链接”；后者的inode号码就是当前目录的父目录的inode号码，等同于父目录的”硬链接”。所以，任何一个目录的”硬链接”总数，总是等于2加上它的子目录总数（含隐藏目录）,这里的2是父目录对其的“硬链接”和当前目录下的”.硬链接“。</p><h2 id="7、软连接"><a href="#7、软连接" class="headerlink" title="7、软连接"></a>7、软连接</h2><p>除了硬链接以外，还有一种特殊情况。文件A和文件B的inode号码虽然不一样，但是文件A的内容是文件B的路径。读取文件A时，系统会自动将访问者导向文件B。因此，无论打开哪一个文件，最终读取的都是文件B。这时，文件A就称为文件B的”软链接”（soft link）或者”符号链接（symbolic link）。</p><p>这意味着，文件A依赖于文件B而存在，如果删除了文件B，打开文件A就会报错：”No such file or directory”。这是软链接与硬链接最大的不同：文件A指向文件B的文件名，而不是文件B的inode号码，文件B的inode”链接数”不会因此发生变化。</p><p>ln -s命令可以创建软链接。</p><p>ln -s 源文文件或目录 目标文件或目录</p><h2 id="8、inode的特殊作用"><a href="#8、inode的特殊作用" class="headerlink" title="8、inode的特殊作用"></a>8、inode的特殊作用</h2><p>由于inode号码与文件名分离，这种机制导致了一些Unix/Linux系统特有的现象。</p><p>\1. 有时，文件名包含特殊字符，无法正常删除。这时，直接删除inode节点，就能起到删除文件的作用。</p><p>\2. 移动文件或重命名文件，只是改变文件名，不影响inode号码。</p><p>\3. 打开一个文件以后，系统就以inode号码来识别这个文件，不再考虑文件名。因此，通常来说，系统无法从inode号码得知文件名。</p><p>第3点使得软件更新变得简单，可以在不关闭软件的情况下进行更新，不需要重启。因为系统通过inode号码，识别运行中的文件，不通过文件名。更新的时候，新版文件以同样的文件名，生成一个新的inode，不会影响到运行中的文件。等到下一次运行这个软件的时候，文件名就自动指向新版文件，旧版文件的inode则被回收。</p><h2 id="9、实际问题"><a href="#9、实际问题" class="headerlink" title="9、实际问题"></a>9、实际问题</h2><p>在一台配置较低的Linux服务器（内存、硬盘比较小）的/data分区内创建文件时，系统提示磁盘空间不足，用df -h命令查看了一下磁盘使用情况，发现/data分区只使用了66%，还有12G的剩余空间，按理说不会出现这种问题。 后来用df -i查看了一下/data分区的索引节点(inode)，发现已经用满(IUsed=100%)，导致系统无法创建新目录和文件。</p><p><strong>查找原因：</strong></p><p>/data/cache目录中存在数量非常多的小字节缓存文件，占用的Block不多，但是占用了大量的inode。</p><p><strong>解决方案：</strong></p><p>1、删除/data/cache目录中的部分文件，释放出/data分区的一部分inode。<br>2、用软连接将空闲分区/opt中的newcache目录连接到/data/cache，使用/opt分区的inode来缓解/data分区inode不足的问题：</p><pre><code class="hljs bash">ln -s /opt/newcache /data/cache</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> HTTPS 原理分析 </title>
    <link href="/2019/12/24/2019-12-24-https-principle/"/>
    <url>/2019/12/24/2019-12-24-https-principle/</url>
    
    <content type="html"><![CDATA[<p>随着 HTTPS 建站的成本下降，现在大部分的网站都已经开始用上 HTTPS 协议。大家都知道 HTTPS 比 HTTP 安全，也听说过与 HTTPS 协议相关的概念有 SSL 、非对称加密、 CA证书等，但对于以下灵魂三拷问可能就答不上了：</p><ol><li>为什么用了 HTTPS 就是安全的？</li><li>HTTPS 的底层原理如何实现？</li><li>用了 HTTPS 就一定安全吗？</li></ol><p>本文将层层深入，从原理上把 HTTPS 的安全性讲透。</p><h2 id="HTTPS-的实现原理"><a href="#HTTPS-的实现原理" class="headerlink" title="HTTPS 的实现原理"></a>HTTPS 的实现原理</h2><p>大家可能都听说过 HTTPS 协议之所以是安全的是因为 HTTPS 协议会对传输的数据进行加密，而加密过程是使用了非对称加密实现。但其实，HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。</p><p>HTTPS的整体过程分为证书验证和数据传输阶段，具体的交互过程如下：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-24/1.png" srcset="/img/loading.gif" alt=""></p><p>1、证书验证阶段</p><ul><li>浏览器发起 HTTPS 请求</li><li>服务端返回 HTTPS 证书</li><li>客户端验证证书是否合法，如果不合法则提示告警</li></ul><p>2、数据传输阶段</p><ul><li>当证书验证合法后，在本地生成随机数</li><li>通过公钥加密随机数，并把加密后的随机数传输到服务端</li><li>服务端通过私钥对随机数进行解密</li><li>服务端通过客户端传入的随机数构造对称加密算法，对返回结果内容进行加密后传输</li></ul><h2 id="为什么数据传输是用对称加密？"><a href="#为什么数据传输是用对称加密？" class="headerlink" title="为什么数据传输是用对称加密？"></a>为什么数据传输是用对称加密？</h2><p>首先，非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的；</p><p>另外，在 HTTPS 的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加解密，所以 HTTPS 中内容传输加密采取的是对称加密，而不是非对称加密。</p><h2 id="为什么需要-CA-认证机构颁发证书？"><a href="#为什么需要-CA-认证机构颁发证书？" class="headerlink" title="为什么需要 CA 认证机构颁发证书？"></a>为什么需要 CA 认证机构颁发证书？</h2><p>HTTP 协议被认为不安全是因为传输过程容易被监听者勾线监听、伪造服务器，而 HTTPS 协议主要解决的便是网络传输的安全性问题。</p><p>首先我们假设不存在认证机构，任何人都可以制作证书，这带来的安全风险便是经典的“中间人攻击”问题。“中间人攻击”的具体过程如下：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-24/2.png" srcset="/img/loading.gif" alt=""></p><p>过程原理：</p><ol><li>本地请求被劫持（如DNS劫持等），所有请求均发送到中间人的服务器</li><li>中间人服务器返回中间人自己的证书</li><li>客户端创建随机数，通过中间人证书的公钥对随机数加密后传送给中间人，然后凭随机数构造对称加密对传输内容进行加密传输</li><li>中间人因为拥有客户端的随机数，可以通过对称加密算法进行内容解密</li><li>中间人以客户端的请求内容再向正规网站发起请求</li><li>因为中间人与服务器的通信过程是合法的，正规网站通过建立的安全通道返回加密后的数据</li><li>中间人凭借与正规网站建立的对称加密算法对内容进行解密</li><li>中间人通过与客户端建立的对称加密算法对正规内容返回的数据进行加密传输</li><li>客户端通过与中间人建立的对称加密算法对返回结果数据进行解密</li></ol><p>由于缺少对证书的验证，所以客户端虽然发起的是 HTTPS 请求，但客户端完全不知道自己的网络已被拦截，传输内容被中间人全部窃取。</p><h2 id="浏览器是如何确保-CA-证书的合法性？"><a href="#浏览器是如何确保-CA-证书的合法性？" class="headerlink" title="浏览器是如何确保 CA 证书的合法性？"></a>浏览器是如何确保 CA 证书的合法性？</h2><p>1、证书包含什么信息？<br>颁发机构信息 公钥 公司信息 域名 有效期 指纹 ……</p><p>2、证书的合法性依据是什么？<br>首先，权威机构是要有认证的，不是随便一个机构都有资格颁发证书，不然也不叫做权威机构。另外，证书的可信性基于信任制，权威机构需要对其颁发的证书进行信用背书，只要是权威机构生成的证书，我们就认为是合法的。所以权威机构会对申请者的信息进行审核，不同等级的权威机构对审核的要求也不一样，于是证书也分为免费的、便宜的和贵的。</p><p>3、浏览器如何验证证书的合法性？<br>浏览器发起 HTTPS 请求时，服务器会返回网站的 SSL 证书，浏览器需要对证书做以下验证：</p><ul><li>验证域名、有效期等信息是否正确。证书上都有包含这些信息，比较容易完成验证；</li><li>判断证书来源是否合法。每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证；</li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-24/3.png" srcset="/img/loading.gif" alt=""></p><ul><li>判断证书是否被篡改。需要与 CA 服务器进行校验；</li><li>判断证书是否已吊销。通过CRL（Certificate Revocation List 证书注销列表）和 OCSP（Online Certificate Status Protocol 在线证书状态协议）实现，其中 OCSP 可用于第3步中以减少与 CA 服务器的交互，提高验证效率</li></ul><p>以上任意一步都满足的情况下浏览器才认为证书是合法的。</p><blockquote><p>这里插一个我想了很久的但其实答案很简单的问题：</p><p>既然证书是公开的，如果要发起中间人攻击，我在官网上下载一份证书作为我的服务器证书，那客户端肯定会认同这个证书是合法的，如何避免这种证书冒用的情况？</p><p>其实这就是非加密对称中公私钥的用处，虽然中间人可以得到证书，但私钥是无法获取的，一份公钥是不可能推算出其对应的私钥，中间人即使拿到证书也无法伪装成合法服务端，因为无法对客户端传入的加密数据进行解密。</p></blockquote><p>4、只有认证机构可以生成证书吗？</p><p>如果需要浏览器不提示安全风险，那只能使用认证机构签发的证书。但浏览器通常只是提示安全风险，并不限制网站不能访问，所以从技术上谁都可以生成证书，只要有证书就可以完成网站的 HTTPS 传输。例如早期的 12306 采用的便是手动安装私有证书的形式实现 HTTPS 访问。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-24/4.png" srcset="/img/loading.gif" alt=""></p><h2 id="本地随机数被窃取怎么办？"><a href="#本地随机数被窃取怎么办？" class="headerlink" title="本地随机数被窃取怎么办？"></a>本地随机数被窃取怎么办？</h2><p>证书验证是采用非对称加密实现，但是传输过程是采用对称加密，而其中对称加密算法中重要的随机数是由本地生成并且存储于本地的，HTTPS 如何保证随机数不会被窃取？</p><p>其实 HTTPS 并不包含对随机数的安全保证，HTTPS 保证的只是传输过程安全，而随机数存储于本地，本地的安全属于另一安全范畴，应对的措施有安装杀毒软件、反木马、浏览器升级修复漏洞等。</p><h2 id="用了-HTTPS-会被抓包吗？"><a href="#用了-HTTPS-会被抓包吗？" class="headerlink" title="用了 HTTPS 会被抓包吗？"></a>用了 HTTPS 会被抓包吗？</h2><p>HTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看。</p><p>但是，正如前文所说，浏览器只会提示安全风险，如果用户授权仍然可以继续访问网站，完成请求。因此，只要客户端是我们自己的终端，我们授权的情况下，便可以组建中间人网络，而抓包工具便是作为中间人的代理。通常 HTTPS 抓包工具的使用方法是会生成一个证书，用户需要手动把证书安装到客户端中，然后终端发起的所有请求通过该证书完成与抓包工具的交互，然后抓包工具再转发请求到服务器，最后把服务器返回的结果在控制台输出后再返回给终端，从而完成整个请求的闭环。</p><p>既然 HTTPS 不能防抓包，那 HTTPS 有什么意义？</p><p>HTTPS 可以防止用户在不知情的情况下通信链路被监听，对于主动授信的抓包操作是不提供防护的，因为这个场景用户是已经对风险知情。要防止被抓包，需要采用应用级的安全防护，例如采用私有的对称加密，同时做好移动端的防反编译加固，防止本地算法被破解。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以下用简短的 Q&amp;A 形式进行全文总结：</p><ul><li>Q: HTTPS 为什么安全？</li></ul><p>​    A: 因为 HTTPS 保证了传输安全，防止传输过程被监听、防止数据被窃取，可以确认网站的真实性。</p><ul><li>Q: HTTPS 的传输过程是怎样的？</li></ul><p>​    A: 客户端发起 HTTPS 请求，服务端返回证书，客户端对证书进行验证，验证通过后本地生成用于改造对称加密算法的随机数，通过证书中的公钥对随机数进行加密传输到服务端，服务端接收后通过私钥解密得到随机数，之后的数据交互通过对称加密算法进行加解密。</p><ul><li>Q: 为什么需要证书？</li></ul><p>​    A: 防止”中间人“攻击，同时可以为网站提供身份证明。</p><ul><li>Q: 使用 HTTPS 会被抓包吗？</li></ul><p>​    A: 会被抓包，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。</p><p>最后顺手分享一张学习 HTTPS  的过程图。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-24/5.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>HTTP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 云原生云景高清大图 </title>
    <link href="/2019/12/23/2019-12-23-cloudnativemap/"/>
    <url>/2019/12/23/2019-12-23-cloudnativemap/</url>
    
    <content type="html"><![CDATA[<p><strong>云原生地图</strong></p><p>Cloud Native Trail Map ，是由CNCF发布的云原生云景大图，为企业拥抱云原生指明方向，地图涵盖：容器Registry，存储，容器运行时，网络，编排系统，服务发现，服务代理，API网关，服务网格，数据库，CI/CD等。（地图最后更新时间：2019.12.29）</p><p>图片较大，加载速度较为缓慢，请耐心等待。。。</p><p>图片下载地址：链接: <a href="https://pan.baidu.com/s/1m8Gqt3Rw974P42Hcy3_jnw" target="_blank" rel="noopener">https://pan.baidu.com/s/1m8Gqt3Rw974P42Hcy3_jnw</a> 提取码: 7bc3    </p><p>(含PDF和PNG格式)</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-23/landscape.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>CloudNative</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Nginx 原理和架构 </title>
    <link href="/2019/12/22/2019-12-22-nginx-principle/"/>
    <url>/2019/12/22/2019-12-22-nginx-principle/</url>
    
    <content type="html"><![CDATA[<h1 id="1、Nginx-的整体架构"><a href="#1、Nginx-的整体架构" class="headerlink" title="1、Nginx 的整体架构"></a>1、Nginx 的整体架构</h1><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-22/191222principle1.png" srcset="/img/loading.gif" alt=""></p><p>Nginx 里有一个 master 进程和多个 worker 进程。master 进程并不处理网络请求，主要负责调度工作进程：加载配置、启动工作进程及非停升级。worker 进程负责处理网络请求与响应。</p><p><strong>1.1. 主进程</strong></p><p>master进程主要用来管理worker进程，具体包括如下4个主要功能：</p><ol><li>接收来自外界的信号。</li><li>向各worker进程发送信号。</li><li>监控woker进程的运行状态。</li><li>当woker进程退出后（异常情况下），会自动重新启动新的woker进程。</li></ol><p><strong>1.2. 工作进程</strong></p><p>woker进程主要用来处理基本的网络事件：</p><ol><li>多个worker进程之间是对等且相互独立的，他们同等竞争来自客户端的请求。</li><li>一个请求，只可能在一个worker进程中处理，一个worker进程，不可能处理其它进程的请求。</li><li>worker进程的个数是可以设置的，一般我们会设置与机器cpu核数一致。同时，nginx为了更好的利用多核特性，具有cpu绑定选项，我们可以将某一个进程绑定在某一个核上，这样就不会因为进程的切换带来cache的失效。</li></ol><p><strong>1.3. 模块化设计</strong></p><p>Nginx的worker进程，包括核心和功能性模块，核心模块负责维持一个运行循环 （ run-loop ），执行网络请求处理的 不同阶段 的模块功能。</p><p>比如： 网络读写 、 存储读写、 内容传输 、 外出过滤 ，以及将请求发往上游服务器等。</p><p>而其代码的模块化设计 ，也使得我们可以根据需要对 功能模块 进行适当的 选择 和 修改 ，编译成具有 特定功能的服务器。</p><p><strong>1.4. 事件驱动模型</strong></p><p>基于 异步及非阻塞的事件驱动模型 ，可以说是 Nginx 得以获得高并发、高性能的关键因素，同时也得益于对 Linux 、 Solaris 及类 BSD 等操作系统内核中 事件通知 及 I/O 性能增强功能 的采用，如 kqueue 、 epoll 及 event ports 。</p><p><strong>1.5. 代理（proxy）设计</strong></p><p>代理设计，可以说是 Nginx 深入骨髓的设计，无论是对于 HTTP ，还是对于 FastCGI 、 Memcache 、 Redis 等的网络请求或响应，本质上都采用了 代理机制 。所以， Nginx 天生就是高性能的 代理服务器 。</p><h1 id="2、Nginx的模块化设计"><a href="#2、Nginx的模块化设计" class="headerlink" title="2、Nginx的模块化设计"></a>2、Nginx的模块化设计</h1><p>高度模块化的设计是 Nginx 的架构基础。Nginx 服务器被分解为多个模块 ，每个模块就是一个功能模块 ，只负责自身的功能，模块之间严格遵循 “高内聚，低耦合” 的原则。</p><p>如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-22/191222principle2.png" srcset="/img/loading.gif" alt=""></p><p><strong>2.1. 核心模块</strong></p><p>核心模块是 Nginx 服务器正常运行 必不可少的模块，提供错误日志记录 、 配置文件解析 、 事件驱动机制 、 进程管理 等核心功能。</p><p><strong>2.2. 标准HTTP模块</strong></p><p>标准 HTTP 模块提供 HTTP 协议解析相关的功能，比如： 端口配置 、 网页编码设置 、 HTTP响应头设置 等等。</p><p><strong>2.3. 可选HTTP模块</strong></p><p>可选 HTTP 模块主要用于 扩展 标准的 HTTP 功能，让 Nginx 能处理一些特殊的服务，比如：Flash 多媒体传输 、解析 GeoIP 请求、 网络传输压缩 、 安全协议 SSL 支持等。</p><p><strong>2.4. 邮件服务模块</strong></p><p>邮件服务模块主要用于支持 Nginx 的 邮件服务 ，包括对 POP3 协议、 IMAP 协议和 SMTP协议的支持。</p><p><strong>2.5. 第三方模块</strong></p><p>第三方模块是为了扩展 Nginx 服务器应用，完成开发者自定义功能，比如：Json 支持、 Lua 支持等。</p><h1 id="3、Nginx的请求方式处理"><a href="#3、Nginx的请求方式处理" class="headerlink" title="3、Nginx的请求方式处理"></a>3、Nginx的请求方式处理</h1><p>Nginx 是一个 高性能 的 Web 服务器，能够同时处理大量的并发请求 。它结合多进程机制和 异步机制 ，异步机制使用的是 异步非阻塞方式 ，接下来就给大家介绍一下 Nginx 的 多线程机制 和 异步非阻塞机制 。</p><p><strong>3.1. 多进程机制</strong></p><p>服务器每当收到一个客户端时，就有 服务器主进程 （ master process ）生成一个 子进程（ worker process ）出来和客户端建立连接进行交互，直到连接断开，该子进程就结束了。</p><p>使用 进程 的好处是 各个进程之间相互独立 ， 不需要加锁 ，减少了使用锁对性能造成影响，同时降低编程的复杂度，降低开发成本。</p><p>其次，采用独立的进程，可以让 进程互相之间不会影响 ，如果一个进程发生异常退出时，其它进程正常工作， master 进程则很快启动新的 worker 进程，确保服务不会中断，从而将风险降到最低。</p><p>缺点是操作系统生成一个 子进程 需要进行 内存复制 等操作，在 资源 和 时间 上会产生一定的开销。当有 大量请求 时，会导致 系统性能下降 。</p><p><strong>3.2. 异步非阻塞机制</strong></p><p>每个 工作进程 使用 异步非阻塞方式 ，可以处理多个客户端请求 。</p><p>当某个 工作进程 接收到客户端的请求以后，调用 IO 进行处理，如果不能立即得到结果，就去处理其他请求 （即为非阻塞 ），而客户端在此期间也无需等待响应 ，可以去处理其他事情（即为异步 ）</p><p>当 IO 返回时，就会通知此工作进程，该进程得到通知，暂时挂起当前处理的事务去 响应客户端请求 。</p><h1 id="4、Nginx事件驱动模型"><a href="#4、Nginx事件驱动模型" class="headerlink" title="4、Nginx事件驱动模型"></a>4、Nginx事件驱动模型</h1><p>在 Nginx 的 异步非阻塞机制 中， 工作进程在调用 IO 后，就去处理其他的请求，当 IO 调用返回后，会通知该工作进程 。</p><p>对于这样的系统调用，主要使用 Nginx 服务器的<strong>事件驱动模型</strong>来实现，如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-22/191222principle3.png" srcset="/img/loading.gif" alt=""></p><p>如上图所示， Nginx 的 事件驱动模型 由 事件收集器 、 事件发送器 和 事件处理器 三部分基本单元组成。</p><ul><li><strong>事件收集器</strong>：负责收集 worker 进程的各种 IO 请求；</li><li><strong>事件发送器</strong>：负责将 IO 事件发送到 事件处理器 ；</li><li><strong>事件处理器</strong>：负责各种事件的 响应工作 。</li></ul><p>事件发送器将每个请求放入一个 待处理事件列表 ，使用非阻塞 I/O 方式调用 事件处理器来处理该请求。</p><p>其处理方式称为 “多路 IO 复用方法” ，常见的包括以下三种：select 模型、 poll模型、 epoll 模型。</p><h1 id="5、Nginx进程处理模型"><a href="#5、Nginx进程处理模型" class="headerlink" title="5、Nginx进程处理模型"></a>5、Nginx进程处理模型</h1><p>Nginx 服务器使用 <strong>master/worker</strong> 多进程模式，多线程启动和执行的流程如下：</p><ol><li>主程序Master process启动后，通过一个 for 循环来接收和处理外部信号</li></ol><ol start="2"><li>主进程通过 fork() 函数产生 worker 子进程 ，每个 子进程 执行一个 for 循环来实现 Nginx 服务器 对事件的接收 和 处理 </li></ol><p>一般推荐 worker 进程数 与 CPU 内核数 一致，这样一来不存在 大量的子进程 生成和管理任务，避免了进程之间 竞争 CPU 资源 和 进程切换 的开销。</p><p>而且 Nginx 为了更好的利用 多核特性 ，提供了 CPU 亲缘性 的绑定选项，我们可以将某 一个进程绑定在某一个核上，这样就不会因为 进程的切换 带来 Cache 的失效。</p><p>对于每个请求，有且只有一个 工作进程 对其处理。首先，每个 worker 进程都是从 master进程 fork 过来。在 master 进程里面，先建立好需要 listen 的 socket（listenfd） 之后，然后再 fork 出多个 worker 进程。</p><p>所有 worker 进程的 listenfd 会在 新连接 到来时变得 可读 ，为保证只有一个进程处理该连接，所有 worker 进程在注册 listenfd 读事件前抢占 accept_mutex </p><p>抢到 互斥锁 的那个进程注册 listenfd 读事件 ，在读事件里调用 accept 接受该连接。</p><p>当一个 worker 进程在 accept 这个连接之后，就开始读取请求 ， 解析请求 ， 处理请求，产生数据后，再返回给客户端 ，最后才断开连接 ，一个完整的请求就是这样。</p><p>我们可以看到，一个请求，完全由 worker 进程来处理，而且只在一个 worker 进程中处理。</p><p>如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-22/191222principle4.png" srcset="/img/loading.gif" alt=""></p><p>在 Nginx 服务器的运行过程中， 主进程 和 工作进程 需要进程交互。交互依赖于 Socket 实现的管道来实现。</p><p><strong>5.1. 主进程与工作进程交互</strong></p><p>这条管道与普通的管道不同，它是由 主进程 指向 工作进程 的单向管道 ，包含主进程向工作进程发出的指令工，作进程 ID 等。同时主进程与外界通过信号通信 ；每个子进程具备接收信号 ，并处理相应的事件的能力。</p><p><strong>5.2. 工作进程与工作进程交互</strong></p><p>这种交互和 主进程-工作进程 交互基本一致，但是会通过主进程间接完成，工作进程之间是相互隔离的。</p><p>所以当工作进程 W1 需要向工作进程 W2 发指令时，首先找到 W2 的 进程ID ，然后将正确的指令写入指向 W2 的 通道，W2 收到信号采取相应的措施。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 运维工程师必知必会的一些定律 </title>
    <link href="/2019/12/21/2019-12-21-operation-law/"/>
    <url>/2019/12/21/2019-12-21-operation-law/</url>
    
    <content type="html"><![CDATA[<h1 id="墨菲定律"><a href="#墨菲定律" class="headerlink" title="墨菲定律"></a>墨菲定律</h1><p><strong>墨菲定律</strong>，指的是小概率事情，只要某件事情发生概率不为0，如果样本数足够多，那么这个事情最终会发生。</p><h1 id="海恩法则"><a href="#海恩法则" class="headerlink" title="海恩法则"></a>海恩法则</h1><p><strong>海恩法则</strong>，每一起严重事故的背后，必然有29次轻微事故和300起未遂先兆以及1000起事故隐患。法则强调两点：一是事故的发生是量的积累的结果；二是再好的技术，再完美的规章，在实际操作层面，也无法取代人自身的素质和责任心。</p><p>结合上面的墨菲定律，是不是所有的事故隐患都需要立即处理，都需要当做非常重要的事情呢？答案是否定的，有如下两点考虑：</p><ul><li>不是所有轻微事故的原因都能导致严重事故，因此需要对事故原因进行深入分析，重点放在可能导致严重事故的那部分上。</li><li>事不过三，如果一个问题已经发生了两次，不论概率大小，一定会发生第三次时，那么不管是否为轻微事故，都需要着手处理。</li></ul><h1 id="因果连锁理论"><a href="#因果连锁理论" class="headerlink" title="因果连锁理论"></a>因果连锁理论</h1><p><strong>因果连锁理论</strong>，伤亡事故的发生不是一个孤立的事件，尽管伤害可能在某瞬间突然发生，却是一系列事件相继发生的结果。也称为多米诺骨牌理论。</p><p>从因果连锁理论我们也可以引申出运维的两点内容，首先运维是整个系统可用性保障的最后一道门槛，如果这道门槛不轻易倒下，那么系统的可用性就能够得到保障；其次，需要更宏观的来看系统的可用性保障，不仅仅是运维，还有测试，研发，只要有一个环节能够拦截故障，那么故障就不会真正的发生。所以，我们既要加强自身的能力建设，同时，也需要和研发，测试更好的合作，从而降低最后一道门槛的压力。</p><h1 id="二八定律"><a href="#二八定律" class="headerlink" title="二八定律"></a>二八定律</h1><p><strong>二八定律</strong>，在任何一组东西中，最重要的只占其中一小部分，约20%，其余80%尽管是多数，却是次要的，也成为关键少数法则。</p><p>二八定律在运维中是非常重要的，举几个例子，大家举一反三，多找找运维领域中符合二八定律的场景。</p><ul><li>80%的广告收入由20%的业务贡献，那么在资源有限的情况下，通过保证这20%的业务可用性，就可以确保收入不会出现较大损失</li><li>80%的报警由少数无效策略导致，那么只要优化这些策略，就可以大幅减少报警数量</li></ul><h1 id="帕金森定律和布鲁克斯法则"><a href="#帕金森定律和布鲁克斯法则" class="headerlink" title="帕金森定律和布鲁克斯法则"></a>帕金森定律和布鲁克斯法则</h1><p><strong>帕金森定理 ：</strong>只要还有时间，工作就会不断扩展，直到用完所有的时间</p><p><strong>布鲁克斯法则 ：</strong>向进度落后的项目中增加人手，只会使项目更加落后</p><p>基于上面的两个法则，留给大家的一个思考题，在实际的工作中，如果基础运维出现较大问题，作为团队的负责人，你应该怎么办？笔者列举了几种不太认可的方法：</p><ul><li>将一二线运维分离，确保基础运维工作不会扩散到二线运维团队中，一线运维确保其人员规模在一定范围，及时补充，然后看一线运维中是否有人能够脱颖而出</li><li>暂停各类项目，全员投入到基础运维中</li><li>给基础运维团队足够的HC，快速招人来解决问题</li></ul><h1 id="一万小时定律"><a href="#一万小时定律" class="headerlink" title="一万小时定律"></a>一万小时定律</h1><p><strong>一万小时定律</strong>，要成为某个领域的专家，需要10000小时。按比例计算就是：如果每天工作八个小时，一周工作五天，那么成为一个领域的专家需要五年。不管你信不信，反正有人是相信了，日复一日的重复着枯燥乏味的工作，幻想着自己只要坚持一万小时，就能够成为这个领域的专家。</p><h1 id="根因分析法"><a href="#根因分析法" class="headerlink" title="根因分析法"></a>根因分析法</h1><p><strong>根因分析法</strong>，也称为5why分析法，在故障复盘中，对于我个人始终强调的“一次性的投入，持续彻底的解决一类问题”是非常有用的，通过不断的提问，寻找出真正的根因，才有彻底解决问题的可能。</p><p>所谓5why分析法，又称“5问法”，也就是对一个问题点连续以5个“为什么”来自问，以追究其根本原因。虽为5个为什么，但使用时不限定只做“5次为什么的探讨”，主要是必须找到根本原因为止，有时可能只要3次，有时也许要10次，如古话所言：打破砂锅问到底。5why法的关键所在：鼓励解决问题的人要努力避开主观或自负的假设和逻辑陷阱，从结果着手，沿着因果关系链条，顺藤摸瓜，直至找出原有问题的根本原因。</p>]]></content>
    
    
    
    <tags>
      
      <tag>运维定律</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Linux find命令教程：15个find命令用法 </title>
    <link href="/2019/12/20/2019-12-20-linux-find/"/>
    <url>/2019/12/20/2019-12-20-linux-find/</url>
    
    <content type="html"><![CDATA[<h1 id="查找目录"><a href="#查找目录" class="headerlink" title="查找目录"></a>查找目录</h1><p>您可以使用-type d选项告诉find命令专门查找目录。这将使find命令仅搜索匹配的目录名，而不搜索文件名。</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> d -name <span class="hljs-string">"name-of-dir"</span></code></pre><p>#查找隐藏文件</p><p>由于Linux中的隐藏文件和目录以句点开头，因此我们可以在搜索字符串中指定此搜索模式，以便递归列出隐藏的文件和目录。</p><pre><code class="hljs bash">$ find /path/to/search -name <span class="hljs-string">".*"</span></code></pre><p>#查找特定大小或大于X的文件</p><p>find的-size选项允许我们搜索特定大小的文件。它可用于查找确切大小的文件，大于或小于特定大小的文件或适合指定大小范围的文件。以下有些例子：<br>搜索大于10MB的文件：</p><pre><code class="hljs bash">$ find /path/to/search -size +10M</code></pre><p>搜索小于10MB的文件：</p><pre><code class="hljs bash">$ find /path/to/search -size -10M</code></pre><p>搜索大小恰好为10MB的文件：</p><pre><code class="hljs bash">$ find /path/to/search -size 10M</code></pre><p>搜索大小在100MB到1GB之间的文件：</p><pre><code class="hljs bash">$ find /path/to/search -size +100M -size -1G</code></pre><h1 id="从文件列表中查找"><a href="#从文件列表中查找" class="headerlink" title="从文件列表中查找"></a>从文件列表中查找</h1><p>如果您有需要搜索的文件列表（例如，在.txt文件中），则可以使用find和grep命令的组合来搜索文件列表。为了使此命令起作用，只需确保要搜索的每个模式之间都用换行符隔开。</p><pre><code class="hljs bash">$ find /path/to/search | grep -f filelist.txt</code></pre><p>grep的-f选项表示“file”，并允许我们指定要匹配的字符串文件。这导致find命令返回与列表中的文件或目录名称匹配的任何文件或目录名称。</p><h1 id="不在列表中查找"><a href="#不在列表中查找" class="headerlink" title="不在列表中查找"></a>不在列表中查找</h1><p>使用上一个示例中提到的相同文件列表，您还可以使用find来搜索与文本文件内的模式不符的任何文件。再一次，我们将结合使用find和grep命令；我们只需要用grep指定一个附加选项：</p><pre><code class="hljs bash">$ find /path/to/search | grep -f filelist.txt</code></pre><p>grep的-v选项表示“逆向匹配”，并且将返回与文件列表中指定的任何模式都不匹配的文件列表。</p><h1 id="设置maxdepth"><a href="#设置maxdepth" class="headerlink" title="设置maxdepth"></a>设置maxdepth</h1><p>find命令默认将进行递归搜索。这意味着它将在指定的目录中搜索您指定的模式，以及您告诉它要搜索的目录中的所有子目录。<br>例如，如果告诉find搜索Linux（/）的根目录，则无论存在多少个子目录，它都会搜索整个硬盘。您可以使用-maxdepth选项来规避此行为。<br>在-maxdepth之后指定一个数字，以指示查找应递归搜索的子目录数。<br>仅搜索当前目录中的文件，而不递归搜索：</p><pre><code class="hljs bash">$ find . -maxdepth 0 -name <span class="hljs-string">"myfile.txt"</span></code></pre><p>仅在当前目录和更深的一个子目录中搜索文件：</p><pre><code class="hljs bash">$ find . -maxdepth 1 -name <span class="hljs-string">"myfile.txt"</span></code></pre><h1 id="查找空文件（零长度）"><a href="#查找空文件（零长度）" class="headerlink" title="查找空文件（零长度）"></a>查找空文件（零长度）</h1><p>要使用find搜索空文件，可以使用-empty标志。搜索所有空文件：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -empty</code></pre><p>搜索所有空目录：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> d -empty</code></pre><p>如果希望自动删除find返回的空文件或目录，那么将此命令与-delete选项结合使用也非常方便。<br>删除目录（和子目录）中的所有空文件：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -empty -delete</code></pre><h1 id="查找最大的目录或文件"><a href="#查找最大的目录或文件" class="headerlink" title="查找最大的目录或文件"></a>查找最大的目录或文件</h1><p>如果您想快速确定系统上哪些文件或目录占用了最多的空间，则可以使用find进行递归搜索，并按文件和目录的大小输出排序的列表。<br>如何显示目录中最大的文件：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -<span class="hljs-built_in">printf</span> <span class="hljs-string">"%s\t%p\n"</span> | sort -n | tail -1</code></pre><p>请注意，find命令已被排序到另外两个方便的Linux实用程序：sort和tail。 Sort将按文件的大小顺序排列文件列表，而tail将仅输出列表中的最后一个文件，该文件也是最大的。<br>如果您要输出例如最大的前5个文件，则可以调整tail命令。</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -<span class="hljs-built_in">printf</span> <span class="hljs-string">"%s\t%p\n"</span> | sort -n | tail -5</code></pre><p>或者，您可以使用head命令来确定最小的文件：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -<span class="hljs-built_in">printf</span> <span class="hljs-string">"%s\t%p\n"</span> | sort -n | head -5</code></pre><p>如果要搜索目录而不是文件，只需在类型选项中指定“ d”即可。如何显示最大目录：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> d -<span class="hljs-built_in">printf</span> <span class="hljs-string">"%s\t%p\n"</span> | sort -n | tail -1</code></pre><h1 id="查找setuid设置文件"><a href="#查找setuid设置文件" class="headerlink" title="查找setuid设置文件"></a>查找setuid设置文件</h1><p>Setuid是“set user ID on execution”的缩写，它是一种文件权限，允许普通用户运行具有升级特权（例如root）的程序。<br>出于明显的原因，这可能是一个安全问题，但是可以使用find命令和一些选项轻松隔离这些文件。<br>find命令有两个选项可帮助我们搜索具有特定权限的文件：-user和-perm。要查找普通用户能够以root特权执行的文件，可以使用以下命令：</p><pre><code class="hljs bash">$ find /path/to/search -user root -perm /4000</code></pre><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-20/setuid.png" srcset="/img/loading.gif" alt="avatar"></p><p>在上面的屏幕截图中，我们包含了-exec选项，以便显示有关查找返回文件的更多输出。整个命令如下所示：</p><pre><code class="hljs bash">$ find /path/to/search -user root -perm /4000 -<span class="hljs-built_in">exec</span> ls -l &#123;&#125; \;</code></pre><p>您也可以在此命令中用“ root”代替您要作为所有者搜索的任何其他用户。或者，您可以搜索具有SUID权限的所有文件，而根本不指定一个用户：</p><pre><code class="hljs bash">$ find /path/to/search -perm /4000</code></pre><h1 id="查找sgid设置文件"><a href="#查找sgid设置文件" class="headerlink" title="查找sgid设置文件"></a>查找sgid设置文件</h1><p>查找具有SGID设置的文件与查找具有SUID的文件几乎相同，只是需要将4000的权限更改为2000：</p><pre><code class="hljs bash">$ find /path/to/search -perm /2000</code></pre><p>您还可以通过在perms选项中指定6000来搜索，同时设置了SUID和SGID的文件：</p><pre><code class="hljs bash">$ find /path/to/search -perm /6000</code></pre><h1 id="列出文件未经允许被拒绝"><a href="#列出文件未经允许被拒绝" class="headerlink" title="列出文件未经允许被拒绝"></a>列出文件未经允许被拒绝</h1><p>使用find命令搜索文件时，您必须对要搜索的目录和子目录具有读取权限。如果您没有找到，find将输出一条错误消息，但会继续浏览您确实拥有权限的目录。<br>没有权限尽管这可能发生在许多不同的目录中，但在搜索根目录时肯定会发生。<br>这意味着，当您尝试在整个硬盘上搜索文件时，find命令将产生大量错误消息。<br>为避免看到这些错误，您可以将find的stderr输出重定向到stdout，并将其通过管道传递到grep。</p><pre><code class="hljs bash">$ find / -name <span class="hljs-string">"myfile.txt"</span> 2&gt;%1 | grep -v <span class="hljs-string">"Permission denied"</span></code></pre><p>此命令使用grep的-v（反向）选项来显示所有输出，除了显示“拒绝权限”之外的所有输出。</p><h1 id="在最近X天内查找修改过的文件"><a href="#在最近X天内查找修改过的文件" class="headerlink" title="在最近X天内查找修改过的文件"></a>在最近X天内查找修改过的文件</h1><p>使用find命令上的-mtime选项搜索最近X天内被修改的文件或目录。它也可以用于搜索X天之前的文件，或X天之前被完全修改过的的文件。<br>以下是一些如何在find命令上使用-mtime选项的示例：<br>搜索最近30天内修改过的所有文件：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -mtime -30</code></pre><p>搜索超过30天之前已修改的所有文件：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -mtime +30</code></pre><p>搜索30天前刚修改过的所有文件：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -mtime 30</code></pre><p>如果希望find命令输出有关找到的文件的更多信息，例如修改日期，则可以使用-exec选项并包含ls命令：</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">type</span> f -mtime -30 -<span class="hljs-built_in">exec</span> ls -l &#123;&#125; \;</code></pre><h1 id="按时间排序"><a href="#按时间排序" class="headerlink" title="按时间排序"></a>按时间排序</h1><p>要按文件的修改时间对查找结果进行排序，您可以使用-printf选项以可排序的方式列出时间，然后将其输出到sort实用程序。</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">printf</span> <span class="hljs-string">"%T+\t%p\n"</span> | sort</code></pre><p>此命令将对旧的文件进行排序。如果您希望较新的文件首先显示，只需传递-r（反向）选项即可进行排序。</p><pre><code class="hljs bash">$ find /path/to/search -<span class="hljs-built_in">printf</span> <span class="hljs-string">"%T+\t%p\n"</span> | sort -r</code></pre><h1 id="定位和查找之间的区别"><a href="#定位和查找之间的区别" class="headerlink" title="定位和查找之间的区别"></a>定位和查找之间的区别</h1><p>Linux上的locate命令是搜索系统上文件的另一种好方法。它没有像find命令那样包含过多的搜索选项，因此它的灵活性较差，但仍然很方便。</p><pre><code class="hljs bash">$ locate myfile.txt</code></pre><p>locate命令通过搜索包含系统上所有文件名的数据库来工作。搜索到的数据库已使用upatedb命令进行更新。<br>由于locate命令不必实时搜索系统上的所有文件，因此它比find命令效率更高。但是，除了缺少选项之外，还有另一个缺点：文件数据库每天仅更新一次。<br>您可以通过运行updatedb命令手动更新此文件数据库：</p><pre><code class="hljs bash">$ updatedb</code></pre><p>当您需要在整个硬盘驱动器中搜索文件时，locate命令特别有用，因为find命令自然需要更长的时间，因为它必须实时遍历每个目录。<br>如果搜索一个特定目录（已知其中不包含大量子目录），则最好坚持使用find命令。</p><h1 id="find命令的CPU负载"><a href="#find命令的CPU负载" class="headerlink" title="find命令的CPU负载"></a>find命令的CPU负载</h1><p>在搜索大量目录时，find命令可能会占用大量资源。它本来应该允许更重要的系统进程具有优先级，但是如果需要确保find命令占用生产服务器上的较少资源，则可以使用ionice或nice命令。<br>监视find命令的CPU使用情况：</p><pre><code class="hljs bash">$ top</code></pre><p>降低find命令的输入/输出优先级：</p><pre><code class="hljs bash">$ ionice -c3 -n7 find /path/to/search -name <span class="hljs-string">"myfile.txt"</span></code></pre><p>降低find命令的CPU优先级：</p><pre><code class="hljs bash">$ nice -n 19 find /path/to/search -name <span class="hljs-string">"myfile.txt"</span></code></pre><p>或结合使用这两个实用程序以真正确保低I / O和低CPU优先级：</p><pre><code class="hljs bash">$ nice -n ionice -c2 -n7 find /path/to/search -name <span class="hljs-string">"myfile.txt"</span></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Nginx的这些妙用，你肯定有不知道的 </title>
    <link href="/2019/12/19/2019-12-19-nginx-usage/"/>
    <url>/2019/12/19/2019-12-19-nginx-usage/</url>
    
    <content type="html"><![CDATA[<h1 id="Nginx-简介"><a href="#Nginx-简介" class="headerlink" title="Nginx 简介"></a>Nginx 简介</h1><p>Nginx 是一个免费、开源、高性能、轻量级的 HTTP 和反向代理服务器，也是一个电子邮件（IMAP/POP3）代理服务器，其特点是占有内存少，并发能力强。</p><p>Nginx 由内核和一系列模块组成，内核提供 Web 服务的基本功能，如启用网络协议，创建运行环境，接收和分配客户端请求，处理模块之间的交互。</p><p>Nginx 的各种功能和操作都由模块来实现。Nginx 的模块从结构上分为：</p><ul><li><strong>核心模块：</strong>HTTP 模块、EVENT 模块和 MAIL 模块。</li><li><strong>基础模块：</strong>HTTP Access 模块、HTTP FastCGI 模块、HTTP Proxy 模块和 HTTP Rewrite 模块。</li><li><strong>第三方模块：</strong>HTTP Upstream Request Hash 模块、Notice 模块和 HTTP Access Key 模块及用户自己开发的模块。</li></ul><p>这样的设计使 Nginx 方便开发和扩展，也正因此才使得 Nginx 功能如此强大。</p><p>Nginx 的模块默认编译进 Nginx 中，如果需要增加或删除模块，需要重新编译 Nginx，这一点不如 Apache 的动态加载模块方便。</p><p>如果有需要动态加载模块，可以使用由淘宝网发起的 Web 服务器 Tengine，在 Nginx 的基础上增加了很多高级特性，完全兼容 Nginx，已被国内很多网站采用。</p><p>Nginx 有很多扩展版本：</p><ul><li><p><strong>开源版 nginx.org</strong></p></li><li><p><strong>商业版 NGINX Plus</strong></p></li><li><p><strong>淘宝网发起的 Web 服务器 Tengine</strong></p></li><li><p><strong>基于 Nginx 和 Lua 的 Web 平台 OpenResty</strong></p></li></ul><h1 id="Nginx-作为-Web-服务器"><a href="#Nginx-作为-Web-服务器" class="headerlink" title="Nginx 作为 Web 服务器"></a>Nginx 作为 Web 服务器</h1><p>Web 服务器也称为 WWW（World Wide Web）服务器，主要功能是提供网上信息浏览服务，常常以 B/S（Browser/Server）方式提供服务：</p><ul><li><strong>应用层使用 HTTP 协议。</strong></li><li><strong>HTML 文档格式。</strong></li><li><strong>浏览器统一资源定位器(URL)。</strong></li></ul><p>Nginx 可以作为静态页面的 Web 服务器，同时还支持 CGI 协议的动态语言，比如 Perl、PHP 等，但是不支持 Java。</p><p>Java 程序一般都通过与 Tomcat 配合完成。作为一名 Java 程序员，肯定要理解下 Nginx 和 Tomcat 的区别了。</p><p>Nginx、Apache 和 Tomcat：</p><ul><li><p><strong>Nginx：</strong>由俄罗斯程序员 Igor Sysoev 所开发的轻量级、高并发 HTTP 服务器。</p></li><li><p><strong>Apache HTTP Server Project：</strong>一个 Apache 基金会下的 HTTP 服务项目，和 Nginx 功能类似。</p></li><li><p><strong>Apache Tomcat：</strong>是 Apache 基金会下的另外一个项目，是一个 Application Server。</p><p>更准确的说是一个 Servlet 应用容器，与 Apache HTTP Server 和 Nginx 相比，Tomcat 能够动态的生成资源并返回到客户端。</p></li></ul><p>Apache HTTP Server 和 Nginx 本身不支持生成动态页面，但它们可以通过其他模块来支持（例如通过 Shell、PHP、Python 脚本程序来动态生成内容）。</p><p>一个 HTTP Server 关心的是 HTTP 协议层面的传输和访问控制，所以在 Apache/Nginx 上你可以看到代理、负载均衡等功能。</p><p>客户端通过 HTTP Server 访问服务器上存储的资源（HTML 文件、图片文件等等）。</p><p>通过 CGI 技术，也可以将处理过的内容通过 HTTP Server 分发，但是一个 HTTP Server 始终只是把服务器上的文件如实的通过 HTTP 协议传输给客户端。</p><p>而应用服务器，则是一个应用执行的容器。它首先需要支持开发语言的运行（对于 Tomcat 来说，就是 Java），保证应用能够在应用服务器上正常运行。</p><p>其次，需要支持应用相关的规范，例如类库、安全方面的特性。对于 Tomcat 来说，就是需要提供 JSP/Sevlet 运行需要的标准类库、Interface 等。</p><p>为了方便，应用服务器往往也会集成 HTTP Server 的功能，但是不如专业的 HTTP Server 那么强大。</p><p>所以应用服务器往往是运行在 HTTP Server 的背后，执行应用，将动态的内容转化为静态的内容之后，通过 HTTP Server 分发到客户端。</p><h2 id="正向代理"><a href="#正向代理" class="headerlink" title="正向代理"></a>正向代理</h2><p><strong>正向代理：</strong>如果把局域网外的 Internet 想象成一个巨大的资源库，则局域网中的客户端要访问 Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理。</p><p>正向代理“代理”的是客户端。比如你想去 Google 看个“动作片”，可国内不允许呀，就需要找翻墙代理，这个就是所谓的“正向代理”。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/1.png" srcset="/img/loading.gif" alt="avatar"></p><h2 id="反向代理与负载均衡"><a href="#反向代理与负载均衡" class="headerlink" title="反向代理与负载均衡"></a>反向代理与负载均衡</h2><p>反向代理正好与正向代理相反，反向代理是指以代理服务器来接收 Internet 上的连接请求，然后将请求转发到内部网络上的服务器，并将服务器上得到的结果返回给客户端。</p><p>此时代理服务器对外表现就是一个服务器，客户端对代理是无感知的。反向代理“代理”的是服务端。</p><p>再比如，你想本本分分的在“优酷”上看个“爱情片”，youku.com 会把你的请求分发到存放片片的那台机器上，这个就是所谓的“反向代理”。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/2.png" srcset="/img/loading.gif" alt="avatar"></p><p>为什么使用反向代理，原因如下：</p><ul><li><strong>保护和隐藏原始资源服务器</strong></li><li><strong>加密和 SSL 加速</strong></li><li><strong>通过缓存静态资源，加速 Web 请求</strong></li><li><strong>实现负载均衡</strong></li></ul><p><strong>负载均衡：</strong>TODO: 留一个负载均衡详细介绍传送门。</p><p><strong>地址重定向：</strong>Nginx 的 Rewrite 主要的功能就是实现 URL 重写，比如输入 360.com  跳转到了 360.cn，baidu.cn 跳转到了 baidu.com。</p><h2 id="动静分离"><a href="#动静分离" class="headerlink" title="动静分离"></a>动静分离</h2><p>为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速度，降低原来单个服务器的压力。</p><p>这里指的就是让动态程序（Java、PHP）去访问应用服务器，让缓存、图片、JS、CSS 等去访问 Nginx。</p><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>nginx.conf 配置文件主要分为三部分：</p><ul><li><strong>全局块</strong></li><li><strong>Events 块</strong></li><li><strong>HTTPS 块</strong></li></ul><p>Nginx 配置语法：</p><ul><li>配置文件由指令和指令块构成</li><li>每条指令以分号（;）结尾，指令和参数间以空格符分隔</li><li>指令块以大括号{}将多条指令组织在一起</li><li>include 语句允许组合多个配置文件以提高可维护性</li><li>使用 # 添加注释</li><li>使用 $ 定义变量</li><li>部分指令的参数支持正则表达式</li></ul><h3 id="全局块"><a href="#全局块" class="headerlink" title="全局块"></a>全局块</h3><p>全局配置部分用来配置对整个 Server 都有效的参数。主要会设置一些影响 Nginx 服务器整体运行的配置指令，包括配置运行 Nginx 服务器的用户（组）、允许生成的 Worker Process 数，进程 PID 存放路径、日志存放路径和类型以及配置文件的引入等。</p><p>示例如下：</p><pre><code class="hljs bash">user nobody;worker_processes  4;error_log  /data/nginx/logs/error.log  notice;</code></pre><h3 id="Events-块"><a href="#Events-块" class="headerlink" title="Events 块"></a>Events 块</h3><p>Events 块涉及的指令主要影响 Nginx 服务器与用户的网络连接，常用的设置包括是否开启对多 Work Process 下的网络连接进行序列化，是否允许同时接收多个网络连接，选取哪种事件驱动模型来处理连接请求，每个 Word Process 可以同时支持的最大连接数等。</p><pre><code class="hljs bash">events &#123;    <span class="hljs-comment">#每个 work process 支持的最大连接数为 1024.</span>    worker_connections  1024;&#125;</code></pre><h3 id="HTTP-块"><a href="#HTTP-块" class="headerlink" title="HTTP 块"></a>HTTP 块</h3><p>这算是 Nginx 服务器配置中最频繁的部分，代理、缓存和日志定义等绝大多数功能和第三方模块的配置都在这里。 需要注意的是：HTTP 块也可以包括 HTTP 全局块、Server 块。</p><p><strong>①HTTP 全局块</strong></p><p>HTTP 全局块配置的指令包括文件引入、MIME-TYPE 定义、日志自定义、连接超时时间、单链接请求数上限等。</p><pre><code class="hljs bash">http &#123;    include       mime.types;    default_type  application/octet-stream;    sendfile        on;    keepalive_timeout  65;</code></pre><p><strong>②Server 块</strong></p><p>这块和虚拟主机有密切关系，虚拟主机从用户角度看，和一台独立的硬件主机是完全一样的，该技术的产生是为了节省互联网服务器硬件成本。</p><p>每个 HTTP 块可以包括多个 Server 块，而每个 Server 块就相当于一个虚拟主机。</p><p>而每个 Server 块也分为全局 Server 块，以及可以同时包含多个 Locaton 块。</p><p><strong>全局 Server 块：</strong>也被叫做“虚拟服务器”部分，它描述的是一组根据不同server_name指令逻辑分割的资源，这些虚拟服务器响应 HTTP 请求，因此都包含在 HTTP 部分。</p><p>最常见的配置是本虚拟机主机的监听配置和本虚拟主机的名称或 IP 配置。</p><pre><code class="hljs bash">server &#123;  listen       80;  <span class="hljs-comment">#server_name也支持通配符，*.example.com、www.example.*、.example.com</span>  server_name  localhost;  <span class="hljs-comment">#charset koi8-r;</span>  <span class="hljs-comment">#access_log  logs/host.access.log  main;</span></code></pre><p><strong>Location 块：</strong>一个 Server 块可以配置多个 Location 块。</p><p>这块的主要作用是基于 Nginx 服务器接收到的请求字符串（例如 server_name/uri-string），对虚拟主机名称 （也可以是 IP 别名）之外的字符串（例如前面的 /uri-string）进行匹配，对特定的请求进行处理。</p><p>地址定向、数据缓存和应答控制等功能，还有许多第三方模块的配置也在这里进行。</p><p><strong>Location 指令说明：</strong>该指令用于匹配 URL。</p><p>语法如下：</p><pre><code class="hljs nginx"><span class="hljs-attribute">location</span> [ = | <span class="hljs-regexp">~ |</span> <span class="hljs-regexp">~* |</span><span class="hljs-regexp"> ^~]</span> uri&#123;&#125;</code></pre><ul><li><strong>= ：</strong>该修饰符使用精确匹配并且终止搜索。</li><li><strong>~：</strong>该修饰符使用区分大小写的正则表达式匹配。</li><li><strong>~*：</strong>该修饰符使用不区分大小写的正则表达式匹配。</li><li><strong>^~：</strong>用于不含正则表达式的 URI 前，要求 Nginx 服务器找到标识 URI 和请求字符串匹配度最高的 Location 后，立即使用此 Location 处理请求，而不再使用 Location 块中的正则 URI 和请求字符串做匹配。</li></ul><p>?&gt;Tip 注意：如果 URI 包含正则表达式，则必须要有 ~ 或者 ~* 标识。</p><p>当一个请求进入时，URI 将会被检测匹配一个最佳的 Location：</p><ul><li>没有正则表达式的 Location 被作为最佳的匹配，独立于含有正则表达式的 Location 顺序。</li><li>在配置文件中按照查找顺序进行正则表达式匹配。在查找到第一个正则表达式匹配之后结束查找。由这个最佳的 Location 提供请求处理。</li></ul><pre><code class="hljs bash">location / &#123;    root   html;   index  index.html index.htm;   &#125; <span class="hljs-comment">#error_page  404              /404.html;</span> <span class="hljs-comment"># redirect server error pages to the static page /50x.html</span> <span class="hljs-comment">#</span> error_page   500 502 503 504  /50x.html; location = /50x.html &#123;     root   html; &#125; location / &#123;     <span class="hljs-comment">#try_files指令将会按照给定的参数顺序进行匹配尝试</span>     try_files <span class="hljs-variable">$uri</span> <span class="hljs-variable">$uri</span>/ /index.html; &#125;</code></pre><p>nginx.conf 详细配置如下：</p><pre><code class="hljs bash"><span class="hljs-comment">#定义Nginx运行的用户和用户组</span>user www www; <span class="hljs-comment">#nginx进程数，通常设置成和cpu的数量相等</span>worker_processes 4; <span class="hljs-comment">#全局错误日志定义类型，[debug | info | notice | warn | error | crit]</span><span class="hljs-comment">#error_log  /data/nginx/logs/error.log;</span><span class="hljs-comment">#error_log  /data/nginx/logs/error.log  notice;</span><span class="hljs-comment">#日志文件存放路径 access_log path [format [buffer=size | off]]</span>access_log /data/nginx/logs/lazyegg.com/web/access.log combinedio;<span class="hljs-comment">#进程pid文件</span><span class="hljs-comment">#pid        logs/nginx.pid;</span><span class="hljs-comment">#指定进程可以打开的最大描述符：数目</span><span class="hljs-comment">#工作模式与连接数上限</span><span class="hljs-comment">##这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。</span><span class="hljs-comment">#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。</span>worker_rlimit_nofile 65535;<span class="hljs-comment">#################################  events  ###############################</span>events &#123;    <span class="hljs-comment">#参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型</span>    use epoll    <span class="hljs-comment">#单个进程最大连接数（最大连接数=连接数+进程数）</span>    worker_connections  1024;    <span class="hljs-comment">#keepalive 超时时间</span>    keepalive_timeout 60;    <span class="hljs-comment">#客户端请求头部的缓冲区大小。</span>    client_header_buffer_size 4k;    <span class="hljs-comment">#这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。</span>    open_file_cache max=65535 inactive=60s;    <span class="hljs-comment">#这个是指多长时间检查一次缓存的有效信息。</span>    open_file_cache_valid 80s;        <span class="hljs-comment">#open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。</span>    open_file_cache_min_uses 1;    <span class="hljs-comment">#语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误.</span>    open_file_cache_errors on;&#125;<span class="hljs-comment">##############################   http    ##################################</span><span class="hljs-comment">#设定http服务器，利用它的反向代理功能提供负载均衡支持</span>http&#123;    <span class="hljs-comment">#文件扩展名与文件类型映射表</span>    include mime.types;    <span class="hljs-comment">#默认文件类型</span>    default_type application/octet-stream;    <span class="hljs-comment">#默认编码</span>    charset utf-8;    <span class="hljs-comment">#服务器名字的hash表大小</span>    server_names_hash_bucket_size 128;    <span class="hljs-comment">#客户端请求头部的缓冲区大小。</span>    client_header_buffer_size 32k;    <span class="hljs-comment">#客户请求头缓冲大小。</span>    large_client_header_buffers 4 64k;    <span class="hljs-comment">#允许客户端请求的最大单个文件字节数</span>    client_max_body_size 8m;    <span class="hljs-comment">#开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。</span>    sendfile on;    <span class="hljs-comment">#开启目录列表访问，适合下载服务器，默认关闭。</span>    autoindex on;    <span class="hljs-comment">#此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用</span>    tcp_nopush on;    tcp_nodelay on;    <span class="hljs-comment">#长连接超时时间，单位是秒</span>    keepalive_timeout 120;    <span class="hljs-comment">#FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。</span>    fastcgi_connect_timeout 300;    fastcgi_send_timeout 300;    fastcgi_read_timeout 300;    fastcgi_buffer_size 64k;    fastcgi_buffers 4 64k;    fastcgi_busy_buffers_size 128k;    fastcgi_temp_file_write_size 128k;    <span class="hljs-comment">#gzip模块设置</span>    gzip on; <span class="hljs-comment">#开启gzip压缩输出</span>    gzip_min_length 1k;    <span class="hljs-comment">#最小压缩文件大小</span>    gzip_buffers 4 16k;    <span class="hljs-comment">#压缩缓冲区</span>    gzip_http_version 1.0; <span class="hljs-comment">#压缩版本（默认1.1，前端如果是squid2.5请使用1.0）</span>    gzip_comp_level 2;     <span class="hljs-comment">#压缩等级</span>    gzip_types text/plain application/x-javascript text/css application/xml;    <span class="hljs-comment">#压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。</span>    gzip_vary on;    <span class="hljs-comment">#开启限制IP连接数的时候需要使用</span>    <span class="hljs-comment">#limit_zone crawler $binary_remote_addr 10m;</span>        <span class="hljs-comment">#负载均衡配置</span>    upstream lazyegg.net &#123;        <span class="hljs-comment">#upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。</span>        server 192.168.80.121:80 weight=3;        server 192.168.80.122:80 weight=2;        server 192.168.80.123:80 weight=3;        <span class="hljs-comment">#nginx的upstream目前支持4种方式的分配</span>        <span class="hljs-comment">#1、轮询（默认）</span>        <span class="hljs-comment">#每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。</span>        <span class="hljs-comment">#2、weight</span>        <span class="hljs-comment">#指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。</span>        <span class="hljs-comment">#例如：</span>        <span class="hljs-comment">#upstream bakend &#123;</span>        <span class="hljs-comment">#    server 192.168.0.14 weight=10;</span>        <span class="hljs-comment">#    server 192.168.0.15 weight=10;</span>        <span class="hljs-comment">#&#125;</span>        <span class="hljs-comment">#2、ip_hash</span>        <span class="hljs-comment">#每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。</span>        <span class="hljs-comment">#例如：</span>        <span class="hljs-comment">#upstream bakend &#123;</span>        <span class="hljs-comment">#    ip_hash;</span>        <span class="hljs-comment">#    server 192.168.0.14:88;</span>        <span class="hljs-comment">#    server 192.168.0.15:80;</span>        <span class="hljs-comment">#&#125;</span>        <span class="hljs-comment">#3、fair（第三方）</span>        <span class="hljs-comment">#按后端服务器的响应时间来分配请求，响应时间短的优先分配。</span>        <span class="hljs-comment">#upstream backend &#123;</span>        <span class="hljs-comment">#    server server1;</span>        <span class="hljs-comment">#    server server2;</span>        <span class="hljs-comment">#    fair;</span>        <span class="hljs-comment">#&#125;</span>        <span class="hljs-comment">#4、url_hash（第三方）</span>        <span class="hljs-comment">#按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。</span>        <span class="hljs-comment">#例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法</span>        <span class="hljs-comment">#upstream backend &#123;</span>        <span class="hljs-comment">#    server squid1:3128;</span>        <span class="hljs-comment">#    server squid2:3128;</span>        <span class="hljs-comment">#    hash $request_uri;</span>        <span class="hljs-comment">#    hash_method crc32;</span>        <span class="hljs-comment">#&#125;</span>        <span class="hljs-comment">#tips:</span>        <span class="hljs-comment">#upstream bakend&#123;#定义负载均衡设备的Ip及设备状态&#125;&#123;</span>        <span class="hljs-comment">#    ip_hash;</span>        <span class="hljs-comment">#    server 127.0.0.1:9090 down;</span>        <span class="hljs-comment">#    server 127.0.0.1:8080 weight=2;</span>        <span class="hljs-comment">#    server 127.0.0.1:6060;</span>        <span class="hljs-comment">#    server 127.0.0.1:7070 backup;</span>        <span class="hljs-comment">#&#125;</span>        <span class="hljs-comment">#在需要使用负载均衡的server中增加 proxy_pass http://bakend/;</span>        <span class="hljs-comment">#每个设备的状态设置为:</span>        <span class="hljs-comment">#1.down表示单前的server暂时不参与负载</span>        <span class="hljs-comment">#2.weight为weight越大，负载的权重就越大。</span>        <span class="hljs-comment">#3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误</span>        <span class="hljs-comment">#4.fail_timeout:max_fails次失败后，暂停的时间。</span>        <span class="hljs-comment">#5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。</span>        <span class="hljs-comment">#nginx支持同时设置多组的负载均衡，用来给不用的server来使用。</span>        <span class="hljs-comment">#client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug</span>        <span class="hljs-comment">#client_body_temp_path设置记录文件的目录 可以设置最多3层目录</span>        <span class="hljs-comment">#location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡</span>    &#125;       <span class="hljs-comment">#虚拟主机的配置</span>    server &#123;        <span class="hljs-comment">#监听端口</span>        listen 80;        <span class="hljs-comment">#域名可以有多个，用空格隔开</span>        server_name lazyegg.net;        <span class="hljs-comment">#默认入口文件名称</span>        index index.html index.htm index.php;        root /data/www/lazyegg;        <span class="hljs-comment">#对******进行负载均衡</span>        location ~ .*.(php|php5)?$        &#123;            fastcgi_pass 127.0.0.1:9000;            fastcgi_index index.php;            include fastcgi.conf;        &#125;        <span class="hljs-comment">#图片缓存时间设置</span>        location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$        &#123;            expires 10d;        &#125;        <span class="hljs-comment">#JS和CSS缓存时间设置</span>        location ~ .*.(js|css)?$        &#123;            expires 1h;        &#125;        <span class="hljs-comment">#日志格式设定</span>        <span class="hljs-comment">#$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址；</span>        <span class="hljs-comment">#$remote_user：用来记录客户端用户名称；</span>        <span class="hljs-comment">#$time_local： 用来记录访问时间与时区；</span>        <span class="hljs-comment">#$request： 用来记录请求的url与http协议；</span>        <span class="hljs-comment">#$status： 用来记录请求状态；成功是200，</span>        <span class="hljs-comment">#$body_bytes_sent ：记录发送给客户端文件主体内容大小；</span>        <span class="hljs-comment">#$http_referer：用来记录从那个页面链接访问过来的；</span>        <span class="hljs-comment">#$http_user_agent：记录客户浏览器的相关信息；</span>        <span class="hljs-comment">#通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。</span>        log_format access <span class="hljs-string">'$remote_addr - $remote_user [$time_local] "$request" '</span>        <span class="hljs-string">'$status $body_bytes_sent "$http_referer" '</span>        <span class="hljs-string">'"$http_user_agent" $http_x_forwarded_for'</span>;        <span class="hljs-comment">#定义本虚拟主机的访问日志</span>        access_log  /usr/<span class="hljs-built_in">local</span>/nginx/logs/host.access.log  main;        access_log  /usr/<span class="hljs-built_in">local</span>/nginx/logs/host.access.404.log  log404;        <span class="hljs-comment">#对 "/connect-controller" 启用反向代理</span>        location /connect-controller &#123;            proxy_pass http://127.0.0.1:88; <span class="hljs-comment">#请注意此处端口号不能与虚拟主机监听的端口号一样（也就是server监听的端口）</span>            proxy_redirect off;            proxy_set_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;            <span class="hljs-comment">#后端的Web服务器可以通过X-Forwarded-For获取用户真实IP</span>            proxy_set_header X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;            <span class="hljs-comment">#以下是一些反向代理的配置，可选。</span>            proxy_set_header Host <span class="hljs-variable">$host</span>;            <span class="hljs-comment">#允许客户端请求的最大单文件字节数</span>            client_max_body_size 10m;            <span class="hljs-comment">#缓冲区代理缓冲用户端请求的最大字节数，</span>            <span class="hljs-comment">#如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。</span>            <span class="hljs-comment">#无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误</span>            client_body_buffer_size 128k;            <span class="hljs-comment">#表示使nginx阻止HTTP应答代码为400或者更高的应答。</span>            proxy_intercept_errors on;            <span class="hljs-comment">#后端服务器连接的超时时间_发起握手等候响应超时时间</span>            <span class="hljs-comment">#nginx跟后端服务器连接超时时间(代理连接超时)</span>            proxy_connect_timeout 90;            <span class="hljs-comment">#后端服务器数据回传时间(代理发送超时)</span>            <span class="hljs-comment">#后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据</span>            proxy_send_timeout 90;            <span class="hljs-comment">#连接成功后，后端服务器响应时间(代理接收超时)</span>            <span class="hljs-comment">#连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间）</span>            proxy_read_timeout 90;            <span class="hljs-comment">#设置代理服务器（nginx）保存用户头信息的缓冲区大小</span>            <span class="hljs-comment">#设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小</span>            proxy_buffer_size 4k;            <span class="hljs-comment">#proxy_buffers缓冲区，网页平均在32k以下的设置</span>            <span class="hljs-comment">#设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k</span>            proxy_buffers 4 32k;            <span class="hljs-comment">#高负荷下缓冲大小（proxy_buffers*2）</span>            proxy_busy_buffers_size 64k;            <span class="hljs-comment">#设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长</span>            <span class="hljs-comment">#设定缓存文件夹大小，大于这个值，将从upstream服务器传</span>            proxy_temp_file_write_size 64k;        &#125;        <span class="hljs-comment">#本地动静分离反向代理配置</span>        <span class="hljs-comment">#所有jsp的页面均交由tomcat或resin处理</span>        location ~ .(jsp|jspx|<span class="hljs-keyword">do</span>)?$ &#123;            proxy_set_header Host <span class="hljs-variable">$host</span>;            proxy_set_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;            proxy_set_header X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;            proxy_pass http://127.0.0.1:8080;        &#125;    &#125;&#125;</code></pre><h2 id="Nginx-配置：负载均衡"><a href="#Nginx-配置：负载均衡" class="headerlink" title="Nginx 配置：负载均衡"></a>Nginx 配置：负载均衡</h2><p>随着互联网信息的爆炸性增长，负载均衡（Load Balance）已经不再是一个很陌生的话题。</p><p>顾名思义，负载均衡即是将负载分摊到不同的服务单元，既保证服务的可用性，又保证响应足够快，给用户很好的体验。</p><p>快速增长的访问量和数据流量催生了各式各样的负载均衡产品，很多专业的负载均衡硬件提供了很好的功能，但却价格不菲。</p><p>这使得负载均衡软件大受欢迎，Nginx 就是其中的一个，在 Linux 下有 Nginx、LVS、Haproxy 等等服务可以提供负载均衡服务。</p><p>Nginx 的负载均衡是 Proxy 模块和 Upstream 模块搭配实现的。Upstream模块将会启用一个新的配置区段，在该区段定义了一组上游服务器。</p><p><strong>实现效果：配置负载均衡。</strong></p><p>①同时启动两个 Tomcat（为了方便验证效果，修改 Tomcat 端口号的同时，顺便将 Tomcat 默认欢迎页面 apache-tomcat-9.0.29/webapps/ROOR 目录下的 index.jsp 修改下，看下 8081 欢迎页的“蛋蛋”没）：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/3.png" srcset="/img/loading.gif" alt="avatar"></p><p>②修改 nginx.conf：</p><pre><code class="hljs bash">http &#123;    upstream myserver &#123;        server localhost:8080;        server localhost:8081;    &#125;    server &#123;        listen 80;        location / &#123;            proxy_pass http://myserver;        &#125;    &#125;&#125;</code></pre><p>③重启 Nginx，验证效果（默认轮询的方式，每次打开新窗口，8080 和 8081 会交替出现，同一个窗口的话需要关闭浏览器缓存)。</p><p>Nginx 分配策略：</p><ul><li>轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 Down 掉，能自动剔除。</li><li>Weight 代表权重，默认为 1，权重越高被分配的客户端越多，指定轮询几率，Weight 和访问比率成正比，用于后端服务器性能不均的情况。</li></ul><p>例如：</p><pre><code class="hljs bash">upstream server_pool&#123;    server 192.168.5.21 weight=10;    server 192.168.5.22 weight=10; &#125;</code></pre><p>ip_hash 每个请求按访问 IP 的 Hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 Session 的问题。 </p><p>例如：</p><pre><code class="hljs bash">upstream server_pool&#123;    ip_hash; server 192.168.5.21:80;     server 192.168.5.22:80; &#125;</code></pre><p>Fair（第三方） 按后端服务器的响应时间来分配请求，响应时间短的优先分配。</p><pre><code class="hljs bash">upstream server_pool&#123;     server 192.168.5.21:80;    server 192.168.5.22:80; fair; &#125;</code></pre><h2 id="Nginx-配置：动静分离"><a href="#Nginx-配置：动静分离" class="headerlink" title="Nginx 配置：动静分离"></a>Nginx 配置：动静分离</h2><p>Nginx 动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和静态页面物理分离。</p><p>严格意义上说应该是动态请求跟静态请求分开，可以理解成使用 Nginx 处理静态页面，Tomcat 处理动态页面。</p><p>动静分离从目前实现角度来讲大致分为两种：</p><ul><li>纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案； </li><li>动态跟静态文件混合在一起发布，通过 Nginx 来分开。 </li></ul><p>通过 Location 指定不同的后缀名实现不同的请求转发。通过 Expires 参数设置，可以使浏览器缓存过期时间，减少与服务器之前的请求和流量。</p><p><strong>具体 Expires 定义：</strong>是给一个资源设定一个过期时间，也就是说无需去服务端验证，直接通过浏览器自身确认是否过期即可， 所以不会产生额外的流量。</p><p>此种方法非常适合不经常变动的资源（如果经常更新的文件， 不建议使用 Expires 来缓存）。</p><p>我这里设置 3d，表示在这 3 天之内访问这个 URL，发送一个请求，比对服务器该文件最后更新时间没有变化，则不会从服务器抓取，返回状态码 304，如果有修改，则直接从服务器重新下载，返回状态码 200。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/4.png" srcset="/img/loading.gif" alt="avatar"></p><p>①服务器找个目录存放自己的静态文件：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/12.png" srcset="/img/loading.gif" alt="avatar"></p><p>②修改 nginx.conf：</p><pre><code class="hljs bash">server &#123;    listen       80;    server_name  localhost;    location /static/ &#123;        root   /usr/data/www;    &#125;    location /image/ &#123;         root /usr/data/;         autoindex on;    &#125;</code></pre><p>③./nginx -s reload，验证效果：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/5.png" srcset="/img/loading.gif" alt="avatar"></p><p>添加监听端口、访问名字重点是添加 Location，最后检查 Nginx 配置是否正确即可，然后测试动静分离是否成功，只需要删除后端 Tomcat 服务器上的某个静态文件，查看是否能访问，如果可以访问说明静态资源 Nginx 直接返回了，不走后端 Tomcat 服务器。</p><h2 id="Nginx-的-Rewrite"><a href="#Nginx-的-Rewrite" class="headerlink" title="Nginx 的 Rewrite"></a>Nginx 的 Rewrite</h2><p>Rewrite 是 Nginx 服务器提供的一个重要的功能，它可以实现 URL 重写和重定向功能。</p><p>场景如下：</p><ul><li>URL 访问跳转，支持开发设计。页面跳转、兼容性支持（新旧版本更迭）、展示效果（网址精简）等</li><li>SEO 优化（Nginx 伪静态的支持）</li><li>后台维护、流量转发等</li><li>安全（动态界面进行伪装）</li></ul><p>该指令是通过正则表达式的使用来改变 URI。可以同时存在一个或多个指令。需要按照顺序依次对 URL 进行匹配和处理。</p><p>该指令可以在 Server 块或 Location 块中配置，其基本语法结构如下：</p><pre><code class="hljs bash">rewrite regex replacement [flag];</code></pre><p>①采用反向代理 Demo2 中的例子，修改 nginx.conf（只多加了一行 Rewrite）：</p><pre><code class="hljs bash">server &#123;        listen       80;        server_name  localhost;        location /java/ &#123;            proxy_pass http://127.0.0.1:8080;            rewrite ^/java /egg/ redirect;        &#125;        location /egg/ &#123;            proxy_pass http://127.0.0.1:8081;        &#125;&#125;</code></pre><p>②./nginx -s reload，验证效果（输入 ip/java/ 被重定向到了 egg）：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/6.png" srcset="/img/loading.gif" alt="avatar"></p><p>Rewrite 指令可以在 Server 块或 Location 块中配置，其基本语法结构如下：</p><pre><code class="hljs abnf">rewrite regex replacement [flag]<span class="hljs-comment">;</span></code></pre><ul><li><strong>rewrite 的含义：</strong>该指令是实现 URL 重写的指令。</li><li><strong>regex 的含义：</strong>用于匹配 URI 的正则表达式。</li><li><strong>replacement：</strong>将 regex 正则匹配到的内容替换成 replacement。</li><li><strong>flag：</strong>flag 标记。</li></ul><p>flag 有如下值：</p><ul><li><strong>last：</strong>本条规则匹配完成后，继续向下匹配新的 Location URI 规则。(不常用)</li><li><strong>break：</strong>本条规则匹配完成即终止，不再匹配后面的任何规则(不常用)。</li><li><strong>redirect：</strong>返回 302 临时重定向，浏览器地址会显示跳转新的 URL 地址。</li><li><strong>permanent：</strong>返回 301 永久重定向。浏览器地址会显示跳转新的 URL 地址。</li></ul><pre><code class="hljs nginx"><span class="hljs-attribute">rewrite</span><span class="hljs-regexp"> ^/(.*)</span> http://www.360.cn/<span class="hljs-variable">$1</span> <span class="hljs-literal">permanent</span>;</code></pre><h2 id="Nginx-高可用"><a href="#Nginx-高可用" class="headerlink" title="Nginx 高可用"></a>Nginx 高可用</h2><p>如果将 Web 服务器集群当做一个城池，那么负载均衡服务器就相当于城门。如果“城门”关闭了，与外界的通道就断了。</p><p>如果只有一台 Nginx 负载服务器，当故障宕机的时候，就会导致整个网站无法访问。</p><p>所以我们需要两台以上 Nginx 来实现故障转移和高可用：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/7.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong>那么如何配置高可用?</strong></p><p><strong>①双机热备方案</strong></p><p>这种方案是国内企业中最为普遍的一种高可用方案，双机热备其实就是指一台服务器在提供服务，另一台为某服务的备用状态，当一台服务器不可用另外一台就会顶替上去。</p><p>Keepalived 是什么？Keepalived 软件起初是专为 LVS 负载均衡软件设计的，用来管理并监控 LVS 集群系统中各个服务节点的状态。</p><p>后来又加入了可以实现高可用的 VRRP (Virtual Router Redundancy Protocol ，虚拟路由器冗余协议）功能。</p><p>因此，Keepalived 除了能够管理 LVS 软件外，还可以作为其他服务（例如：Nginx、Haproxy、MySQL 等）的高可用解决方案软件。</p><p><strong>②故障转移机制</strong></p><p>Keepalived 高可用服务之间的故障切换转移，是通过 VRRP 来实现的。</p><p>在 Keepalived服务正常工作时，主 Master 节点会不断地向备节点发送（多播的方式）心跳消息，用以告诉备 Backup 节点自己还活着。</p><p>当主 Master 节点发生故障时，就无法发送心跳消息，备节点也就因此无法继续检测到来自主  Master 节点的心跳了，于是调用自身的接管程序，接管主 Master 节点的 IP 资源及服务。</p><p>而当主 Master节点恢复时，备 Backup 节点又会释放主节点故障时自身接管的 IP 资源及服务，恢复到原来的备用角色。</p><p>实现方法如下：</p><p>①准备两台安装 Nginx 和 Keepaliver(yum install keepalived -y)的服务器</p><p>②修改两台服务器上的 /etc/keepalived/keepalived.conf</p><pre><code class="hljs bash"><span class="hljs-comment">#主机</span><span class="hljs-comment">#检测脚本</span>vrrp_script chk_http_port &#123;    script <span class="hljs-string">"/usr/local/src/check_nginx.sh"</span> <span class="hljs-comment">#心跳执行的脚本，检测nginx是否启动</span>    interval 2                          <span class="hljs-comment">#（检测脚本执行的间隔，单位是秒）</span>    weight 2                            <span class="hljs-comment">#权重</span>&#125;<span class="hljs-comment">#vrrp 实例定义部分</span>vrrp_instance VI_1 &#123;    state MASTER            <span class="hljs-comment"># 指定keepalived的角色，MASTER为主，BACKUP为备</span>    interface ens33         <span class="hljs-comment"># 当前进行vrrp通讯的网络接口卡(当前centos的网卡) 用ifconfig查看你具体的网卡</span>    virtual_router_id 66    <span class="hljs-comment"># 虚拟路由编号，主从要一直</span>    priority 100            <span class="hljs-comment"># 优先级，数值越大，获取处理请求的优先级越高</span>    advert_int 1            <span class="hljs-comment"># 检查间隔，默认为1s(vrrp组播周期秒数)</span>    <span class="hljs-comment">#授权访问</span>    authentication &#123;        auth_type PASS <span class="hljs-comment">#设置验证类型和密码，MASTER和BACKUP必须使用相同的密码才能正常通信</span>        auth_pass 1111    &#125;    track_script &#123;        chk_http_port            <span class="hljs-comment">#（调用检测脚本）</span>    &#125;    virtual_ipaddress &#123;        192.168.16.150            <span class="hljs-comment"># 定义虚拟ip(VIP)，可多设，每行一个</span>    &#125;&#125;</code></pre><pre><code class="hljs bash"><span class="hljs-comment"># 备机</span><span class="hljs-comment">#检测脚本</span>vrrp_script chk_http_port &#123;    script <span class="hljs-string">"/usr/local/src/check_nginx.sh"</span> <span class="hljs-comment">#心跳执行的脚本，检测nginx是否启动</span>    interval 2                          <span class="hljs-comment">#（检测脚本执行的间隔）</span>    weight 2                            <span class="hljs-comment">#权重</span>&#125;<span class="hljs-comment">#vrrp 实例定义部分</span>vrrp_instance VI_1 &#123;    state BACKUP                        <span class="hljs-comment"># 指定keepalived的角色，MASTER为主，BACKUP为备</span>    interface ens33                      <span class="hljs-comment"># 当前进行vrrp通讯的网络接口卡(当前centos的网卡) 用ifconfig查看你具体的网卡</span>    virtual_router_id 66                <span class="hljs-comment"># 虚拟路由编号，主从要一直</span>    priority 99                         <span class="hljs-comment"># 优先级，数值越大，获取处理请求的优先级越高</span>    advert_int 1                        <span class="hljs-comment"># 检查间隔，默认为1s(vrrp组播周期秒数)</span>    <span class="hljs-comment">#授权访问</span>    authentication &#123;        auth_type PASS <span class="hljs-comment">#设置验证类型和密码，MASTER和BACKUP必须使用相同的密码才能正常通信</span>        auth_pass 1111    &#125;    track_script &#123;        chk_http_port                   <span class="hljs-comment">#（调用检测脚本）</span>    &#125;    virtual_ipaddress &#123;        192.168.16.150                   <span class="hljs-comment"># 定义虚拟ip(VIP)，可多设，每行一个</span>    &#125;&#125;</code></pre><p>③新建检测脚本(chmod 775 check_nginx.sh)：</p><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><span class="hljs-comment">#检测nginx是否启动了</span>A=`ps -C nginx --no-header |wc -l`        <span class="hljs-keyword">if</span> [ <span class="hljs-variable">$A</span> -eq 0 ];<span class="hljs-keyword">then</span>    <span class="hljs-comment">#如果nginx没有启动就启动nginx                        </span>      systemctl start nginx                <span class="hljs-comment">#重启nginx</span>      <span class="hljs-keyword">if</span> [ `ps -C nginx --no-header |wc -l` -eq 0 ];<span class="hljs-keyword">then</span>    <span class="hljs-comment">#nginx重启失败，则停掉keepalived服务，进行VIP转移</span>              killall keepalived                          <span class="hljs-keyword">fi</span><span class="hljs-keyword">fi</span></code></pre><p>④启动 Nginx 和 Keepalived（systemctl start keepalived.service）</p><p>⑤模拟 Nginx 故障（关闭主服务器 Nginx），验证，仍可以通过配置的虚拟 IP 访问，OK。</p><h1 id="Nginx-原理与优化参数配置"><a href="#Nginx-原理与优化参数配置" class="headerlink" title="Nginx 原理与优化参数配置"></a>Nginx 原理与优化参数配置</h1><p>Nginx 默认采用多进程工作方式，Nginx 启动后，会运行一个 Master 进程和多个 Worker 进程。</p><p>其中 Master 充当整个进程组与用户的交互接口，同时对进程进行监护，管理 Worker 进程来实现重启服务、平滑升级、更换日志文件、配置文件实时生效等功能。</p><p>Worker 用来处理基本的网络事件，Worker 之间是平等的，他们共同竞争来处理来自客户端的请求。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/8.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong>master-workers 的机制的好处：</strong></p><ul><li>可以使用 nginx-s reload 热部署。</li><li>每个 Worker 是独立的进程，不需要加锁，省掉了锁带来的开销。采用独立的进程，可以让互相之间不会影响，一个进程退出后，其他进程还在工作，服务不会中断，Master 进程则很快启动新的 Worker 进程。</li></ul><p><strong>需要设置多少个 Worker？</strong>Nginx 同 Redis 类似都采用了 IO 多路复用机制，每个 Worker 都是一个独立的进程，但每个进程里只有一个主线程，通过异步非阻塞的方式来处理请求，即使是成千上万个请求也不在话下。</p><p>每个 Worker 的线程可以把一个 CPU 的性能发挥到极致。所以 Worker 数和服务器的 CPU 数相等是最为适宜的。设少了会浪费 CPU，设多了会造成 CPU 频繁切换上下文带来的损耗。</p><pre><code class="hljs bash"><span class="hljs-comment">#设置 worker 数量。</span> worker_processes 4 <span class="hljs-comment">#work 绑定 cpu(4 work 绑定 4cpu)。 </span> worker_cpu_affinity 0001 0010 0100 1000 <span class="hljs-comment">#work 绑定 cpu (4 work 绑定 8cpu 中的 4 个) 。 </span> worker_cpu_affinity 0000001 00000010 00000100 00001000</code></pre><p><strong>连接数 worker_connection：</strong>这个值是表示每个 Worker 进程所能建立连接的最大值。</p><p>所以，一个 Nginx 能建立的最大连接数，应该是 worker_connections*worker_processes。</p><p>当然，这里说的是最大连接数，对于 HTTP 请 求 本 地 资 源 来 说 ， 能 够 支 持 的 最 大 并 发 数 量 是 worker_connections*worker_processes，如果是支持 http1.1 的浏览器每次访问要占两个连接。</p><p>所以普通的静态访问最大并发数是：worker_connections*worker_processes /2。</p><p>而如果是 HTTP 作为反向代理来说，最大并发数量应该是 worker_connections*worker_processes/4。</p><p>因为作为反向代理服务器，每个并发会建立与客户端的连接和与后端服务的连接，会占用两个连接。</p><p><strong>Nginx 请求处理流程如下图：</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/9.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="Nginx-模块开发"><a href="#Nginx-模块开发" class="headerlink" title="Nginx 模块开发"></a>Nginx 模块开发</h1><p>由于 Nginx 的模块化特性，所以可以支持模块配置，也可以自定义模块，Nginx 的模块开发，程序员目前还不需要太深入。</p><p>Nginx 模块分类如下图：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/10.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong>Nginx配置选项，解压 Nginx 后的配置操作示例：</strong></p><pre><code class="hljs jboss-cli"><span class="hljs-string">./configure</span> <span class="hljs-params">--prefix=/usr/local/nginx</span> <span class="hljs-params">--with-http_stub_status_module</span> <span class="hljs-params">--with-pcre</span>  <span class="hljs-params">--with-http_ssl_module</span></code></pre><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-19/11.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="Nginx-面试题"><a href="#Nginx-面试题" class="headerlink" title="Nginx 面试题"></a>Nginx 面试题</h1><p>①Nginx 功能，你们项目中用到的 Nginx？</p><ul><li><strong>反向代理服务器</strong></li><li><strong>实现负载均衡</strong></li><li><strong>做静态资源服务器</strong></li><li><strong>作为 HTTP Server</strong></li></ul><p>②Nginx 常用命令有哪些？</p><pre><code class="hljs bash">启动nginx    ./sbin/nginx停止nginx    ./sbin/nginx -s stop   ./sbin/nginx -s quit重载配置      ./sbin/nginx -s reload(平滑重启) service nginx reload重载指定配置文件    ./sbin/nginx -c  /usr/<span class="hljs-built_in">local</span>/nginx/conf/nginx.conf查看nginx版本  ./sbin/nginx -v检查配置文件是否正确  ./sbin/nginx -t显示帮助信息  ./sbin/nginx  -h</code></pre><p>③Nginx 常用配置？</p><pre><code class="hljs bash">worker_processes 4;   <span class="hljs-comment">#工作进程数</span>work_connections 65535; <span class="hljs-comment">#每个进程的并发能力</span>error_log  /data/nginx/logs/error.log;  <span class="hljs-comment">#错误日志</span></code></pre><p>④Nginx 是如何实现高并发的？</p><p>Nginx 采用的是多进程（单线程）&amp;多路 IO 复用模型，异步，非阻塞。</p><p>一个主进程 Master，多个工作进程 Worker，每个工作进程可以处理多个请求 ，Master 进程主要负责收集、分发请求。</p><p>每当一个请求过来时，Master 就拉起一个 Worker 进程负责处理这个请求。同时 Master 进程也负责监控 Woker 的状态，保证高可靠性。</p><p>在 Nginx 中的 Work 进程中，为了应对高并发场景，采取了 Reactor 模型（也就是 I/O 多路复用，NIO）。</p><p><strong>I/O 多路复用模型：</strong>在 I/O 多路复用模型中，最重要的系统调用函数就是 Select（其他的还有 epoll 等）。</p><p>该方法能够同时监控多个文件描述符的可读可写情况（每一个网络连接其实都对应一个文件描述符），当其中的某些文件描述符可读或者可写时，Select 方法就会返回可读以及可写的文件描述符个数。</p><p>Nginx Work 进程使用 I/O 多路复用模块同时监听多个 FD（文件描述符），当 Accept、Read、Write 和 Close 事件产生时，操作系统就会回调 FD 绑定的事件处理器。</p><p>这时候 Work 进程再去处理相应事件，而不是阻塞在某个请求连接上等待。</p><p>这样就可以实现一个进程同时处理多个连接。每一个 Worker 进程通过 I/O 多路复用处理多个连接请求。</p><p>为了减少进程切换（需要系统调用）的性能损耗，一般设置 Worker 进程数量和 CPU 数量一致。</p><p>⑤Nginx 和 Apache 的区别？</p><p>轻量级，同样起 Web 服务，比 Apache 占用更少的内存及资源抗并发，Nginx 处理请求是异步非阻塞的，而 Apache 则是阻塞型的。</p><p>在高并发下 Nginx 能保持低资源低消耗高性能高度模块化的设计，编写模块相对简单，最核心的区别在于 Apache 是同步多进程模型，一个连接对应一个进程；Nginx是异步的，多个连接（万级别）可以对应一个进程。</p><p>⑥Nginx 的 Upstream 支持的负载均衡方式？</p><ul><li><strong>轮询（默认）</strong></li><li><strong>weight：</strong>指定权重</li><li><strong>ip_hash：</strong>每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器</li><li><strong>第三方：</strong>fair、url_hash</li></ul><p>⑦Nginx 常见的优化配置有哪些?</p><ul><li><strong>调整 worker_processes：</strong>指 Nginx 要生成的 Worker 数量，最佳实践是每个 CPU 运行 1 个工作进程。</li><li><strong>最大化 worker_connections。</strong></li><li><strong>启用 Gzip 压缩：</strong>压缩文件大小，减少了客户端 HTTP 的传输带宽，因此提高了页面加载速度。</li><li><strong>为静态文件启用缓存。</strong></li><li><strong>禁用 access_logs：</strong>访问日志记录，它记录每个 Nginx 请求，因此消耗了大量 CPU 资源，从而降低了 Nginx 性能。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Prometheus 部署架构选择 </title>
    <link href="/2019/12/18/2019-12-18-prometheus1-architecture/"/>
    <url>/2019/12/18/2019-12-18-prometheus1-architecture/</url>
    
    <content type="html"><![CDATA[<p>本次 Prometheus 实践采用的是 Prometheus Sever + Kube-state-metrics + Grafana 的架构，每一个组件的作用如下:</p><p><strong>Promethues：</strong>提供强大的数据采集、数据存储、数据展示、告警等，天生完美支持 kubernetes，CNCF 基金会的第二个成员，第一个是 Kubernetes。而且 Prometheus 里面很多思想都来源于Google 内部的监控系统 Borgmon，可以说是 Google 的干儿子。</p><p><strong>kube-state-metrics：</strong>在这里作为 prometheus 的一个 exporter 来使用，提供 deployment、daemonset、cronjob 等服务的监控数据，由 kubernestes 官方提供，与 prometheus 紧密结合。因为prometheus 不能获取集群中 Deployment, Job, CronJob 的监控信息。 更多关于 kube-state-metrics 的信息可以参考：<a href="https://github.com/kubernetes/kube-state-metrics" target="_blank" rel="noopener">https://github.com/kubernetes/kube-state-metrics</a> </p><p><strong>Grafana</strong>: 开源 dashboard，后端支持多种数据库，如：Influxdb、Prometheus…，插件也比较多，功能强大。非常适合用于做展示。</p><p>基本架构图可以参考下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-18/191218.png" srcset="/img/loading.gif" alt="avatar"></p><p>需要说明的是本次实践是单集群部署，Prometheus 的联邦机制暂且不考虑。本次的单集群规格信息如下：</p><pre><code class="hljs bash">[root@master-10 ~]<span class="hljs-comment"># kubectl get nodes</span>NAME                    STATUS   ROLES    AGE   VERSIONmaster-10.200.100.216   Ready    master   7d    v1.15.3node-10.200.100.214     Ready    &lt;none&gt;   7d    v1.15.3node-10.200.100.215     Ready    &lt;none&gt;   7d    v1.15.3</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Prometheus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Prometheus Server组件安装 </title>
    <link href="/2019/12/18/2019-12-18-prometheus2-server/"/>
    <url>/2019/12/18/2019-12-18-prometheus2-server/</url>
    
    <content type="html"><![CDATA[<p>这里 Prometheus Server 使用一个带 RBAC 权限的账号采集集群中现有监控信息（其实是从 cadvisor 获取）和节点信息。本次部署是基于比较新的 v2.11.1 版本，网上的一些教程还停留在比较早的版本，所以有些东西改动还是比较大的。</p><p>yaml文件地址：<a href="https://github.com/ILIKETWICE/prometheus-k8s" target="_blank" rel="noopener">https://github.com/ILIKETWICE/prometheus-k8s</a></p><p>具体配置清单文件如下：</p><pre><code class="hljs bahs">├── prometheus-cm-alerts.yaml├── prometheus-cm-config.yaml├── prometheus-deploy.yaml├── prometheus-ingress.yaml├── prometheus-rbac.yaml└── prometheus-svc.yaml</code></pre><p>各个配置文件的作用如下：</p><p><strong>prometheus-cm-alerts.yaml：</strong>该配置文件主要是针对 Alertmanager 定制的一些告警策略，这里暂时不需要 apply <strong>；</strong></p><p><strong>prometheus-cm-config.yaml：</strong> 该配置文件主要是 prometheus 主要的配置文件，prometheus 的配置基本都在此文件中定义；</p><p><strong>prometheus-rbac.yaml</strong>：该文件主要定义了 Prometheus 容器访问 k8s apiserver 所需的 ServiceAccount、ClusterRole 以及 ClusterRoleBinding；</p><p><strong>prometheus-deploy.yaml</strong>：该文件是 Prometheus 的主要的部署文件，里面定义了一些具体参数，可以根据实际集群的规格做相应的调整，个人建议 Prometheus 不要在业务机器上做部署，可以通过节点亲和等特性实现；</p><p><strong>prometheus-svc.yaml</strong>：该文件定义了 Prometheus 的 Service，为了方便服务的暴露方式我选择了 NodePort 的方式，可以根据自己需求做相应的更改；</p><p><strong>prometheus-ingress.yaml</strong>：该文件主要定义了 prometheus 服务的 Ingress 规则，可选；</p><p>配置清单文件准备好后就可以 apply 了：</p><pre><code class="hljs bash">[root@master-10 prometheus-server]<span class="hljs-comment"># kubectl apply -f . --dry-run</span>configmap/prometheus-rule-config configured (dry run)configmap/prometheus-config configured (dry run)deployment.apps/prometheus-server configured (dry run)ingress.extensions/prometheus-server-ingress configured (dry run)serviceaccount/prometheus configured (dry run)clusterrole.rbac.authorization.k8s.io/prometheus configured (dry run)clusterrolebinding.rbac.authorization.k8s.io/prometheus configured (dry run)service/prometheus configured (dry run)</code></pre><p>部署完成后就可以看到对应的 pod 运行起来了：</p><pre><code class="hljs bash">[root@master-10 prometheus-server]<span class="hljs-comment"># kubectl get pods -n prometheus-huang</span>NAME                                 READY   STATUS    RESTARTS   AGEalertmanager-59577dc4bf-qcnvw        1/1     Running   0          19hkube-state-metrics-c8b4bcf8-jstlr    2/2     Running   0          43hnode-exporter-l4228                  1/1     Running   0          42hnode-exporter-qhqf2                  1/1     Running   0          42hnode-exporter-vjtdn                  1/1     Running   0          41hprometheus-grafana-6d8676fb6-w8cvl   1/1     Running   0          24hprometheus-server-784595847-kqznb    1/1     Running   0          19h</code></pre><p>需要注意的是上述信息是部署完成各个组件的部署信息，这里只关注 prometheus-server pod 即可；</p><p>Prometheus Server 到现在就可以说部署好了，当然我这里用的是 Deployment 在正式生产环境下还是建议用 SatefulSet ,保证后端存储的可靠性。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Prometheus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> kube-state-metrics 组件安装 </title>
    <link href="/2019/12/18/2019-12-18-prometheus3-kube-state-metrics/"/>
    <url>/2019/12/18/2019-12-18-prometheus3-kube-state-metrics/</url>
    
    <content type="html"><![CDATA[<p>在这里 kube-state-metrics 是作为 prometheus 的一个 exporter 来使用，主要用来提供集群中的 deployment、daemonset、cronjob 等服务的监控数据。</p><p>yaml文件地址：<a href="https://github.com/ILIKETWICE/prometheus-k8s" target="_blank" rel="noopener">https://github.com/ILIKETWICE/prometheus-k8s</a></p><p>具体配置清单文件如下：</p><pre><code class="hljs bash">├── kube-state-metrics-deploy.yaml├── kube-state-metrics-rbac.yaml└── kube-state-metrics-svc.yaml</code></pre><p>各个配置文件的作用如下：</p><p><strong>kube-state-metrics-rbac.yaml：</strong>该文件主要定义了 kube-state-metrics 容器访问 k8s apiserver 所需的 ServiceAccount、ClusterRole 以及 ClusterRoleBinding；</p><p><strong>prometheus-deploy.yaml</strong>：该文件是 kube-state-metrics 的主要的部署文件，里面定义了一些具体参数，可以根据实际集群的规格做相应的调整；</p><p><strong>prometheus-svc.yaml</strong>：该文件定义了 kube-state-metrics 的 Service，可以根据自己需求做相应的更改，这里只需要使用默认的 ClusterIP 就可以了，因为它只提供给集群内部的 promethes 访问；</p><p>配置清单文件准备好后就可以 apply 了：</p><pre><code class="hljs bash">[root@master-10 kube-state-metrics]<span class="hljs-comment"># kubectl apply -f . --dry-run</span>deployment.apps/kube-state-metrics configured (dry run)serviceaccount/kube-state-metrics configured (dry run)clusterrole.rbac.authorization.k8s.io/kube-state-metrics configured (dry run)role.rbac.authorization.k8s.io/kube-state-metrics-resizer configured (dry run)clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics configured (dry run)rolebinding.rbac.authorization.k8s.io/kube-state-metrics configured (dry run)service/kube-state-metrics configured (dry run)</code></pre><p>部署完成后就可以看到对应的 pod 运行起来了：</p><pre><code class="hljs bash">[root@master-10 prometheus-server]<span class="hljs-comment"># kubectl get pods -n prometheus-huang</span>NAME                                 READY   STATUS    RESTARTS   AGEalertmanager-59577dc4bf-qcnvw        1/1     Running   0          19hkube-state-metrics-c8b4bcf8-jstlr    2/2     Running   0          43hnode-exporter-l4228                  1/1     Running   0          42hnode-exporter-qhqf2                  1/1     Running   0          42hnode-exporter-vjtdn                  1/1     Running   0          41hprometheus-grafana-6d8676fb6-w8cvl   1/1     Running   0          24hprometheus-server-784595847-kqznb    1/1     Running   0          19h</code></pre><p>需要注意的是上述信息是部署完成各个组件的部署信息，这里只关注 kube-state-metrics-c8b4bcf8-jstlr pod 即可.</p>]]></content>
    
    
    
    <tags>
      
      <tag>Prometheus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Node-Expoter 组件安装 </title>
    <link href="/2019/12/18/2019-12-18-prometheus4-node-expoter/"/>
    <url>/2019/12/18/2019-12-18-prometheus4-node-expoter/</url>
    
    <content type="html"><![CDATA[<p>node-exporter 这里主要用来提供集群节点本身的信息的，包括 CPU、内存、硬盘、IO 等等信息。</p><p>yaml文件地址：<a href="https://github.com/ILIKETWICE/prometheus-k8s" target="_blank" rel="noopener">https://github.com/ILIKETWICE/prometheus-k8s</a></p><p>具体配置清单文件如下：</p><pre><code class="hljs bash">├── node-exporter-ds.yaml├── node-exporter-rbac.yaml└── node-exporter-svc.yaml</code></pre><p>各个配置文件的作用如下：</p><p><strong>node-exporter-rbac.yaml：</strong>该文件主要定义了 node-exporter 容器访问 k8s apiserver 所需的 ServiceAccount、ClusterRole 以及 ClusterRoleBinding；</p><p><strong>node-exporter-ds.yaml</strong>：该文件是 <strong>node-exporter</strong> 的主要的部署文件，里面定义了一些具体参数，可以根据实际集群的规格做相应的调整,这里用 DaemonSet,保证可以提供每个节点的信息；</p><p><strong>node-exporter-svc.yaml</strong>：该文件定义了 <strong>node-exporter</strong> 的 Service，可以根据自己需求做相应的更改；</p><p>配置清单文件准备好后就可以 apply 了：</p><pre><code class="hljs bash">[root@master-10 node-exporter]<span class="hljs-comment"># kubectl apply -f . --dry-run</span>daemonset.extensions/node-exporter configured (dry run)serviceaccount/node-exporter configured (dry run)service/node-exporter configured (dry run)</code></pre><p>部署完成后就可以看到对应的 pod 运行起来了：</p><pre><code class="hljs bash">[root@master-10 prometheus-server]<span class="hljs-comment"># kubectl get pods -n prometheus-huang</span>NAME                                 READY   STATUS    RESTARTS   AGEalertmanager-59577dc4bf-qcnvw        1/1     Running   0          19hkube-state-metrics-c8b4bcf8-jstlr    2/2     Running   0          43hnode-exporter-l4228                  1/1     Running   0          42hnode-exporter-qhqf2                  1/1     Running   0          42hnode-exporter-vjtdn                  1/1     Running   0          41hprometheus-grafana-6d8676fb6-w8cvl   1/1     Running   0          24hprometheus-server-784595847-kqznb    1/1     Running   0          19h</code></pre><p>需要注意的是上述信息是部署完成各个组件的部署信息，这里只关注 node-exporter pod 即可；还有就是默认 pod 不会调度到 master 节点上的，需要添加容忍度才可以，具体如下：</p><pre><code class="hljs bash">spec:      ...      tolerations:        - key: node-role.kubernetes.io/master          effect: NoSchedule</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Prometheus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Grafana 可视化组件安装 </title>
    <link href="/2019/12/18/2019-12-18-prometheus5-grafana/"/>
    <url>/2019/12/18/2019-12-18-prometheus5-grafana/</url>
    
    <content type="html"><![CDATA[<p>Grafana 是一个开源的 dashboard，支持用 prometheus 作为数据源，部署起来也比较简单，这里我用的是 ConfigMap 做配置，所以看起来比较复杂。</p><p>yaml文件地址：<a href="https://github.com/ILIKETWICE/prometheus-k8s" target="_blank" rel="noopener">https://github.com/ILIKETWICE/prometheus-k8s</a></p><p>具体配置清单文件如下：</p><pre><code class="hljs bash">├── grafana-cm.yaml├── grafana-dashboard.yaml├── grafana-deploy.yaml├── grafana-ingress.yaml├── grafana-sa.yaml├── grafana-secret.yaml└── grafana-svc.yaml</code></pre><p>文件有点多，其实这样做也是在 Kubernetes 集群部署 Grafana 的比较规范的方式，所有的配置都是通过 ConfigMap 做配置，上述每个文件的具体用途如下：</p><p><strong>grafana-cm.yaml：</strong>该配置文件主要是 grafana 的一些配置文件，包括数据源的配置以及数据保存的位置等等<strong>；</strong></p><p><strong>grafana-dashboard.yaml：</strong> 该配置文件主要是 grafana 导入的 dashboard 模板，其实这些模板可以在部署完成后自己导入的；</p><p><strong>grafana-deploy.yaml</strong>：该文件主要定义了 Grafana 的部署信息，可以根据自己实际情况做修改，生产环境建议做持久化存储；</p><p><strong>grafana-ingress.yaml</strong>：该文件主要是定义 grafana 服务的 Ingress 的规则，通过 Ingress 暴露服务；</p><p><strong>grafana-sa.yaml</strong>：该文件主要定义了 grafana 在集群中的 ServiceAccount；</p><p><strong>grafana-secret.yaml</strong>：该文件主要定义了登录 Grafana 的默认账号密码，通过挂载到部署文件生效；</p><p><strong>grafana-svc.yaml</strong>：该文件主要定义了 Grafana 的服务；</p><p>配置清单文件准备好后就可以 apply 了：</p><pre><code class="hljs bash">[root@master-10 grafana]<span class="hljs-comment"># kubectl apply -f . --dry-run</span>configmap/grafana-ini configured (dry run)configmap/grafana-datasources configured (dry run)configmap/grafana-dashboardproviders configured (dry run)configmap/dashboards configured (dry run)deployment.apps/prometheus-grafana configured (dry run)ingress.extensions/prometheus-grafana-ingress configured (dry run)serviceaccount/grafana configured (dry run)secret/grafana configured (dry run)service/grafana configured (dry run)</code></pre><p>部署完成后就可以看到对应的 pod 运行起来了：</p><pre><code class="hljs bash">[root@master-10 prometheus-server]<span class="hljs-comment"># kubectl get pods -n prometheus-huang</span>NAME                                 READY   STATUS    RESTARTS   AGEalertmanager-59577dc4bf-qcnvw        1/1     Running   0          19hkube-state-metrics-c8b4bcf8-jstlr    2/2     Running   0          43hnode-exporter-l4228                  1/1     Running   0          42hnode-exporter-qhqf2                  1/1     Running   0          42hnode-exporter-vjtdn                  1/1     Running   0          41hprometheus-grafana-6d8676fb6-w8cvl   1/1     Running   0          24hprometheus-server-784595847-kqznb    1/1     Running   0          19h</code></pre><p>需要注意的是上述信息是部署完成各个组件的部署信息，这里只关注 grafana pod 即可；</p><p>部署完成后直接访问就可以，因为之前 ConfigMap 已经做过配置，所以不需要单独配置数据源什么的，默认账号密码如下：</p><table><thead><tr><th align="center">账号</th><th align="left">密码</th></tr></thead><tbody><tr><td align="center">admin</td><td align="left">admin</td></tr></tbody></table><table><thead><tr><th align="center">Kubernetes 版本</th><th align="center">v 1.9</th><th align="center">v 1.10</th><th align="center">v 1.11</th><th align="center">v 1.12</th><th align="center">v 1.13</th><th align="center">v 1.14</th><th align="center">v 1.15</th></tr></thead><tbody><tr><td align="center">版本兼容性</td><td align="center">x</td><td align="center">？</td><td align="center">？</td><td align="center">？</td><td align="center">？</td><td align="center">？</td><td align="center">✓</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>Prometheus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> AlertManager 组件安装 </title>
    <link href="/2019/12/18/2019-12-18-prometheus6-alertmanager/"/>
    <url>/2019/12/18/2019-12-18-prometheus6-alertmanager/</url>
    
    <content type="html"><![CDATA[<p>在 Prometheus 监控中， AlertManager 主要是用来除了告警信息和发送告警的。</p><p>yaml文件地址：<a href="https://github.com/ILIKETWICE/prometheus-k8s" target="_blank" rel="noopener">https://github.com/ILIKETWICE/prometheus-k8s</a></p><p>具体配置清单文件如下：</p><pre><code class="hljs bash">├── alertmanager-cm.yaml├── alertmanager-deploy.yaml├── alertmanager-ingress.yaml├── alertmanager-operated-svc.yaml├── alertmanager-pvc.yaml├── alertmanager-rbac.yaml└── alertmanager-svc.yaml</code></pre><p>各个配置文件的作用如下：</p><p><strong>alertmanager-cm.yaml：</strong>该配置文件主要是针对 Alertmanager 做的一些基础配置以及一些告警途径，包括邮箱、企业微信以及Slack <strong>；</strong></p><p><strong>alertmanager-deploy.yaml：</strong> 该配置文件主要定义 alertmanager 的部署文件,真正生产环境需要考虑后端存储的可靠性；</p><p><strong>alertmanager-ingress.yaml</strong>：该文件主要定义了 alertmanager 的 Ingress 规则，可选，非必须；</p><p><strong>alertmanager-operated-svc.yaml</strong>：该文件主要定义<strong>alertmanager-operated服务，服务地址会在部署文件中用到；</strong></p><p><strong>alertmanager-svc.yaml</strong>：该文件主要用来定义 alertmanager 服务；</p><p><strong>alertmanager-rbac.yaml</strong>：该文件主要定义了部署文件用到的 ServiceAccount；</p><p>配置清单文件准备好后就可以 apply 了：</p><pre><code class="hljs bash">[root@master-10 alertmanager]<span class="hljs-comment"># kubectl apply -f . --dry-run</span>configmap/alertmanager-config configured (dry run)deployment.apps/alertmanager configured (dry run)ingress.extensions/prometheus-alert-ingress configured (dry run)service/alertmanager-operated configured (dry run)persistentvolumeclaim/alertmanager configured (dry run)serviceaccount/alertmanager configured (dry run)service/alertmanager configured (dry run)</code></pre><p>部署完成后就可以看到对应的 pod 运行起来了：</p><pre><code class="hljs bash">[root@master-10 prometheus-server]<span class="hljs-comment"># kubectl get pods -n prometheus-huang</span>NAME                                 READY   STATUS    RESTARTS   AGEalertmanager-59577dc4bf-qcnvw        1/1     Running   0          19hkube-state-metrics-c8b4bcf8-jstlr    2/2     Running   0          43hnode-exporter-l4228                  1/1     Running   0          42hnode-exporter-qhqf2                  1/1     Running   0          42hnode-exporter-vjtdn                  1/1     Running   0          41hprometheus-grafana-6d8676fb6-w8cvl   1/1     Running   0          24hprometheus-server-784595847-kqznb    1/1     Running   0          19h</code></pre><p>需要注意的是上述信息是部署完成各个组件的部署信息，这里只关注 alertmanager pod 即可；</p><p>到这里基本整个 prometheus 就部署完成了，告警及其规则也配置好了，这里只是配置了微信企业告警，需要其他方式告警可以参考<a href="https://aeric.io/post/prometheus-alertmanager-config/" target="_blank" rel="noopener">https://aeric.io/post/prometheus-alertmanager-config/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Prometheus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> Prometheus架构原理及其组成 </title>
    <link href="/2019/12/17/2019-12-17-prometheus-introduction/"/>
    <url>/2019/12/17/2019-12-17-prometheus-introduction/</url>
    
    <content type="html"><![CDATA[<h1 id="Prometheus-系统介绍"><a href="#Prometheus-系统介绍" class="headerlink" title="Prometheus 系统介绍"></a><a href="https://confluence.qingclass.cn/pages/viewpage.action?pageId=27755014#prometheus-系统介绍" target="_blank" rel="noopener">Prometheus 系统介绍</a></h1><p>首先，Prometheus 受启发于 Google 的 Brogmon 监控系统，相似的 Kubernetes 是从 Google 的 Brog 系统演变而来，从 2012 年开始由前 Google 工程师在 Soundcloud 以开源软件的形式进行研发，并且于 2015 年早期对外发布早期版本。2016 年 5 月继 Kubernetes 之后成为第二个正式加入 CNCF 基金会的项目，同年 6 月正式发布 1.0 版本。2017 年底发布了基于全新存储层的 2.0 版本，能更好地与容器平台、云平台配合。</p><p>Prometheus 的发展简史如下图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-17/191217-1.png" srcset="/img/loading.gif" alt="avatar"></p><p>作为新一代监控系统，Prometheus 可以说是彻底颠覆了传统的监控系统，作为监控系统，一般都离不开以下监控<strong>目标：</strong></p><p><strong>长期趋势分析</strong>：通过对监控样本数据的持续收集和统计，对监控指标进行长期趋势分析。例如，通过对磁盘空间增长率的判断，我们可以提前预测在未来什么时间节点上需要对资源进行扩容。</p><p><strong>对照分析</strong>：两个版本的系统运行资源使用情况的差异如何？在不同容量情况下系统的并发和负载变化如何？通过监控能够方便的对系统进行跟踪和比较。</p><p><strong>告警</strong>：当系统出现或者即将出现故障时，监控系统需要迅速反应并通知管理员，从而能够对问题进行快速的处理或者提前预防问题的发生，避免出现对业务的影响。</p><p><strong>故障分析与定位</strong>：当问题发生后，需要对问题进行调查和处理。通过对不同监控监控以及历史数据的分析，能够找到并解决根源问题。</p><p><strong>数据可视化：</strong>通过可视化仪表盘能够直接获取系统的运行状态、资源使用情况、以及服务运行状态等直观的信息。</p><p>与传统监控系统相比， Prometheus 是一个开源的完整监控解决方案，其对传统监控系统的测试和告警模型进行了彻底的颠覆，形成了基于中央化的规则计算、统一分析和告警的新模型。Prometheus 具有许多独特的优势：</p><p><strong>易于管理</strong>: Prometheus 核心部分只有一个单独的二进制文件，不存在任何的第三方依赖(数据库，缓存等等)。唯一需要的就是本地磁盘，因此不会有潜在级联故障的风险。Prometheus 基于 Pull 模型的架构方式，可以在任何地方（本地电脑，开发环境，测试环境）搭建我们的监控系统。对于一些复杂的情况，还可以使用Prometheus 服务发现 (Service Discovery) 的能力动态管理监控目标。<br>监控服务的内部运行状态: Pometheus 鼓励用户监控服务的内部状态，基于 Prometheus 丰富的 Client 库，用户可以轻松的在应用程序中添加对 Prometheus 的支持，从而让用户可以获取服务和应用内部真正的运行状态。<br><strong>数据模型</strong>: Prometheus 所有采集的监控数据均以指标 (metric) 的形式保存在内置的时间序列数据库当中(TSDB)。所有的样本除了基本的指标名称以外，还包含一组用于描述该样本特征的标签。每一条时间序列由指标名称(Metrics Name)以及一组标签(Labels)唯一标识。每条时间序列按照时间的先后顺序存储一系列的样本值。<br><strong>查询语言PromQL</strong>: Prometheus 内置了一个强大的数据查询语言 PromQL。 通过 PromQL 可以实现对监控数据的查询、聚合。同时 PromQL 也被应用于数据可视化(如Grafana)以及告警当中。<br><strong>高效、可扩展、易于集成</strong>: Prometheus 对于联邦集群的支持，可以让多个 Prometheus 实例产生一个逻辑集群，当单实例 Prometheus Server 处理的任务量过大时，通过使用功能分区(sharding)+联邦集群(federation)可以对其进行扩展。使用Prometheus可以快速搭建监控服务，并且可以非常方便地在应用程序中进行集成。<br>Prometheus 还有许多特性，这里不再一一赘述，详细信息可以查阅<a href="https://prometheus.io/docs/introduction/overview/" target="_blank" rel="noopener">官方文档</a></p><h1 id="Prometheus-核心组件"><a href="#Prometheus-核心组件" class="headerlink" title="Prometheus 核心组件"></a><a href="https://confluence.qingclass.cn/pages/viewpage.action?pageId=27755014#prometheus-核心组件" target="_blank" rel="noopener">Prometheus 核心组件</a></h1><p>Prometheus 的基本架构图如下图所示：（图片来源于官方网站）</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-17/191217-2.png" srcset="/img/loading.gif" alt="avatar"></p><h2 id="Prometheus-Server"><a href="#Prometheus-Server" class="headerlink" title="Prometheus Server"></a><a href="https://confluence.qingclass.cn/pages/viewpage.action?pageId=27755014#prometheus-server" target="_blank" rel="noopener">Prometheus Server</a></h2><p>Prometheus Server 是 Prometheus 组件中的核心部分，负责实现对监控数据的获取，存储以及查询。 Prometheus Server 可以通过静态配置管理监控目标，也可以配合使用 Service Discovery 的方式动态管理监控目标，并从这些监控目标中获取数据。其次 Prometheus Server 需要对采集到的监控数据进行存储，Prometheus Server 本身就是一个时序数据库，将采集到的监控数据按照时间序列的方式存储在本地磁盘当中。最后 Prometheus Server 对外提供了自定义的 PromQL 语言，实现对数据的查询以及分析。</p><p>Prometheus Server 内置的 Express Browser UI，通过这个 UI 可以直接通过 PromQL 实现数据的查询以及可视化。</p><p>Prometheus Server 的联邦集群能力可以使其从其他的 Prometheus Server 实例中获取数据，因此在大规模监控的情况下，可以通过联邦集群以及功能分区的方式对 Prometheus Server 进行扩展。</p><h2 id="Exporters"><a href="#Exporters" class="headerlink" title="Exporters"></a><a href="https://confluence.qingclass.cn/pages/viewpage.action?pageId=27755014#exporters" target="_blank" rel="noopener">Exporters</a></h2><p>Exporter 将监控数据采集的端点通过 HTTP 服务的形式暴露给 Prometheus Server，然后 Prometheus Server 通过访问该 Exporter 提供的 Endpoint 端点，即可获取到需要采集的监控数据。</p><p>一般来说可以将 Exporter 分为 2 类：</p><ul><li>直接采集：这一类 Exporter 直接内置了对 Prometheus 监控的支持，比如 cAdvisor，Kubernetes，Etcd，Gokit等，都直接内置了用于向 Prometheus 暴露监控数据的端点；</li><li>间接采集：间接采集，原有监控目标并不直接支持 Prometheus，因此我们需要通过 Prometheus 提供的 Client Library 编写该监控目标的监控采集程序。例如: Mysql Exporter，JMX Exporter，Consul Exporter等。</li></ul><h2 id="AlertManager"><a href="#AlertManager" class="headerlink" title="AlertManager"></a><a href="https://confluence.qingclass.cn/pages/viewpage.action?pageId=27755014#alertmanager" target="_blank" rel="noopener">AlertManager</a></h2><p>在 Prometheus Server 中支持基于 PromQL 创建告警规则，如果满足 PromQL 定义的规则，则会产生一条告警，而告警的后续处理流程则由 AlertManager 进行管理。在 AlertManager 中我们可以与邮件，Slack 等等内置的通知方式进行集成，也可以通过 Webhook 自定义告警处理方式。AlertManager 即 Prometheus 体系中的告警处理中心。</p><h2 id="PushGateway"><a href="#PushGateway" class="headerlink" title="PushGateway"></a><a href="https://confluence.qingclass.cn/pages/viewpage.action?pageId=27755014#pushgateway" target="_blank" rel="noopener">PushGateway</a></h2><p>由于 Prometheus 数据采集基于 Pull 模型进行设计，因此在网络环境的配置上必须要让 Prometheus Server 能够直接与 Exporter 进行通信。 当这种网络需求无法直接满足时，就可以利用 PushGateway 来进行中转。可以通过PushGateway 将内部网络的监控数据主动 Push 到 Gateway 当中。而 Prometheus Server 则可以采用同样 Pull 的方式从 PushGateway 中获取到监控数据。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Prometheus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> GitLab 连接 K8S 集群 </title>
    <link href="/2019/12/16/2019-12-16-gitlab-k8s/"/>
    <url>/2019/12/16/2019-12-16-gitlab-k8s/</url>
    
    <content type="html"><![CDATA[<p>用 <code>GitLab</code> 连接 <code>Kubernetes</code> 需要明确以下几点内容：</p><ul><li>目标集群的 API 连接地址；</li><li>集群的 CA 证书；</li><li>基于RBAC 的特定 ServiceAccount 的 Token；</li><li>需要部署 pod 到哪个 NameSpace；</li></ul><p><strong>获取集群的 API 连接地址</strong></p><p>获取集群的 API 连接地址可以用如下命令查看：</p><pre><code class="hljs bash">$ kubectl cluster-infoKubernetes master is running at https://59.110.217.141:6443metrics-server is running at https://59.110.217.141:6443/api/v1/namespaces/kube-system/services/heapster/proxyKubeDNS is running at https://59.110.217.141:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</code></pre><pre><code class="hljs groovy">命令输出内容可以很快知道集群的连接地址是：<span class="hljs-string">https:</span><span class="hljs-comment">//59.110.217.141:6443</span></code></pre><blockquote><p>需要注意的是，有时候集群的连接地址分为公网地址和内网地址，这个时候可以根据需求自己做取舍，轻课的 GitLab 和阿里云的 Kubernetes 集群没有打通，一般用的是公网地址</p></blockquote><p><strong>集群 CA 证书</strong></p><p>一般一个集群的 CA 证书是唯一的，所以只需弄一次即可，以后就可以重复利用。</p><p>比如说，我们要在某个集群中的 <code>gatlin</code> 命名空间中部署我们的微服务应用，我们需要创建一个 <code>ServiceAccount</code> 并绑定某个 <code>Role</code> 来获取该命名空间的特定权限。</p><p>一般情况下，在 <code>kubernetes</code> 集群中创建某个 <code>sa</code> 会对应生成该<code>sa</code> 的 <code>secret</code>，我们可以通过该 <code>secret</code> 的具体信息获取整个集群的 <code>CA</code> 证书和对应 <code>sa</code> 的 <code>token</code></p><p><strong>创建对应命名空间的 ServiceAccount</strong></p><pre><code class="hljs bash">$ kubectl -n gatlin create sa gatlin-admin为了规范管理，以后所用 ns 的 sa 统一格式为 &lt;NS-NAME&gt;-admin</code></pre><p><strong>查看对应命名空间的 ServiceAccount</strong></p><pre><code class="hljs bash">$ kubectl -n gatlin get serviceaccountsNAME                     SECRETS   AGEdefault                  1         20hgatlin-admin             1         103m</code></pre><p><strong>给新建的 SA 绑定 Role</strong></p><p>对于权限这块，我一般都是直接绑定名称为 admin 的集群角色，这样该 sa 就获得了该 ns 的管理员权限，而这个 sa 对其他 ns 是没有任何权限的，符合 ci 流程。</p><pre><code class="hljs bash">$ kubectl -n gatlin create clusterrolebinding gatlin-ns-admin --clusterrole=admin --serviceaccount=gatlin:gatlin-admin</code></pre><p><strong>查看自动生成的 secret</strong></p><pre><code class="hljs bash">$ kubectl get secrets -n gatlinNAME                                 TYPE                                  DATA   AGEdefault-token-fgw4s                  kubernetes.io/service-account-token   3      20hgatlin-admin-token-78pqk             kubernetes.io/service-account-token   3      108mgatlin-service-account-token-nwhh2   kubernetes.io/service-account-token   3      88mgatlin-token                         kubernetes.io/service-account-token   3      88m然后用自动生成的 secret 查看集群 CA 证书就可以了：$ kubectl -n gatlin get secrets gatlin-admin-token-78pqk -o jsonpath=<span class="hljs-string">"&#123;['data']['ca\.crt']&#125;"</span> | base64 -d-----BEGIN CERTIFICATE-----MIIDGjCCAgKgAwIBAgIBADANBgkqhkiG9w0BAQsFADA+MScwFAYDVQQKEw1hbGliYWJhIGNsb3VkMA8GA1UEChMIaGFuZ3pob3UxEzARBgNVBAMTCmt1YmVybmV0ZXMwHhcNMTkwNDIzMDkyMjI5WhcNMjkwNDIwMDkyMjI5WjA+MScwFAYDVQQKEw1hbGliYWJhIGNsb3VkMA8GA1UEChMIaGFuZ3pob3UxEzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCrNkXt3CB9HewGw6FLorrmrpYqLxsNhouv61d5nbyD1mxcIQAlK879yFmEfvs6Mo1kp0X/BalTBR6tLvXnQrv6t89n8sflmK5eaw4XVi+bpvVxyG2EyI0VBMzI77y5XYVkg0dD90lTfVyuLYx0h5e7g5/SQmXBvSFotN6+ci3eZDDHluOr72QPLnRukWZNfJfQa5njTb53AxyHEV/qk35NTfI7Er4GEAHniRjbg3zBHypIGc2XQlBROvON/CGehUI20agi+LZ6cNwDeYMQvbMaVc3hMlQRrscfwKWwiHumdCv1B6ALoF6dtqJp3ry+MS4VTnMc5ZdCsBRa64aaA61/AgMBAAGjIzAhMA4GA1UdDwEB/wQEAwICpDAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4IBAQAetfaOvLN7eklSZRnWBOqZN30ohgN9na/q+vsdtrFSqpRiqeWCE0IIb0G8188J8ITLczgT3h3BrMOvh3KTrIre1+P6zH2bRdfhFD4upbYQRIpLWYaEPxsEihjQFGhpOrvgJVdYpMC1/m09Ili9C82f1hZQp1S0+a0lvRXfR/ox76mvZ2CKoXcP7n2uoh4cVbta8B1r4RSLwdBybWNAkD6NMPZ53LphRLdNaN1KvQJmAkOGX7MiAZeupX+jz0mW+m+cWN0ftiUAuqsaPAfMOg6JgxHpM44WU+wu2aduFxKXzzudXns1hh44sDQ5vg5JhNd/06tyy2NiaqZRDRJ+ie1ewrwerjewrewj-----END CERTIFICATE-----</code></pre><p>然后就可以看到神奇的 CA 证书了，哈哈。</p><p><strong>获取特定 ServiceAccount 的 Token</strong></p><p>获取 token 的时候直接用上面的 secret 即可：</p><pre><code class="hljs bash">$ kubectl -n gatlin get secrets gatlin-admin-token-78pqk -o jsonpath=&#123;.data.token&#125; | base64 -d</code></pre><p>记得将上述命令输出的 token 保存一下就可以，因为 token 太长而且没有自动换行，这里就不贴了。</p><p><strong>配置 GitLab</strong></p><p>上面的工作做完了，配置 GitLab 连接 Kubernetes 集群就简单多了，『运维』–『Kubernetes』–『Add existing cluster』,然后直接把上述信息复制粘贴到对应框框里就行了</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-16/gitlab-k8s.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong><em>需要注意的是每个命名空间对应的 token 都不一样，不要盲目复制粘贴，还有就是如果配置错误需要将添加的集群删掉重建，有时候可能因为缓存的问题导致连接集群失败，这是重点，切记！！！</em></strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Gitlab</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 阿里云子账号权限策略 </title>
    <link href="/2019/12/15/2019-12-15-aliyun-policy/"/>
    <url>/2019/12/15/2019-12-15-aliyun-policy/</url>
    
    <content type="html"><![CDATA[<h1 id="1、OSS读写权限"><a href="#1、OSS读写权限" class="headerlink" title="1、OSS读写权限"></a>1、OSS读写权限</h1><pre><code class="hljs json">&#123;    <span class="hljs-attr">"Version"</span>: <span class="hljs-string">"1"</span>,    <span class="hljs-attr">"Statement"</span>: [        &#123;            <span class="hljs-attr">"Effect"</span>: <span class="hljs-string">"Allow"</span>,            <span class="hljs-attr">"Action"</span>: [                <span class="hljs-string">"oss:ListBuckets"</span>,                <span class="hljs-string">"oss:GetBucketStat"</span>,                <span class="hljs-string">"oss:GetBucketInfo"</span>,                <span class="hljs-string">"oss:GetBucketTagging"</span>,                <span class="hljs-string">"oss:GetBucketAcl"</span>            ],            <span class="hljs-attr">"Resource"</span>: <span class="hljs-string">"acs:oss:*:*:*"</span>        &#125;,        &#123;            <span class="hljs-attr">"Effect"</span>: <span class="hljs-string">"Allow"</span>,            <span class="hljs-attr">"Action"</span>: [                <span class="hljs-string">"oss:Get*"</span>,                <span class="hljs-string">"oss:List*"</span>            ],            <span class="hljs-attr">"Resource"</span>: <span class="hljs-string">"acs:oss:*:*:oss名称"</span>        &#125;,        &#123;            <span class="hljs-attr">"Effect"</span>: <span class="hljs-string">"Allow"</span>,            <span class="hljs-attr">"Action"</span>: [                <span class="hljs-string">"oss:Get*"</span>,                <span class="hljs-string">"oss:List*"</span>,                <span class="hljs-string">"oss:Put*"</span>,                <span class="hljs-string">"oss:AbortMultipartUpload"</span>            ],            <span class="hljs-attr">"Resource"</span>: <span class="hljs-string">"acs:oss:*:*:oss名称/*"</span>        &#125;    ]&#125;</code></pre><h1 id="2、日志服务查看指定Project-只能创建，不能删除"><a href="#2、日志服务查看指定Project-只能创建，不能删除" class="headerlink" title="2、日志服务查看指定Project (只能创建，不能删除)"></a>2、日志服务查看指定Project (只能创建，不能删除)</h1><pre><code class="hljs json">&#123;    <span class="hljs-attr">"Version"</span>: <span class="hljs-string">"1"</span>,    <span class="hljs-attr">"Statement"</span>: [        &#123;            <span class="hljs-attr">"Action"</span>: [                <span class="hljs-string">"log:ListProject"</span>            ],            <span class="hljs-attr">"Resource"</span>: [                <span class="hljs-string">"acs:log:*:*:project/*"</span>            ],            <span class="hljs-attr">"Effect"</span>: <span class="hljs-string">"Allow"</span>        &#125;,        &#123;            <span class="hljs-attr">"Action"</span>: [                <span class="hljs-string">"log:Get*"</span>,                <span class="hljs-string">"log:List*"</span>,                <span class="hljs-string">"log:Create*"</span>,                <span class="hljs-string">"log:Update*"</span>,                <span class="hljs-string">"log:Post*"</span>            ],            <span class="hljs-attr">"Resource"</span>: <span class="hljs-string">"acs:log:*:*:project/project名称/*"</span>,            <span class="hljs-attr">"Effect"</span>: <span class="hljs-string">"Allow"</span>        &#125;    ]&#125;</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>阿里云</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 执行力，才是拉开人与人差距的关键 </title>
    <link href="/2019/12/14/2019-12-14-execution/"/>
    <url>/2019/12/14/2019-12-14-execution/</url>
    
    <content type="html"><![CDATA[<h1 id="01-如果你想变得更好-首先要学会执行"><a href="#01-如果你想变得更好-首先要学会执行" class="headerlink" title="01 如果你想变得更好 首先要学会执行"></a>01 如果你想变得更好 首先要学会执行</h1><p>我们常常慨叹，为什么都是吃五谷杂粮长大的，人与人之间的差距怎么就那么大呢？</p><p>为什么毕业于同一个学校的人，几年后的差距也会变得那么大呢？</p><p>这其中有一个很重要的因素——<strong>执行力。</strong></p><p>那些能够超越同龄人的人，往往都有这样一个优点，他们除了有远见外，还特别肯吃苦，他们的勤奋和执行力往往能甩出同龄人几条街。</p><p>缺乏执行力，人就会焦虑、迷茫。</p><p>执行力是改变人生最直接有效的方法，没有之一。</p><p>当我们哀叹老板一直没有给自己涨工资，现在的工作不适合自己发展时，我们付出了多少行动？</p><p>一个人在舒适的环境待久了，无论他多么想改变，如果他不去执行，最终都很难取得大的成就。</p><p>马云有一句非常经典的名言：</p><p><strong>“晚上想想千条路，早上起来走原路。**</strong>”**</p><p><strong>说的就是一群人不去执行，只是空想，最终就只能是黄粱一梦。</strong></p><p>所以，想要变得更好，首先，你要学会去执行。</p><p>写出文章最好的办法，不是搜集资料，而是马上在键盘上敲下第一个字。提升演讲能力，突破演讲恐惧，最好的办法是，先冲上台再说。</p><p>等待、默默发力不是最好的准备，只有跨出第一步才叫执行。</p><p>我们常常看到很多曾经淡出人们视野的明星在某段时间后又重新成为人们谈论的话题，人们慨叹他们在年华逝去后还能保持美好的容颜，匀称的身材，仿佛岁月这把杀猪刀已经将他们遗忘。</p><p>其实，岁月并没有饶过谁，只是他们出色的自律力和执行力，让他们能够坚持自我锻炼，将自己塑造成了人们期待的样子。</p><h1 id="02-如何提升执行力？"><a href="#02-如何提升执行力？" class="headerlink" title="02  如何提升执行力？"></a>02  如何提升执行力？</h1><p>有人会说，如何才能练就这样的执行力呢？</p><p>最好的执行者从来都是自己主动去做，不是被动等待别人安排工作，自己才去做的人，而是能根据需求，主动去找事情做的人。</p><p>我们来看看香港的“珠宝大王”郑裕彤是怎么做的。</p><p>郑裕彤出身贫寒，为了养家糊口，小学毕业后，郑裕彤便到父亲的朋友周至元所开的“周大福金铺”去当学徒。</p><p>尽管做的是最底层的工作，但他丝毫不懈怠，每天都早早的赶到金铺，将金铺收拾打扫得干干净净。</p><p>往往是等他收拾完了之后，大伙计们才姗姗来迟。</p><p>店里的伙计，大多只知道埋头做本分事，而郑裕彤，除了做好本分事外，还特别爱动脑筋，经常琢磨和研究怎样做才能更有利于金铺的发展。</p><p>一天，老板让他到码头接一位亲戚。这时他看到有一位南洋侨商上了码头，并向人打听哪里可以兑换港币。</p><p>郑裕彤灵机一动，立即走上前去，说周大福金铺可以兑换，价格也最公道，并立即带路，将这位侨商带进了周大福，之后又马不停蹄地赶回码头接那位亲戚。</p><p>郑裕彤的这一做法，让周老板大为赞赏。</p><p>还有一次，伙计们开工好一会儿了，郑裕彤才气喘吁吁地跑进来。老板很生气，问他到哪里去了。</p><p>郑裕彤回答说，自己看别人家珠宝行做生意去了。</p><p>老板不禁有些好奇，于是问他看出了什么名堂没有。</p><p>“我看别家的生意，比我们店里做得精明，只要客人一踏进店门，店里老板、伙计总是笑脸相迎，有问必答；无论生意大小，一视同仁。</p><p>即使这回生意做不成，给人家留下一个好印象，下回他们自然还会光顾！”</p><p>“另外，店铺一定要开在做生意的旺地，门面装修也要讲究，特别是做珠宝生意，一定要显得十分气派。”</p><p><strong>郑裕彤的回答让老板不禁对这个小伙计有些刮目相看，他没想到这些经商诀窍能够从这个小学徒的口中总结出来。</strong></p><p>自那以后，老板开始有意识地培养他，还将女儿嫁给了他。</p><p>后来，他成为香港金行龙头老大“周大福”的掌门人。在郑裕彤的经营下，“周大福”已经成为了珠宝行和金铺的代名词。</p><p>假如郑裕彤面对那位兑换港元的南洋侨商，是这么想的：</p><p>“金铺又没多给我工钱，我主动去管什么闲事，多一事不如少一事。”</p><p>假如他不主动去琢磨怎么做生意，而是想：</p><p>“我一个小伙计，就算操这份心又有什么用？”</p><p>那么，结果又会怎样呢？</p><p><strong>一流的执行者，他首先会觉得那些问题就是自己的问题，要主动地创造性地去解决；</strong></p><p><strong>他会觉得好机会是单位的机会，也是自己的机会。</strong></p><p>无论这件事情与自己有没有直接关系，也无论自己的职位多么普通，他们都会当仁不让地去做。</p><p>而机会，往往就会因此而产生。</p><h1 id="03-执行力-拉开人与人差距的关键"><a href="#03-执行力-拉开人与人差距的关键" class="headerlink" title="03  执行力   拉开人与人差距的关键"></a>03  执行力   拉开人与人差距的关键</h1><p>皇明太阳能集团创始人、总裁黄鸣，就是一个非常典型的拥有超强执行力的人。</p><p>黄鸣大学毕业后被分到了石油钻井技术研究所，在技术装备室工作。工作两年后，地矿部有一个斥资几十亿元的“七五”大型设备改造项目，即为了提升钻井勘探的技术水平而要把所有的钻机都改造一遍。</p><p>当时部里把这个课题交给了比黄鸣所在装备室的级别和规模更高一级的装备研究所，为此还专门召开了钻机改造方案的评审会，黄鸣当时抱着学习的心态参加了评审会。</p><p>当时有几位年龄比较大的高工（高级工程师）在会上介绍方案，黄鸣听得非常仔细。</p><p>但听着听着，他觉得方案有问题。</p><p><strong>一是方案中有很多理论依据、设计计算跟大学的专业教科书和他所看到的国内外相关文献不符；</strong></p><p><strong>二是实施方案缺乏可操作性，设备改造方案与现场情况有很多不符之处。</strong></p><p>在大学期间，黄鸣的专业课程学得非常深入，每门都是优，实习期间，他又把整个井架、钻台、动力系统等摸得一清二楚，写了厚厚的实习报告，工作两年，他特别关注专业动态，写过几篇专业的文章，发表后引起了很大的关注。</p><p>正因为有对专业技术的深度把握，黄鸣快速捕捉到了方案的不足。</p><p>于是，他把自己认为不妥的地方逐条记下来，共列出了二十几条。等几位高工讲完方案让大家提意见的时候，黄鸣鼓起勇气一下讲了十几条。</p><p>讲的时候一气呵成，但讲完之后，他开始忐忑不安起来，毕竟在座的都是专家、司长、总工，自己不知深浅地提建议，会不会让别人感觉这个初出茅庐的小伙子太不知天高地厚了？</p><p>当天晚上，领导就把他叫到了办公室。当时他忐忑不安的心情可想而知。然而，让他没有想到的是，领导告诉他，听了他的意见之后，大家都很重视，为此特意开会进行讨论，认为他提的很多建议很重要，数据也很翔实，说明现在方案不成熟，存在漏洞，需要调整。</p><p>经过慎重考虑，领导决定把设备改造项目的任务分一半给他们科室，由他牵头，与另一科室共同完成，并正式通知他加入“七五”设备领导改造五人小组。</p><p>就这样，刚刚毕业两年的黄鸣获得了这个很多人想都不敢想的机会，他不负重托，带领课题小组顺利完成了任务，并获得了部里的科技进步二等奖。</p><p>从此，他不断承揽科研课题，年纪轻轻就当上了科研室副主任，成为所里的科研主力。这也为他以后的创业打下了坚实的基础。</p><p>黄鸣的做法，正是让执行力与创新力结合的典范。他的过人之处表现在以下两点：</p><p><strong>第一，不迷信权威，即使是众多专家的方案，如果有漏洞，他也敢于大胆说出自己的想法。</strong></p><p><strong>第二，虽然项目和自己无关，但还是主动去想创新的方案。</strong></p><p>工作中，很多人就像机器人一样，执行中很死板，被动地遵守常规。</p><p>其实，最好的执行者往往能够主动打破条条框框，并把创新力落实到执行中，主动为单位做出贡献。</p><p>只要你时刻围绕“如何将工作做得更好”去思考和琢磨，即使在平凡的岗位上，你也能做出有价值的创新。</p><p>这正是在更高的层面上去完善执行力。</p><p>所以，决定人生高度的，从来不是你的高谈阔论，而是你说做就做的执行力，没有执行力一切都是零。</p><p>执行力的不同，人与人的差距自然就拉开了，从开始的一点点到最后的相差甚远，最后会让你悔不当初。</p>]]></content>
    
    
    
    <tags>
      
      <tag>执行力</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 一行 Python 代码能实现这么多丧心病狂的功能？</title>
    <link href="/2019/12/13/2019-12-13-one-line-of-python-code/"/>
    <url>/2019/12/13/2019-12-13-one-line-of-python-code/</url>
    
    <content type="html"><![CDATA[<h1 id="一行代码打印乘法口诀"><a href="#一行代码打印乘法口诀" class="headerlink" title="一行代码打印乘法口诀"></a>一行代码打印乘法口诀</h1><pre><code class="hljs python">print(<span class="hljs-string">'\n'</span>.join([<span class="hljs-string">' '</span>.join([<span class="hljs-string">"%2s x%2s = %2s"</span>%(j,i,i*j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,i+<span class="hljs-number">1</span>)]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)]))</code></pre><h1 id="一行代码打印迷宫"><a href="#一行代码打印迷宫" class="headerlink" title="一行代码打印迷宫"></a>一行代码打印迷宫</h1><pre><code class="hljs python">print(<span class="hljs-string">''</span>.join(__import__(<span class="hljs-string">'random'</span>).choice(<span class="hljs-string">'\u2571\u2572'</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">50</span>*<span class="hljs-number">24</span>)))</code></pre><h1 id="一行代码表白爱情"><a href="#一行代码表白爱情" class="headerlink" title="一行代码表白爱情"></a>一行代码表白爱情</h1><pre><code class="hljs python">print(<span class="hljs-string">'\n'</span>.join([<span class="hljs-string">''</span>.join([(<span class="hljs-string">'Love'</span>[(x-y) % len(<span class="hljs-string">'Love'</span>)] <span class="hljs-keyword">if</span> ((x*<span class="hljs-number">0.05</span>)**<span class="hljs-number">2</span>+(y*<span class="hljs-number">0.1</span>)**<span class="hljs-number">2</span><span class="hljs-number">-1</span>)**<span class="hljs-number">3</span>-(x*<span class="hljs-number">0.05</span>)**<span class="hljs-number">2</span>*(y*<span class="hljs-number">0.1</span>)**<span class="hljs-number">3</span> &lt;= <span class="hljs-number">0</span><span class="hljs-keyword">else</span><span class="hljs-string">' '</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(<span class="hljs-number">-30</span>, <span class="hljs-number">30</span>)]) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(<span class="hljs-number">30</span>, <span class="hljs-number">-30</span>, <span class="hljs-number">-1</span>)]))！</code></pre><h1 id="一行代码打印小龟龟"><a href="#一行代码打印小龟龟" class="headerlink" title="一行代码打印小龟龟"></a>一行代码打印小龟龟</h1><pre><code class="hljs python">print(<span class="hljs-string">'\n'</span>.join([<span class="hljs-string">''</span>.join([<span class="hljs-string">'*'</span> <span class="hljs-keyword">if</span> abs((<span class="hljs-keyword">lambda</span> a:<span class="hljs-keyword">lambda</span> z,c,n:a(a,z,c,n))(<span class="hljs-keyword">lambda</span> s,z,c,n:z <span class="hljs-keyword">if</span> n==<span class="hljs-number">0</span> <span class="hljs-keyword">else</span> s(s,z*z+c,c,n<span class="hljs-number">-1</span>))(<span class="hljs-number">0</span>,<span class="hljs-number">0.02</span>*x+<span class="hljs-number">0.05j</span>*y,<span class="hljs-number">40</span>))&lt;<span class="hljs-number">2</span> <span class="hljs-keyword">else</span> <span class="hljs-string">' '</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(<span class="hljs-number">-80</span>,<span class="hljs-number">20</span>)]) <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(<span class="hljs-number">-20</span>,<span class="hljs-number">20</span>)]))</code></pre><h1 id="一行代码实现-1-100-的和"><a href="#一行代码实现-1-100-的和" class="headerlink" title="一行代码实现 1 - 100 的和"></a>一行代码实现 1 - 100 的和</h1><pre><code class="hljs python">sum(range(<span class="hljs-number">1</span>,<span class="hljs-number">101</span>))</code></pre><h1 id="一行代码实现数值交换"><a href="#一行代码实现数值交换" class="headerlink" title="一行代码实现数值交换"></a>一行代码实现数值交换</h1><pre><code class="hljs python">a = <span class="hljs-number">1</span>b = <span class="hljs-number">2</span>a,b = b,aprint(a,b)</code></pre><h1 id="一行代码求奇偶数"><a href="#一行代码求奇偶数" class="headerlink" title="一行代码求奇偶数"></a>一行代码求奇偶数</h1><pre><code class="hljs python">[x <span class="hljs-keyword">for</span>  x <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>) <span class="hljs-keyword">if</span> x % <span class="hljs-number">2</span> == <span class="hljs-number">1</span>]</code></pre><h1 id="一行代码展开列表"><a href="#一行代码展开列表" class="headerlink" title="一行代码展开列表"></a>一行代码展开列表</h1><pre><code class="hljs python">list = [[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>]][j <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> list <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> i]</code></pre><h1 id="一行代码打乱列表"><a href="#一行代码打乱列表" class="headerlink" title="一行代码打乱列表"></a>一行代码打乱列表</h1><pre><code class="hljs python"><span class="hljs-keyword">import</span> randomlst = [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]random.shuffle(lst)lst</code></pre><h1 id="一行代码反转字符串"><a href="#一行代码反转字符串" class="headerlink" title="一行代码反转字符串"></a>一行代码反转字符串</h1><pre><code class="hljs python">name = <span class="hljs-string">'byf4963cg'</span>name = [::<span class="hljs-number">-1</span>]</code></pre><h1 id="一行代码查看目录下所有文件"><a href="#一行代码查看目录下所有文件" class="headerlink" title="一行代码查看目录下所有文件"></a>一行代码查看目录下所有文件</h1><pre><code class="hljs python"><span class="hljs-keyword">import</span> osos.listdir(<span class="hljs-string">'.'</span>)</code></pre><h1 id="一行代码去除字符串间的空格"><a href="#一行代码去除字符串间的空格" class="headerlink" title="一行代码去除字符串间的空格"></a>一行代码去除字符串间的空格</h1><pre><code class="hljs python">des = <span class="hljs-string">'My name is master123'</span>des.replace(<span class="hljs-string">""</span>,<span class="hljs-string">""</span>)</code></pre><pre><code class="hljs python">des = <span class="hljs-string">'My name is master123'</span><span class="hljs-string">""</span>.join(des.split(<span class="hljs-string">" "</span>))</code></pre><h1 id="一行代码实现字符串整数列表变成整数列表"><a href="#一行代码实现字符串整数列表变成整数列表" class="headerlink" title="一行代码实现字符串整数列表变成整数列表"></a>一行代码实现字符串整数列表变成整数列表</h1><pre><code class="hljs python">a = [<span class="hljs-string">'1'</span>,<span class="hljs-string">'2'</span>,<span class="hljs-string">'3'</span>,<span class="hljs-string">'4'</span>,<span class="hljs-string">'5'</span>]list(map(<span class="hljs-keyword">lambda</span> a : int (a),[<span class="hljs-string">'1'</span>,<span class="hljs-string">'2'</span>,<span class="hljs-string">'3'</span>,<span class="hljs-string">'4'</span>,<span class="hljs-string">'5'</span>]))</code></pre><h1 id="一行代码删除列表中重复的值"><a href="#一行代码删除列表中重复的值" class="headerlink" title="一行代码删除列表中重复的值"></a>一行代码删除列表中重复的值</h1><pre><code class="hljs python">lst = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>]list(set(lst))</code></pre><h1 id="一行代码找出两个列表中相同的元素"><a href="#一行代码找出两个列表中相同的元素" class="headerlink" title="一行代码找出两个列表中相同的元素"></a>一行代码找出两个列表中相同的元素</h1><pre><code class="hljs python">a = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>]b = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]set(a) &amp; set(b)</code></pre><h1 id="一行代码找出两个列表中不同的元素"><a href="#一行代码找出两个列表中不同的元素" class="headerlink" title="一行代码找出两个列表中不同的元素"></a>一行代码找出两个列表中不同的元素</h1><pre><code class="hljs python">a = [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">4</span>]b = [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]set(a) ^ set(b)</code></pre><h1 id="一行代码合并两个字典"><a href="#一行代码合并两个字典" class="headerlink" title="一行代码合并两个字典"></a>一行代码合并两个字典</h1><pre><code class="hljs python">des = &#123;<span class="hljs-string">'name'</span>: <span class="hljs-string">'john'</span>&#125;age = &#123;<span class="hljs-string">'age'</span>: <span class="hljs-string">'25'</span>&#125;des.update(age)des</code></pre><h1 id="一行代码实现字典键从小到大排序"><a href="#一行代码实现字典键从小到大排序" class="headerlink" title="一行代码实现字典键从小到大排序"></a>一行代码实现字典键从小到大排序</h1><pre><code class="hljs python">des = &#123;<span class="hljs-string">'name'</span>: <span class="hljs-string">'john'</span>,<span class="hljs-string">'age'</span>: <span class="hljs-string">'25'</span>,<span class="hljs-string">'like'</span>: <span class="hljs-string">'python'</span>&#125;sorted(des.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx 配置参数中文说明 </title>
    <link href="/2019/12/12/2019-12-12-nginx-chinese/"/>
    <url>/2019/12/12/2019-12-12-nginx-chinese/</url>
    
    <content type="html"><![CDATA[<h1 id="Nginx-配置参数中文详细说明："><a href="#Nginx-配置参数中文详细说明：" class="headerlink" title="Nginx 配置参数中文详细说明："></a>Nginx 配置参数中文详细说明：</h1><pre><code class="hljs bash"><span class="hljs-comment">#定义Nginx运行的用户和用户组</span>user www www;<span class="hljs-comment">#</span><span class="hljs-comment">#nginx进程数,建议设置为等于CPU总核心数.</span>worker_processes 8;<span class="hljs-comment">#</span><span class="hljs-comment">#全局错误日志定义类型,[ debug | info | notice | warn | error | crit ]</span>error_log /var/<span class="hljs-built_in">log</span>/nginx/error.log info;<span class="hljs-comment">#</span><span class="hljs-comment">#进程文件</span>pid /var/run/nginx.pid;<span class="hljs-comment">#</span><span class="hljs-comment">#一个nginx进程打开的最多文件描述符数目,理论值应该是最多打开文件数（系统的值ulimit -n）与nginx进程数相除,但是nginx分配请求并不均匀,所以建议与ulimit -n的值保持一致.</span>worker_rlimit_nofile 65535;<span class="hljs-comment">#</span><span class="hljs-comment">#工作模式与连接数上限</span>events&#123;    <span class="hljs-comment">#参考事件模型,use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型,如果跑在FreeBSD上面,就用kqueue模型.</span>    use epoll;    <span class="hljs-comment">#单个进程最大连接数（最大连接数=连接数*进程数）</span>    worker_connections 65535;&#125;<span class="hljs-comment">#</span><span class="hljs-comment">#设定http服务器</span>http&#123;    include mime.types; <span class="hljs-comment">#文件扩展名与文件类型映射表</span>    default_type application/octet-stream; <span class="hljs-comment">#默认文件类型</span>    <span class="hljs-comment">#charset utf-8; #默认编码</span>    server_names_hash_bucket_size 128; <span class="hljs-comment">#服务器名字的hash表大小</span>    client_header_buffer_size 32k; <span class="hljs-comment">#上传文件大小限制</span>    large_client_header_buffers 4 64k; <span class="hljs-comment">#设定请求缓</span>    client_max_body_size 8m; <span class="hljs-comment">#设定请求缓</span>        <span class="hljs-comment"># 开启目录列表访问,合适下载服务器,默认关闭.</span>    autoindex on; <span class="hljs-comment"># 显示目录</span>    autoindex_exact_size on; <span class="hljs-comment"># 显示文件大小 默认为on,显示出文件的确切大小,单位是bytes 改为off后,显示出文件的大概大小,单位是kB或者MB或者GB</span>    autoindex_localtime on; <span class="hljs-comment"># 显示文件时间 默认为off,显示的文件时间为GMT时间 改为on后,显示的文件时间为文件的服务器时间</span>        sendfile on; <span class="hljs-comment"># 开启高效文件传输模式,sendfile指令指定nginx是否调用sendfile函数来输出文件,对于普通应用设为 on,如果用来进行下载等应用磁盘IO重负载应用,可设置为off,以平衡磁盘与网络I/O处理速度,降低系统的负载.注意：如果图片显示不正常把这个改成off.</span>    tcp_nopush on; <span class="hljs-comment"># 防止网络阻塞</span>    tcp_nodelay on; <span class="hljs-comment"># 防止网络阻塞</span>        keepalive_timeout 120; <span class="hljs-comment"># (单位s)设置客户端连接保持活动的超时时间,在超过这个时间后服务器会关闭该链接</span>        <span class="hljs-comment"># FastCGI相关参数是为了改善网站的性能：减少资源占用,提高访问速度.下面参数看字面意思都能理解.</span>    fastcgi_connect_timeout 300;    fastcgi_send_timeout 300;    fastcgi_read_timeout 300;    fastcgi_buffer_size 64k;    fastcgi_buffers 4 64k;    fastcgi_busy_buffers_size 128k;    fastcgi_temp_file_write_size 128k;        <span class="hljs-comment"># gzip模块设置</span>    gzip on; <span class="hljs-comment">#开启gzip压缩输出</span>    gzip_min_length 1k; <span class="hljs-comment">#允许压缩的页面的最小字节数,页面字节数从header偷得content-length中获取.默认是0,不管页面多大都进行压缩.建议设置成大于1k的字节数,小于1k可能会越压越大</span>    gzip_buffers 4 16k; <span class="hljs-comment">#表示申请4个单位为16k的内存作为压缩结果流缓存,默认值是申请与原始数据大小相同的内存空间来存储gzip压缩结果</span>    gzip_http_version 1.1; <span class="hljs-comment">#压缩版本（默认1.1,目前大部分浏览器已经支持gzip解压.前端如果是squid2.5请使用1.0）</span>    gzip_comp_level 2; <span class="hljs-comment">#压缩等级.1压缩比最小,处理速度快.9压缩比最大,比较消耗cpu资源,处理速度最慢,但是因为压缩比最大,所以包最小,传输速度快</span>    gzip_types text/plain application/x-javascript text/css application/xml;    <span class="hljs-comment">#压缩类型,默认就已经包含text/html,所以下面就不用再写了,写上去也不会有问题,但是会有一个warn.</span>    gzip_vary on;<span class="hljs-comment">#选项可以让前端的缓存服务器缓存经过gzip压缩的页面.例如:用squid缓存经过nginx压缩的数据</span>        <span class="hljs-comment">#开启限制IP连接数的时候需要使用</span>    <span class="hljs-comment">#limit_zone crawler $binary_remote_addr 10m;</span>        <span class="hljs-comment">##upstream的负载均衡,四种调度算法(下例主讲)##</span>        <span class="hljs-comment">#虚拟主机的配置</span>    server    &#123;        <span class="hljs-comment"># 监听端口</span>        listen 80;        <span class="hljs-comment"># 域名可以有多个,用空格隔开</span>        server_name ably.com;        <span class="hljs-comment"># HTTP 自动跳转 HTTPS</span>        rewrite ^(.*) https://<span class="hljs-variable">$server_name</span><span class="hljs-variable">$1</span> permanent;    &#125;        server    &#123;        <span class="hljs-comment"># 监听端口 HTTPS</span>        listen 443 ssl;        server_name ably.com;                <span class="hljs-comment"># 配置域名证书</span>        ssl_certificate      C:\WebServer\Certs\certificate.crt;        ssl_certificate_key  C:\WebServer\Certs\private.key;        ssl_session_cache    shared:SSL:1m;        ssl_session_timeout  5m;        ssl_protocols SSLv2 SSLv3 TLSv1;        ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP;        ssl_prefer_server_ciphers  on;            index index.html index.htm index.php;        root /data/www/;        location ~ .*\.(php|php5)?$        &#123;            fastcgi_pass 127.0.0.1:9000;            fastcgi_index index.php;            include fastcgi.conf;        &#125;                <span class="hljs-comment"># 配置地址拦截转发，解决跨域验证问题</span>        location /oauth/&#123;            proxy_pass https://localhost:13580/oauth/;            proxy_set_header HOST <span class="hljs-variable">$host</span>;            proxy_set_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;            proxy_set_header X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;        &#125;                <span class="hljs-comment"># 图片缓存时间设置</span>        location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$ &#123;            expires 10d;        &#125;                <span class="hljs-comment"># JS和CSS缓存时间设置</span>        location ~ .*\.(js|css)?$ &#123;            expires 1h;        &#125;        <span class="hljs-comment"># 日志格式设定</span>        log_format access <span class="hljs-string">'$remote_addr - $remote_user [$time_local] "$request" '</span>        <span class="hljs-string">'$status $body_bytes_sent "$http_referer" '</span>        <span class="hljs-string">'"$http_user_agent" $http_x_forwarded_for'</span>;        <span class="hljs-comment"># 定义本虚拟主机的访问日志</span>        access_log /var/<span class="hljs-built_in">log</span>/nginx/access.log access;                <span class="hljs-comment"># 设定查看Nginx状态的地址.StubStatus模块能够获取Nginx自上次启动以来的工作状态，此模块非核心模块，需要在Nginx编译安装时手工指定才能使用</span>        location /NginxStatus &#123;            stub_status on;            access_log on;            auth_basic <span class="hljs-string">"NginxStatus"</span>;            auth_basic_user_file conf/htpasswd;            <span class="hljs-comment">#htpasswd文件的内容可以用apache提供的htpasswd工具来产生.</span>        &#125;    &#125;&#125;</code></pre><h1 id="Nginx负载均衡服务器的nginx-conf配置注释"><a href="#Nginx负载均衡服务器的nginx-conf配置注释" class="headerlink" title="Nginx负载均衡服务器的nginx.conf配置注释"></a>Nginx负载均衡服务器的<code>nginx.conf</code>配置注释</h1><p><strong>如下：</strong></p><pre><code class="hljs bash">events&#123;    use epoll;    worker_connections 65535;&#125;http&#123;    <span class="hljs-comment">##upstream的负载均衡,四种调度算法##</span>    <span class="hljs-comment">#调度算法1:轮询.每个请求按时间顺序逐一分配到不同的后端服务器,如果后端某台服务器宕机,故障系统被自动剔除,使用户访问不受影响</span>    upstream webhost &#123;        server 192.168.0.5:6666 ;        server 192.168.0.7:6666 ;    &#125;    <span class="hljs-comment">#调度算法2:weight(权重).可以根据机器配置定义权重.权重越高被分配到的几率越大</span>    upstream webhost &#123;        server 192.168.0.5:6666 weight=2;        server 192.168.0.7:6666 weight=3;    &#125;    <span class="hljs-comment">#调度算法3:ip_hash. 每个请求按访问IP的hash结果分配,这样来自同一个IP的访客固定访问一个后端服务器,有效解决了动态网页存在的session共享问题</span>    upstream webhost &#123;        ip_hash;        server 192.168.0.5:6666 ;        server 192.168.0.7:6666 ;    &#125;    <span class="hljs-comment">#调度算法4:url_hash(需安装第三方插件).此方法按访问url的hash结果来分配请求,使每个url定向到同一个后端服务器,可以进一步提高后端缓存服务器的效率.Nginx本身是不支持url_hash的,如果需要使用这种调度算法,必须安装Nginx 的hash软件包</span>    upstream webhost &#123;        server 192.168.0.5:6666 ;        server 192.168.0.7:6666 ;        <span class="hljs-built_in">hash</span> <span class="hljs-variable">$request_uri</span>;    &#125;    <span class="hljs-comment">#调度算法5:fair(需安装第三方插件).这是比上面两个更加智能的负载均衡算法.此种算法可以依据页面大小和加载时间长短智能地进行负载均衡,也就是根据后端服务器的响应时间来分配请求,响应时间短的优先分配.Nginx本身是不支持fair的,如果需要使用这种调度算法,必须下载Nginx的upstream_fair模块</span>    <span class="hljs-comment">#</span>    <span class="hljs-comment">#虚拟主机的配置(采用调度算法3:ip_hash)</span>    server    &#123;        listen 80;        server_name mongo.demo.com;        <span class="hljs-comment">#对 "/" 启用反向代理</span>        location / &#123;            proxy_pass http://webhost;            proxy_redirect off;            proxy_set_header X-Real-IP <span class="hljs-variable">$remote_addr</span>;            <span class="hljs-comment">#后端的Web服务器可以通过X-Forwarded-For获取用户真实IP</span>            proxy_set_header X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;            <span class="hljs-comment">#以下是一些反向代理的配置,可选.</span>            proxy_set_header Host <span class="hljs-variable">$host</span>;            client_max_body_size 10m; <span class="hljs-comment">#允许客户端请求的最大单文件字节数</span>            client_body_buffer_size 128k; <span class="hljs-comment">#缓冲区代理缓冲用户端请求的最大字节数,</span>            proxy_connect_timeout 90; <span class="hljs-comment">#nginx跟后端服务器连接超时时间(代理连接超时)</span>            proxy_send_timeout 90; <span class="hljs-comment">#后端服务器数据回传时间(代理发送超时)</span>            proxy_read_timeout 90; <span class="hljs-comment">#连接成功后,后端服务器响应时间(代理接收超时)</span>            proxy_buffer_size 4k; <span class="hljs-comment">#设置代理服务器（nginx）保存用户头信息的缓冲区大小</span>            proxy_buffers 4 32k; <span class="hljs-comment">#proxy_buffers缓冲区,网页平均在32k以下的设置</span>            proxy_busy_buffers_size 64k; <span class="hljs-comment">#高负荷下缓冲大小（proxy_buffers*2）</span>            proxy_temp_file_write_size 64k;            <span class="hljs-comment">#设定缓存文件夹大小,大于这个值,将从upstream服务器传</span>        &#125;    &#125;&#125;</code></pre><p>参考链接：<a href="https://mp.weixin.qq.com/s/T0uXiWtvE9tYijVlEsCKLw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/T0uXiWtvE9tYijVlEsCKLw</a></p><p>​                  <a href="http://www.cnblogs.com/xcloudbiz/articles/5234373.html" target="_blank" rel="noopener">http://www.cnblogs.com/xcloudbiz/articles/5234373.html</a></p><p>​                  <a href="http://wangying.sinaapp.com/archives/931" target="_blank" rel="noopener">http://wangying.sinaapp.com/archives/931</a></p><p>​                  <a href="http://www.php100.com/html/program/nginx/2013/0905/5525.html" target="_blank" rel="noopener">http://www.php100.com/html/program/nginx/2013/0905/5525.html</a></p><p>​                  <a href="https://hub.docker.com/_/nginx/" target="_blank" rel="noopener">https://hub.docker.com/_/nginx/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第9篇 网络原理解析</title>
    <link href="/2019/12/11/2019-12-11-kubernetes9-network/"/>
    <url>/2019/12/11/2019-12-11-kubernetes9-network/</url>
    
    <content type="html"><![CDATA[<h1 id="1、Linux网络基础"><a href="#1、Linux网络基础" class="headerlink" title="1、Linux网络基础"></a>1、Linux网络基础</h1><p><strong>Network Namespace(网络命名空间)：</strong></p><p>Linux在网络栈中引入网络命名空间，将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信；docker利用这一特性，实现不容器间的网络隔离。</p><p><strong>Iptables/Netfilter：</strong></p><p>Netfilter负责在内核中执行各种挂接的规则(过滤、修改、丢弃等)，运行在内核模式中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过分析进入主机的网络封包，将封包的表头数据提取出来进行分析，以决定该联机为放行或抵挡的机制。由于这种方式可以直接分析封包表头数据，所以包括硬件地址(MAC), 软件地址 (IP), TCP, UDP, ICMP 等封包的信息都可以进行过滤分析。</p><p><strong>Veth设备对：</strong></p><p>Veth设备对的引入是为了实现在不同网络命名空间的通信。</p><p><strong>Bridge(网桥)：</strong></p><p>网桥是一个二层网络设备，是最简单的CNI网络插件，它首先在Host创建一个网桥，然后再通过veth pair连接该网桥到container netns。另外，Bridge模式下，多主机网络通信需要额外配置主机路由。</p><p><strong>Route(路由)：</strong></p><p>Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候会使用路由表来决定发往哪里。</p><p><strong>Container Network Interface (CNI) ：</strong></p><p>最早是由CoreOS发起的容器网络规范，是 Kubernetes网络插件的基础。其基本思想为:Container Runtime在创建容器时，先创建好network namespace，然后调用CNI插件为这个netns配置网络，其后再启动容器内的进程。</p><h1 id="2、K8S网络模型"><a href="#2、K8S网络模型" class="headerlink" title="2、K8S网络模型"></a>2、K8S网络模型</h1><p>Kubernetes网络有一个重要的基本设计原则：<strong>每个Pod拥有唯一的IP</strong>。</p><p>这个Pod IP被该Pod内的所有容器共享，并且其它所有Pod都可以路由到该Pod。你可曾注意到，你的Kubernetes节点上运行着一些”pause”容器？它们被称作“沙盒容器（sandbox containers）”，其唯一任务是保留并持有一个网络命名空间（netns），该命名空间被Pod内所有容器共享。通过这种方式，即使一个容器死掉，新的容器创建出来代替这个容器，Pod IP也不会改变。这种IP-per-pod模型的巨大优势是，Pod和底层主机不会有IP或者端口冲突。我们不用担心应用使用了什么端口。</p><p>这点满足后，Kubernetes唯一的要求是，这些Pod IP可被其它所有Pod访问，不管那些Pod在哪个节点。</p><h2 id="2-1-节点内通信"><a href="#2-1-节点内通信" class="headerlink" title="2.1 节点内通信"></a>2.1 节点内通信</h2><p>第一步是确保同一节点上的Pod可以相互通信，然后可以扩展到跨节点通信、internet上的通信，等等。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/2-2-1-a.png" srcset="/img/loading.gif" alt="avatar"></p><p>在每个Kubernetes节点（本场景指的是Linux机器）上，都有一个根（root）命名空间（root是作为基准，而不是超级用户）– root netns(root network namespace)。</p><p>主要的网络接口 eth0 就是在这个root netns下。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/2-2-1-b.png" srcset="/img/loading.gif" alt="avatar"></p><p>类似的，每个Pod都有其自身的netns(network namespace)，通过一个虚拟的以太网对连接到root netns。这基本上就是一个管道对，一端在root netns内，另一端在Pod的netns内。</p><p>我们把Pod端的网络接口叫 eth0，这样Pod就不需要知道底层主机，它认为它拥有自己的根网络设备。另一端命名成比如 vethxxx。你可以用 ifconfig 或者 ip a 命令列出你的节点上的所有这些接口。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/2-2-1-c.png" srcset="/img/loading.gif" alt="avatar"></p><p>节点上的所有Pod都会完成这个过程。这些Pod要相互通信，就要用到linux的以太网桥 cbr0 了。Docker使用了类似的网桥，称为docker0。你可以用 brctl show 命令列出所有网桥。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/2-2-1-d.gif" srcset="/img/loading.gif" alt="avatar"></p><p>假设一个网络数据包要由pod1到pod2</p><p>1) 它由pod1中netns的eth0网口离开，通过vethxxx进入root netns。</p><p>2) 然后被传到cbr0，cbr0使用ARP请求，说“谁拥有这个IP”，从而发现目标地址。</p><p>3）vethyyy说它有这个IP，因此网桥就知道了往哪里转发这个包。</p><p>4）数据包到达vethyyy，跨过管道对，到达pod2的netns。</p><p>这就是同一节点内容器间通信的流程。当然也可以用其它方式，但是无疑这是最简单的方式，同时也是Docker采用的方式。</p><h2 id="2-2-不同节点间通信"><a href="#2-2-不同节点间通信" class="headerlink" title="2.2 不同节点间通信"></a>2.2 不同节点间通信</h2><p>正如我前面提到，Pod也需要跨节点可达。Kubernetes不关心如何实现。我们可以使用L2（ARP跨节点），L3（IP路由跨节点，就像云提供商的路由表），Overlay网络，或者甚至信鸽。无所谓，只要流量能到达另一个节点的期望Pod就好。每个节点都为Pod IPs分配了唯一的CIDR块（一段IP地址范围），因此每个Pod都拥有唯一的IP，不会和其它节点上的Pod冲突。</p><p>大多数情况下，特别是在云环境上，云提供商的路由表就能确保数据包到达正确的目的地。我们在每个节点上建立正确的路由也能达到同样的目的。许多其它的网络插件通过自己的方式达到这个目的。</p><p>这里我们有两个节点，与之前看到的类似。每个节点有不同的网络命名空间、网络接口以及网桥。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/2-2-2-a.gif" srcset="/img/loading.gif" alt="avatar"></p><p>假设一个数据包要从pod1到达pod4（在不同的节点上）</p><p>1）它由pod1中netns的eth0网口离开，通过vethxxx进入root netns。</p><p>2）然后被传到cbr0，cbr0通过发送ARP请求来找到目标地址。</p><p>3）本节点上没有Pod拥有pod4的IP地址，根据路由判断数据包由cbr0 传到主网络接口 eth0。</p><p>4）数据包的源地址为pod1，目标地址为pod4，它以这种方式离开node1进入电缆。</p><p>5）路由表有每个节点的CIDR块的路由设定，它把数据包路由到CIDR块包含pod4的IP的节点。</p><p>6）因此数据包到达了node2的主网络接口eth0。现在即使pod4不是eth0的IP，数据包也仍然能转发到cbr0，因为节点配置了IP forwarding enabled。节点的路由表寻找任意能匹配pod4 IP的路由。它发现了 cbr0 是这个节点的CIDR块的目标地址。你可以用route -n命令列出该节点的路由表，它会显示cbr0的路由，类型如下：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/2-2-2-b.png" srcset="/img/loading.gif" alt="avatar"></p><p>7）网桥接收了数据包，发送ARP请求，发现目标IP属于vethyyy。</p><p>8）数据包跨过管道对到达pod4。</p><h1 id="3、覆盖网络"><a href="#3、覆盖网络" class="headerlink" title="3、覆盖网络"></a>3、覆盖网络</h1><p>覆盖⽹络(overlay network)是将TCP数据包装在另⼀种⽹络包⾥⾯进⾏路由转发和通信的技术。Overlay⽹络不是默认必须的，但是它们在特定场景下⾮常有⽤。⽐如当我们没有⾜够的IP空间，或者⽹络⽆法处理额外路由，抑或当我们需要Overlay提供的某些额外管理特性。⼀个常⻅的场景是当云提供商的路由表能处理的路由数是有限制时，例如AWS路由表最多⽀持50条路由才不⾄于影响⽹络性能。因此如果我们有超过50个Kubernetes节点， AWS路由表将不够。这种情况下，使⽤Overlay⽹络将帮到我们。</p><p>本质上来说， Overlay就是在跨节点的本地⽹络上的包中再封装⼀层包。你可能不想使⽤Overlay⽹络，因为它会带来由封装和解封所有报⽂引起的时延和复杂度开销。通常这是不必要的，因此我们应当在知道为什么我们需要它时才使⽤它。</p><p>为了理解Overlay⽹络中流量的流向，我们拿Flannel做例⼦，它是CoreOS 的⼀个开源项⽬。Flannel通过给每台宿主机分配⼀个⼦⽹的⽅式为容器提供虚拟⽹络，它基于Linux TUN/TAP，使⽤UDP封装IP包来创建overlay⽹络，并借助etcd维护⽹络的分配情况。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/3-1.gif" srcset="/img/loading.gif" alt="avatar"></p><p>这⾥我们注意到它和之前我们看到的设施是⼀样的，只是在root netns中新增了⼀个虚拟的以太⽹设备，称为flannel0。它是虚拟扩展⽹络Virtual Extensible LAN（VXLAN）的⼀种实现，但是在Linux上，它只是另⼀个⽹络接⼝。</p><p>从pod1到pod4（在不同节点）的数据包的流向类似如下：</p><p>1）它由pod1中netns的eth0⽹⼝离开，通过vethxxx进⼊root netns。<br>2）然后被传到cbr0， cbr0通过发送ARP请求来找到⽬标地址。<br>3）数据封装</p><p>  3a. 由于本节点上没有Pod拥有pod4的IP地址，因此⽹桥把数据包发送给了flannel0，因为节点的路由表上flannel0被配成了Pod⽹段的⽬标地址。</p><p>  3b. flanneld daemon和Kubernetes apiserver或者底层的etcd通信，它知道所有的Pod IP，并且知道它们在哪个节点上。因此Flannel创建了Pod IP和Node IP之间的映射（在⽤户空间）。flannel0取到这个包，并在其上再⽤⼀个UDP包封装起来，该UDP包头部的源和⽬的IP分别被改成了对应节点的IP，然后发送这个新包到特定的VXLAN端⼝（通常是8472）。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/3-2.png" srcset="/img/loading.gif" alt="avatar"></p><p>尽管这个映射发⽣在⽤户空间，真实的封装以及数据的流动发⽣在内核空间，因此仍然是很快的。  </p><p>  3c. 封装后的包通过eth0发送出去，因为它涉及了节点间的路由流量。</p><p>\4. 包带着节点IP信息作为源和⽬的地址离开本节点。<br>\5. 云提供商的路由表已经知道了如何在节点间发送报⽂，因此该报⽂被发送到⽬标地址node2。<br>\6. 数据解包</p><p>  6a. 包到达node2的eth0⽹卡，由于⽬标端⼝是特定的VXLAN端⼝，内核将报⽂发送给了<br>flannel0。<br>  6b. flannel0解封报⽂，并将其发送到 root 命名空间下。从这⾥开始，报⽂的路径和我们之前在Part1 中看到的⾮Overlay⽹络就是⼀致的了。<br>  6c. 由于IP forwarding开启着，内核按照路由表将报⽂转发给了cbr0。</p><p>\7. ⽹桥获取到了包，发送ARP请求，发现⽬标IP属于vethyyy。<br>\8. 包跨过管道对到达pod4</p><p>这就是Kubernetes中Overlay⽹络的⼯作⽅式，虽然不同的实现还是会有细微的差别。<strong>有个常⻅的误区是，当我们使⽤Kubernetes，我们就不得不使⽤Overlay⽹络。事实是，这完全依赖于特定场景。因此请确保在确实需要的场景下才使⽤</strong>。</p><h1 id="4、动态集群"><a href="#4、动态集群" class="headerlink" title="4、动态集群"></a>4、动态集群</h1><p>由于Kubernetes（更通⽤的说法是分布式系统）天⽣具有不断变化的特性，因此它的Pod（以及Pod的IP）总是在改变。变化的原因可以是针对不可预测的Pod或节点崩溃⽽进⾏的滚动升级和扩展。这使得Pod IP不能直接⽤于通信。</p><p>我们看⼀下Kubernetes Service，它是⼀个虚拟IP，并伴随着⼀组Pod IP作为Endpoint（通过标签选择器识别）。它们充当虚拟负载均衡器，其IP保持不变，⽽后端Pod IP可能会不断变化。</p><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">V1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">svc2</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">ClusterIP</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">myapp</span>  <span class="hljs-attr">ClusterIP:</span> <span class="hljs-number">10.200</span><span class="hljs-number">.100</span><span class="hljs-number">.214</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span></code></pre><p>整个虚拟IP的实现实际上是⼀组iptables（最新版本可以选择使⽤IPVS，但这是另⼀个讨论）规则，由Kubernetes组件kube-proxy管理。这个名字现在实际上是误导性的。它在v 1.0之前确实是⼀个代理，并且由于其实现是内核空间和⽤户空间之间的不断复制，它变得⾮常耗费资源并且速度较慢。现在，它只是⼀个控制器，就像Kubernetes中的许多其它控制器⼀样，它watch api serverendpoint的更改并相应地更新iptables规则。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/4-0-a.png" srcset="/img/loading.gif" alt="avatar"></p><p>有了这些iptables规则，每当数据包发往Service IP时，它就进⾏DNAT（DNAT=⽬标⽹络地址转换）操作，这意味着⽬标IP从Service IP更改为其中⼀个Endpoint - Pod IP - 由iptables随机选择。这可确保负载均匀分布在后端Pod中。</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-11/4-0-b.png" srcset="/img/loading.gif" alt="avatar"></p><p>当这个DNAT发⽣时，这个信息存储在conntrack中——Linux连接跟踪表（iptables规则5元组转译并完成存储：protocol， srcIP， srcPort， dstIP， dstPort）。这样当请求回来时，它可以un-DNAT，这意味着将源IP从Pod IP更改为Service IP。这样，客户端就不⽤关⼼后台如何处理数据包流。</p><p>因此通过使⽤Kubernetes Service，我们可以使⽤相同的端⼝⽽不会发⽣任何冲突（因为我们可以将端⼝重新映射到Endpoint）。这使服务发现变得⾮常容易。我们可以使⽤内部DNS并对服务主机名进⾏硬编码。</p><p>我们甚⾄可以使⽤Kubernetes提供的service主机和端⼝的环境变量来完成服务发现。</p><p><strong>专家建议：</strong>采取第⼆种⽅法，你可节省不必要的DNS调⽤，但是由于环境变量存在创建顺序的局限性（环境变量中不包含后来创建的服务），推荐使⽤DNS来进⾏服务名解析。</p><h2 id="4-1-出站流量"><a href="#4-1-出站流量" class="headerlink" title="4.1 出站流量"></a>4.1 出站流量</h2><p>到⽬前为⽌我们讨论的Kubernetes Service是在⼀个集群内⼯作。但是，在⼤多数实际情况中，应⽤程序需要访问⼀些外部api/website。</p><p>通常，节点可以同时具有私有IP和公共IP。对于互联⽹访问，这些公共和私有IP存在某种1：1的NAT，特别是在云环境中。</p><p>对于从节点到某些外部IP的普通通信，源IP从节点的专⽤IP更改为其出站数据包的公共IP，⼊站的响应数据包则刚好相反。但是，当Pod发出与外部IP的连接时，源IP是Pod IP，云提供商的NAT机制不知道该IP。因此它将丢弃具有除节点IP之外的源IP的数据包。</p><p>因此你可能也猜对了，我们将使⽤更多的iptables！这些规则也由kube-proxy添加，执⾏SNAT（源⽹络地址转换），即IP MASQUERADE（IP伪装）。它告诉内核使⽤此数据包发出的⽹络接⼝的IP，代替源Pod IP同时保留conntrack条⽬以进⾏反SNAT操作。</p><h2 id="4-2-入站流量"><a href="#4-2-入站流量" class="headerlink" title="4.2 入站流量"></a>4.2 入站流量</h2><p>到⽬前为⽌⼀切都很好。Pod可以互相交谈，也可以访问互联⽹。但我们仍然缺少关键部分 - 为⽤户请求流量提供服务。截⾄⽬前，有两种主要⽅法可以做到这⼀点：</p><ul><li><strong>NodePort /云负载均衡器（L4 - IP和端⼝）</strong></li></ul><p>将服务类型设置为NodePort默认会为服务分配范围为30000-33000d的nodePort。即使在特定节点上没有运⾏Pod，此nodePort也会在每个节点上打开。此NodePort上的⼊站流量将再次使⽤iptables发送到其中⼀个Pod（该Pod甚⾄可能在其它节点上！）。</p><p>云环境中的LoadBalancer服务类型将在所有节点之前创建云负载均衡器（例如ELB），命中相同的nodePort。</p><ul><li><strong>Ingress（L7 - HTTP / TCP）</strong></li></ul><p>许多不同的⼯具，如Nginx， Traefik， HAProxy等，保留了http主机名/路径和各⾃后端的映射。通常这是基于负载均衡器和nodePort的流量⼊⼝点，其优点是我们可以有⼀个⼊⼝处理所有服务的⼊站流量，⽽不需要多个nodePort和负载均衡器。</p><h2 id="4-3-网站策略"><a href="#4-3-网站策略" class="headerlink" title="4.3 网站策略"></a>4.3 网站策略</h2><p>可以把它想象为Pod的安全组/ ACL。NetworkPolicy规则允许/拒绝跨Pod的流量。确切的实现取决于⽹络层/CNI，但⼤多数只使⽤iptables。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第8篇 CI/CD 之全流程实践</title>
    <link href="/2019/12/10/2019-12-10-kubernetes8-cicd-full/"/>
    <url>/2019/12/10/2019-12-10-kubernetes8-cicd-full/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>1）本实践中已经的示例代码及jenkins-agent镜像已经推送归档至github，–&gt;传送门（<a href="https://github.com/Kubernetes-Best-Pratice/）" target="_blank" rel="noopener">https://github.com/Kubernetes-Best-Pratice/）</a></p><p>2）注意本实践中均为内网数据，你测试时一定要改为自己的环境的有效数据。</p><p>3) 由于本实践涉及组件较多，若有操作不明确的话，你可以后台留言，我们一起完善。</p><p>4) 具体操作时若有不清楚，或是错误可以留言，大家一起解决。</p><h1 id="1、准备基础数据"><a href="#1、准备基础数据" class="headerlink" title="1、准备基础数据"></a>1、准备基础数据</h1><p><strong>step1：</strong>配置Gitlab</p><ul><li>创建项目</li><li>上传示例代码</li></ul><p><em>注: 本次示例使用的gitlab项目地址：</em></p><p><em><a href="http://gitlab.hanker.com/colynn/hanker-hello.git" target="_blank" rel="noopener">http://gitlab.hanker.com/colynn/hanker-hello.git</a></em></p><p><strong>step2：</strong>配置Harbor</p><p>创建项目, 用于存储构建的镜像</p><p><em>注: 本次示例使用的harbor地址为</em> </p><p><em>10.0.0.185:5000/hanker/hanker-hello:v1</em></p><p><strong>step3：</strong>Jenkins验证信息</p><ul><li><p>添加 gitlab 帐号信息</p><p><u>操作指引：【Credentials】-&gt; 【System】-&gt; 【Global credentials】-&gt; 【Add Credentials】</u></p></li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/1-3-a.png" srcset="/img/loading.gif" alt="avatar"></p><ul><li><p>harbor 信息</p><p><u>操作指引：【Credentials】-&gt; 【System】-&gt; 【Global credentials】-&gt; 【Add Credentials】</u></p></li></ul><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/1-3-b.png" srcset="/img/loading.gif" alt="avatar"></p><ul><li><p>k8s namespace验证信息</p><p>在你的k8s master节点上执行如下操作：</p><p>1) 创建serviceaccount</p></li></ul><pre><code class="hljs bash">$ kubectl -n devops create serviceaccount jenkins-robot</code></pre><p>命令输出：</p><pre><code class="hljs bash">serviceaccount/jenkins-robot created</code></pre><p>  2) 角色绑定</p><pre><code class="hljs bash">$ kubectl -n devops create rolebinding jenkins-robot-binding --clusterrole=cluster-admin --serviceaccount=devops:jenkins-robot</code></pre><p>命令输出：</p><pre><code class="hljs bash">rolebinding.rbac.authorization.k8s.io/jenkins-robot-binding created</code></pre><p>3) 获取 ServiceAccount</p><pre><code class="hljs routeros">$ kubectl -n devops <span class="hljs-builtin-name">get</span> serviceaccount jenkins-robot -o go-template <span class="hljs-attribute">--template</span>=<span class="hljs-string">'&#123;&#123;range .secrets&#125;&#125;&#123;&#123;.name&#125;&#125;&#123;&#123;"\n"&#125;&#125;&#123;&#123;end&#125;&#125;'</span>jenkins-robot-token-n8w6b</code></pre><p> 4) 基于base64解码 ServiceToken</p><pre><code class="hljs jboss-cli">$ kubectl -n devops get secrets jenkins-robot-token-n8w6b -o go-template <span class="hljs-params">--template</span> '&#123;&#123;index <span class="hljs-string">.data</span> <span class="hljs-string">"token"</span>&#125;&#125;' | base64 <span class="hljs-params">--decode</span></code></pre><p>命令输出：</p><pre><code class="hljs smali">eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZXZvcHMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiamVua2lucy1yb2JvdC10b2tlbi1uOHc2YiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJqZW5raW5zLXJvYm90Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTcyZTY0OGYtMTYxZC00NmM5LWI0ZjgtYjFkNTdlOWY4NTBjIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRldm9wczpqZW5raW5zLXJvYm90In0.ArQvcaEqCaeU1ZcJ6nOC5rLaTZr_vLDrpLCt87asltMUWj2gSli_mXUTrl09hBnBDXI3A1D4rJXHKLHjIAA4nN8qRIRGbpqSNzDwmqJr-jmmmWWZFrZ3n3Al9-13KJnNOK8pcWr70rt3Rsigt4B6CIQ0-ZLK8BZhvROJSifeOfJ6xe2KBqdXBv1ccZZZfEhPLgGbaR5yWm5jLvOMr2MQiPDrZoHOEkcMt-C0xipytOp4sJCJ4bQhb-UoMu1owYydxbd6O7xO71fvqP_bMDpZXC601nA2ggK7h-vi6CJffHv5MM59q8X_DWe1NnZS6KXiMmkXqAmBn10Yu20PNj-kjg</code></pre><p> 5) 添加 Secret text验证信息</p><p><u>操作指引：【首页】-&gt;【Credentials】-&gt; 【System】-&gt; 【Global credentials】-&gt; 【Add Credentials】-&gt; 选择【Secret text】类型</u></p><p>然后将上一步 解码的结果 更新至 Secret, Pipeline 中</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/1-5.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="2、如何创建Jenkins-pipline"><a href="#2、如何创建Jenkins-pipline" class="headerlink" title="2、如何创建Jenkins pipline"></a>2、如何创建Jenkins pipline</h1><p><strong>step1：</strong>创建jenkins pipeline item</p><p><u>操作指引：【首页】-&gt;【New Item】</u></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/2-1.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong>step2:</strong> pipeline script 步骤说明</p><p><em>注: pipeline主要包含三个阶段（检出代码、制作镜像、部署服务），下面跟大家解释下，如何编写pipeline， 借助Pipeline Syntax生成的只是部分代码，你可以根据语言规范将其完善。</em></p><p>阶段1: 检出代码</p><p><u>操作指引：【首页】-&gt;【hanker-hello-demo】-&gt; 【Pipeline Syntax】</u></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/2-2-a.png" srcset="/img/loading.gif" alt="avatar"></p><p><em>注: 本实践中选取的 git: Git 类型，当然你也可以选择 checkout: Check out from version control</em></p><p>获取到该步骤的脚本</p><pre><code class="hljs bash">git credentialsId: <span class="hljs-string">'gitlab-project-auth'</span>, url: <span class="hljs-string">'http://gitlab.hanker.com/colynn/hanker-hello.git'</span></code></pre><p>阶段2：构建建镜像操作指引,类似于阶段1</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/2-2-b.png" srcset="/img/loading.gif" alt="avatar"></p><p>完善获取该步骤脚本</p><pre><code class="hljs json">script &#123;    withDockerRegistry(credentialsId: 'harbor-auth', url: 'http://10.0.0.185:5000') &#123;        def customImage =  docker.build("10.0.0.185:5000/devops/hanker-hello:v1")        customImage.push()    &#125;&#125;</code></pre><p><em>注: 支持本阶段需要jenkins-agent镜像里包含docker命令。</em></p><p> 阶段3. 部署服务</p><p>参考: jenkins kubernetes cli plugin</p><p><a href="https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md" target="_blank" rel="noopener">https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md</a></p><p><em>注: 支持本阶段需要jenkins-agent镜像里包含kubectl命令。</em></p><p>*<em>step3: *</em>设置 pipeline</p><p><em>注:General/ Build Triggers/ Advanced Project Options 这三块你可以根据自己需要设置，将各阶段的脚本合并，更新至 Pipline -&gt; Script内。</em></p><p>合并后的pipeline脚本内容如下：</p><pre><code class="hljs json">pipeline &#123;    agent any    stages &#123;        stage('checkout') &#123;            steps &#123;                git credentialsId: 'gitlab-project-auth', url: 'http://gitlab.hanker.com/colynn/hanker-hello.git'                &#125;        &#125;        stage('docker-publish') &#123;            steps&#123;                script &#123;                    withDockerRegistry(credentialsId: 'harbor-auth', url: 'http://10.0.0.185:5000') &#123;                        def customImage =  docker.build("10.0.0.185:5000/devops/hanker-hello:v1")                        customImage.push()                    &#125;                &#125;            &#125;        &#125;        stage('application-deploy') &#123;            steps &#123;                withKubeConfig([credentialsId: '5a5517f3-3d38-459d-bafc-12b55beeb588', serverUrl: 'https://10.0.0.182:6443']) &#123;                    sh '/usr/bin/kubectl apply -f k8s-setup.yml'                &#125;            &#125;        &#125;    &#125;&#125;</code></pre><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/2-3.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="3、触发构建"><a href="#3、触发构建" class="headerlink" title="3、触发构建"></a>3、触发构建</h1><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/3-1.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="4、结果确认"><a href="#4、结果确认" class="headerlink" title="4、结果确认"></a>4、结果确认</h1><p>1) 确认 jenkina-agent 启动状态；</p><pre><code class="hljs bash">$ kubectl -n devops get pods |grep jnlpjnlp-sh8zl                                 1/1     Running   0          14s// 查看jenkins-agent pod日志$ kubectl -n devops logs -f [jenkins-agent-pod-name]</code></pre><p><em>注: 如果长时间没有启动jenkins-agent, 可以确认下集群内是否有足够的资源。</em></p><p>2) 确认pipeline 执行状态；</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/4-2.png" srcset="/img/loading.gif" alt="avatar"></p><p>3) 确认harbor镜像仓库里是否已经有新推送的镜像</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/4-3.png" srcset="/img/loading.gif" alt="avatar"></p><p><em>注: harbor里的项目是需要你先创建好的，不然推送时会报错。</em></p><p>4) 确认部署的服务状态</p><p>k8s master节点上执行如下操作:</p><pre><code class="hljs bash">$ kubectl -n devops get pod,deployment,svc,ingress |grep hanker-hellopod/hanker-hello-5b7586f86d-5j7kk              1/1     Running   0          173mdeployment.extensions/hanker-hello              1/1     1            1           3h8mservice/hanker-hello-svc          ClusterIP   10.233.22.19    &lt;none&gt;        8080/TCP             3h8mingress.extensions/hanker-hello-ingress              hanker-hello-demo.dev.hanker.net                   80      3h8m</code></pre><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-10/4-4.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="5、附录"><a href="#5、附录" class="headerlink" title="5、附录"></a>5、附录</h1><p><strong>1）自定义jenkins-agent镜像</strong></p><pre><code class="hljs bash"><span class="hljs-comment">## 基于 https://github.com/Kubernetes-Best-Pratice/jenkins-jnlp-agent.git</span>$ git checkout  https://github.com/Kubernetes-Best-Pratice/jenkins-jnlp-agent.git$ <span class="hljs-built_in">cd</span> jenkins-jnlp-agent$ docker build .$ docker tag tag-name custom-private-repository-addr</code></pre><p><em>注: 你也可以基于基础镜像创建自定义的镜像</em></p><p><strong>2）可以做的更完善</strong></p><ol><li>配置webhook, 自动触发jenkins job;</li><li>当前我们实践时构建的镜像版本使用的是固定的, 你是否可以将其替换为依赖pipeline环境变量或是传参的形式，将其变是更有意义；</li><li>上一篇文章（<a href="http://mp.weixin.qq.com/s?__biz=MzU5MTkyNzQ0MQ==&mid=2247483797&idx=1&sn=770557060a5bc4507c2ba9a8d71b1ddb&chksm=fe26c06bc951497d6596a2c9a717bcca04ad130ab8f14097ccd0cc9df9d39bb44adbc4d18768&scene=21#wechat_redirect" target="_blank" rel="noopener">点击这里</a>进入传送门）中在设置【配置Kubernetes Pod Template】时，我们提到可以挂载主机或是网络共享存储，你是否可以通过这个将你的构建快起来；</li><li>我们的示例代码使用的go, 直接是镜像内打包，如何更好的就好的其他语言的构建，你可以参考Using Docker with Pipeline；</li><li>你想过如何下载构建过程中的产物吗，等等</li></ol><p><strong>3）参考链接</strong></p><ol><li><p><a href="https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md" target="_blank" rel="noopener">https://github.com/jenkinsci/kubernetes-cli-plugin/blob/master/README.md</a></p></li><li><p>下载kubectl:</p><p><a href="https://docs.docker.com/ee/ucp/user-access/kubectl/" target="_blank" rel="noopener">https://docs.docker.com/ee/ucp/user-access/kubectl/</a></p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第7篇 CI/CD 之组件部署</title>
    <link href="/2019/12/09/2019-12-09-kubernetes7-cicd/"/>
    <url>/2019/12/09/2019-12-09-kubernetes7-cicd/</url>
    
    <content type="html"><![CDATA[<h1 id="1、前言"><a href="#1、前言" class="headerlink" title="1、前言"></a>1、前言</h1><p>应对敏捷开发的需求，对CI(持续集成))/CD（持续交付）的提出了更高的标准，今天来讨论下，如何基于开源组件（gitlab/jenkins/harbor/kubernetes）使用CI/CD，赋能团队的开发、运维。</p><h1 id="2、核心组件"><a href="#2、核心组件" class="headerlink" title="2、核心组件"></a>2、核心组件</h1><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-09/2.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="3、基本流程"><a href="#3、基本流程" class="headerlink" title="3、基本流程"></a>3、基本流程</h1><p><strong>step1：</strong>在GitLab创建对应的项目。</p><p><strong>step2：</strong>开发者将代码提交到GitLab。</p><p><strong>step3：</strong>Jenkins创建对应的任务（Job），集成该项目的Git地址和Kubernetes集群。</p><p><strong>step4：</strong>如有配置钩子，推送（Push）代码会自动触发Jenkins构建，如没有配置钩子，需要手动构建。</p><p><strong>step5：</strong>Jenkins控制Kubernetes（使用的是Kubernetes插件）创建Jenkins Slave。</p><p><strong>step6：</strong>Jenkins Slave根据流水线（Pipeline）定义的步骤执行构建。</p><p>​    a.检出代码、打包、编译。</p><p>​    b.通过Dockerfile生成镜像。</p><p>​    c.将镜像提送（Push）到私有Harbor。</p><p>​    d.Jenkins再次控制Kubernetes进行最新的镜像部署。</p><p>注:    </p><ul><li><em>上面所述为一般步骤，中间还可能会涉及自动化测试等步骤，可自行根据业务场景添加。</em></li><li><em>上面流水线步骤一般由应用代码库的根目录下Jenkinsfile决定，Jenkins会自动读取该文件；另外如果需要对具体的应用流水线实施强管控，可以独立管理jenkinsfile模板，然后根据jenkins API接口即时生成流水线。</em></li><li><em>默认使用的Dockerfile放置在代码仓库的根目录下。</em></li></ul><h1 id="4、组件部署"><a href="#4、组件部署" class="headerlink" title="4、组件部署"></a>4、组件部署</h1><p><strong>step1：</strong><a href="http://mp.weixin.qq.com/s?__biz=MzU5MTkyNzQ0MQ==&mid=2247483708&idx=1&sn=3efc9ecf7d9c04fd58bfcaf068233ec9&chksm=fe26c0c2c95149d47f8e438bf34f443be745b05d419c0837e20d9245e4df7d5f20589e2a1458&scene=21#wechat_redirect" target="_blank" rel="noopener">kubernetes系列 第3篇 Kubernetes集群安装部署</a></p><p><strong>step2：</strong><a href="http://mp.weixin.qq.com/s?__biz=MzU5MTkyNzQ0MQ==&mid=2247483726&idx=1&sn=ce1b45eb224c936e0173b3dea7786de3&chksm=fe26c0b0c95149a62eafa8aff2d2cc6905389d7bed3ff222a7e5afff0f1ccd08ba89368a0919&scene=21#wechat_redirect" target="_blank" rel="noopener">gitlab 无忌过招:手把手教你搭建自己的GitLab库</a></p><p><strong>step3：</strong>harbor 安装配置指南  <a href="https://github.com/goharbor/harbor/blob/v1.7.4/docs/installation_guide.md" target="_blank" rel="noopener">https://github.com/goharbor/harbor/blob/v1.7.4/docs/installation_guide.md</a></p><p><strong>step4：</strong>jenkins</p><p><em>注: 本文主要说明下jenkins的部署及配置，其他组件如果你部署有问题，欢迎留言。</em></p><h1 id="5、Jenkins部署及配置"><a href="#5、Jenkins部署及配置" class="headerlink" title="5、Jenkins部署及配置"></a>5、Jenkins部署及配置</h1><p><strong>0. 说明:</strong> </p><ul><li><p>以下的yaml文件均在 k8s master节点的 /home/jenkins_deploy目录下。</p></li><li><p>部署示例的depployment.yaml 的注解</p></li><li><ul><li>nodeName ipaddress , ipaddress 请确认其为一个有效的ip。</li><li>示例中jenkins的目录 /var/jenkins_home 是直接挂载到host_path, 如果你有条件，建议替换为共享存储。</li><li>因使用的jenkins-master 的基础镜像来自公网，需要k8s maste 节点也要可以访问外网，或者你可以将 jenkins/jenkins:lts-alpine 推送至自己的内网镜像仓库。</li></ul></li><li><p>部署示例的ingress.yaml 的注解</p></li><li><ul><li>需要你也需要办公网（集群外）访问，请将jenkins.dev.hanker.net, 改为有效的域名地址，或是你也可以通过NodePort的形式声明 service，就可以直接通过ip:port的形式访问jenkins了。</li></ul></li></ul><p><strong>1. 准备部署yaml</strong></p><ul><li>deployment.yaml<pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">devops</span><span class="hljs-comment"># Deployment</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">devops</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>  <span class="hljs-attr">revisionHistoryLimit:</span> <span class="hljs-number">3</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">jenkins</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">nodeName:</span> <span class="hljs-number">1.1</span><span class="hljs-number">.1</span><span class="hljs-number">.1</span>      <span class="hljs-attr">serviceAccountName:</span> <span class="hljs-string">jenkins-admin</span>      <span class="hljs-attr">containers:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">jenkins/jenkins:lts-alpine</span>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins</span>        <span class="hljs-attr">volumeMounts:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-volume</span>          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/jenkins_home</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-localtime</span>          <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/localtime</span>        <span class="hljs-attr">env:</span>          <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">JAVA_OPTS</span>            <span class="hljs-attr">value:</span> <span class="hljs-string">'-Xms256m -Xmx1024m -Duser.timezone=Asia/Shanghai'</span>          <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">TRY_UPGRADE_IF_NO_MARKER</span>            <span class="hljs-attr">value:</span> <span class="hljs-string">'true'</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>          <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8080</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">agent</span>          <span class="hljs-attr">containerPort:</span> <span class="hljs-number">50000</span>        <span class="hljs-attr">resources:</span>          <span class="hljs-attr">requests:</span>            <span class="hljs-attr">cpu:</span> <span class="hljs-string">1000m</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">1Gi</span>          <span class="hljs-attr">limits:</span>            <span class="hljs-attr">cpu:</span> <span class="hljs-string">1200m</span>            <span class="hljs-attr">memory:</span> <span class="hljs-string">2Gi</span>      <span class="hljs-attr">volumes:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-localtime</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/localtime</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-volume</span>          <span class="hljs-attr">hostPath:</span>            <span class="hljs-attr">path:</span> <span class="hljs-string">/home/jenkins/jenkins_home</span></code></pre></li><li>配置service, services.yaml<pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-service</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">devops</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>    <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">50000</span>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">50000</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">agent</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">jenkins</span></code></pre></li><li>授权jenkins对k8s的访问 rbac.yaml<pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">jenkins</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-admin</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">devops</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-rbac</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">devops</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> <span class="hljs-string">["","extensions","app"]</span>    <span class="hljs-attr">resources:</span> <span class="hljs-string">["pods","pods/exec","deployments","replicasets"]</span>    <span class="hljs-attr">verbs:</span> <span class="hljs-string">["get","list","watch","create","update","patch","delete"]</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-admin</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">devops</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">jenkins</span><span class="hljs-attr">subjects:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>    <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-admin</span>    <span class="hljs-attr">namespace:</span> <span class="hljs-string">devops</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-rbac</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span></code></pre></li><li>为了便于办公网（集群外）访问，ingress.yaml<pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">jenkins-ingress</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">devops</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">jenkins.dev.hanker.net</span>    <span class="hljs-attr">http:</span>      <span class="hljs-attr">paths:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">backend:</span>          <span class="hljs-attr">serviceName:</span> <span class="hljs-string">jenkins-service</span>          <span class="hljs-attr">servicePort:</span> <span class="hljs-number">8080</span>        <span class="hljs-attr">path:</span> <span class="hljs-string">/</span></code></pre></li></ul><p><strong>2. 应用yaml，部署jenkins</strong></p><pre><code class="hljs shell"><span class="hljs-meta">$</span><span class="bash"> <span class="hljs-built_in">pwd</span></span><span class="hljs-meta">$</span><span class="bash"> /home/jenkins_deploy</span><span class="hljs-meta">$</span><span class="bash"> kubectl apply -f *.yaml</span></code></pre><p><strong>3. 确认jenkins 服务状态</strong></p><pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>node0 jenkins_deploy]# kubectl -n devops <span class="hljs-keyword">get</span> deployment jenkinsNAME      READY   UP-TO-DATE   AVAILABLE   AGEjenkins   <span class="hljs-number">1</span>/<span class="hljs-number">1</span>     <span class="hljs-number">1</span>            <span class="hljs-number">1</span>           <span class="hljs-number">51</span>d[<span class="hljs-symbol">root@</span>node0 jenkins_deploy]#</code></pre><p><strong>4. 访问jenkins 安装插件、设置</strong></p><p><em>注: 步骤1 声明的域名 jenkins.dev.hanker.net 已经解析至ingress，故可直接访问；如果你也想通过自定义域名访问jenkins，麻请解析至正确的ingress服务节点，即可。</em></p><p>  <strong>a. 确认你也已经安装了kubernetes/ kubernetes cli 插件</strong></p><p>操作指引：【Manage Jenkins】 -&gt; 【Manage Plugins】</p><p>你应该可以通过类似的指令获取jenkins-master的密码。</p><pre><code class="hljs bash">$ kubectl -n devops <span class="hljs-built_in">exec</span> jenkins-pod-name cat /var/jenkins_home/secrets/initialAdminPassword</code></pre><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-09/5-4-a.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong>b.配置Kubernetes 插件</strong></p><p>操作指引：【Manage Jenkins】-&gt;【Configure System】</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-09/5-4-b1.png" srcset="/img/loading.gif" alt="avatar"></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-09/5-4-b2.png" srcset="/img/loading.gif" alt="avatar"></p><p>图中标注：</p><p>1) 请修改为你所在环境对应的k8s master。</p><p>2) 声明jenkins-agent 的命令空间，也可以根据需要调整。</p><p>3）jenkins-master的访问地址，本示例使用的是 service-name的形式访问。</p><p>4) 测试与k8s分享群的连接情况。如果你获取到『Connection test successful』，恭喜你可以继续。</p><p>5）配置Kubernetes Pod Template</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-09/5-4-b3.png" srcset="/img/loading.gif" alt="avatar"></p><p>图中标注：</p><p>1）设置基础的jenkins-agent镜像；</p><p>2）指定工作目录：如果你需要下载、导出或是缓存构建的话，指定一个为共享存储的目录就很有意义了。</p><p>3）设置目录挂载：如步骤2如说，你可以将宿主机的目录或是网络存储挂载至jenkins-agent。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第6篇 Ingress controller - nginx组件介绍</title>
    <link href="/2019/12/08/2019-12-08-kubernetes6-nginx-introduction/"/>
    <url>/2019/12/08/2019-12-08-kubernetes6-nginx-introduction/</url>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>在上一篇文章中我们介绍了如何通过helm进行安装部署traefik组件（<a href="http://mp.weixin.qq.com/s?__biz=MzU5MTkyNzQ0MQ==&mid=2247483749&idx=1&sn=458f63caa65a8fbd33992b8414830388&chksm=fe26c09bc951498d183b2eb05d2f526e2e701e0432b51574ca932839f2146a94bc1e3a781bb7&scene=21#wechat_redirect" target="_blank" rel="noopener">链接点这里</a>），文中还提到常用的ingress controller除了traefik还有Nginx、HAProxy、Kong等，在本篇文章中我们就介绍如何安装部署Nginx-ingress，只有在经过积累不同组件的使用经验之后，我们才能更好的比较其优劣，形成最佳实践。</p><h1 id="2、安装部署"><a href="#2、安装部署" class="headerlink" title="2、安装部署"></a>2、安装部署</h1><p><strong>2.1 通过helm查找nginx-ingress</strong></p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> step1: 通过helm查找nginx-ingress</span><span class="hljs-meta">&gt;</span><span class="bash"> helm search nginx-ingress</span><span class="hljs-meta">&gt;</span><span class="bash"> helm inspect stable/nginx-ingress</span></code></pre><p><strong>2.2 镜像下载及上传</strong></p><p>部分企业由于服务器没有外网访问策略以及防火墙的原因无法获取国外Docker镜像，所以我们事先需要将所需镜像准备好，并上传到企业私有镜像仓库。</p><pre><code class="hljs angelscript"># step2: 镜像准备&gt; docker pull quay.io/kubernetes-ingress-controller/nginx-ingress-controller:<span class="hljs-number">0.25</span><span class="hljs-number">.1</span>&gt; docker tag quay.io/kubernetes-ingress-controller/nginx-ingress-controller:<span class="hljs-number">0.25</span><span class="hljs-number">.1</span> registry.hankercloud.com/ingress-controller/nginx-ingress-controller:<span class="hljs-number">0.25</span><span class="hljs-number">.1</span>&gt; docker push registry.hankercloud.com/ingress-controller/nginx-ingress-controller:<span class="hljs-number">0.25</span><span class="hljs-number">.1</span>&gt;&gt; docker pull k8s.gcr.io/defaultbackend-amd64:<span class="hljs-number">1.5</span>&gt; docker tag k8s.gcr.io/defaultbackend-amd64:<span class="hljs-number">1.5</span> registry.hankercloud.com/google_containers/defaultbackend-amd64:<span class="hljs-number">1.5</span>&gt; docker push registry.hankercloud.com/google_containers/defaultbackend-amd64:<span class="hljs-number">1.5</span></code></pre><p><strong>2.3 组件部署</strong></p><p>在上一篇博客中，我们是采用Deployment模式部署的traefik组件，这次我们采用DaemonSet的模式来部署nginx-ingress组件。</p><pre><code class="hljs routeros"><span class="hljs-comment"># step3: 组件部署</span>&gt; helm install stable/nginx-ingress --name nginx-ingress <span class="hljs-attribute">--namespace</span>=kube-system \  --<span class="hljs-builtin-name">set</span> <span class="hljs-attribute">fullnameOverride</span>=nginx-ingress \  --<span class="hljs-builtin-name">set</span> controller.<span class="hljs-attribute">kind</span>=DaemonSet \  --<span class="hljs-builtin-name">set</span> controller.daemonset.<span class="hljs-attribute">useHostPort</span>=<span class="hljs-literal">true</span> \  --<span class="hljs-builtin-name">set</span> controller.metrics.<span class="hljs-attribute">enabled</span>=<span class="hljs-literal">true</span> \  --<span class="hljs-builtin-name">set</span> controller.image.<span class="hljs-attribute">repository</span>=registry.hankercloud.com/ingress-controller/nginx-ingress-controller \  --set  defaultBackend.image.<span class="hljs-attribute">repository</span>=registry.hankercloud.com/google_containers/defaultbackend-amd64<span class="hljs-comment"># step4: 检查部署是否成功</span>&gt; helm list&gt; kubectl <span class="hljs-builtin-name">get</span> all -n kube-system&gt; kubectl logs <span class="hljs-variable">$POD_NAME</span> -n kube-system</code></pre><p><strong>2.4 负载均衡配置及域名解析处理</strong></p><p>本次我们采用DaemonSet部署nginx-ingress组件，并且使用了主机的80和443接口用来分别接收http和https请求，我们将相应的域名解析到nginx-ingress Pod所在的主机IP之后，就可以通过域名来进行相应的域名访问了。</p><p>但上述配置方式无法做到高可用，当nginx-ingress的Pod实例故障或者其所在主机发生故障时，会导致相应的域名无法访问，所以建议在公有云购买负载均衡设备并配置相应的后端服务器列表以实现高可用的目的。</p><p><strong>2.5 安装调试</strong></p><p>在上文中我们通过helm部署了一个wordpress应用，本文我们继续通过该应用进行域名访问，在本机控制台输入 </p><pre><code class="hljs awk">&gt; curl -i http:<span class="hljs-regexp">//</span><span class="hljs-number">10.0</span>.<span class="hljs-number">0.182</span> -H <span class="hljs-string">'Host: blog.hankercloud.com'</span></code></pre><p>如果看到有正常返回则说明部署成功。</p><h1 id="3、企业场景及解决方案"><a href="#3、企业场景及解决方案" class="headerlink" title="3、企业场景及解决方案"></a>3、企业场景及解决方案</h1><p><strong>3.1 如何做内外网的隔离</strong></p><p><strong>Step1:</strong> 我们首先部署了两个ingress组件，其中之一是接收内网访问请求，另外一个是接收外网访问请求，相应配置如下：</p><pre><code class="hljs yaml"><span class="hljs-comment"># 内网nginx-ingress配置声明：</span><span class="hljs-attr">spec:</span><span class="hljs-attr">template:</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><span class="hljs-bullet">-</span> <span class="hljs-string">/nginx-ingress-controller</span><span class="hljs-bullet">-</span> <span class="hljs-string">--default-backend-service=kube-system/nginx-ingress-default-backend</span><span class="hljs-bullet">-</span> <span class="hljs-string">--election-id=ingress-controller-leader</span><span class="hljs-bullet">-</span> <span class="hljs-string">--ingress-class=nginx</span><span class="hljs-bullet">-</span> <span class="hljs-string">--configmap=kube-system/nginx-ingress-controller</span></code></pre><pre><code class="hljs yaml"><span class="hljs-comment"># 外网nginx-ingress配置声明：</span><span class="hljs-attr">spec:</span><span class="hljs-attr">template:</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">args:</span><span class="hljs-bullet">-</span> <span class="hljs-string">/nginx-ingress-controller</span><span class="hljs-bullet">-</span> <span class="hljs-string">--default-backend-service=kube-system/nginx-ingress-external-default-backend</span><span class="hljs-bullet">-</span> <span class="hljs-string">--election-id=ingress-controller-leader</span><span class="hljs-bullet">-</span> <span class="hljs-string">--ingress-class=nginx-external</span><span class="hljs-bullet">-</span> <span class="hljs-string">--configmap=kube-system/nginx-ingress-external-controller</span></code></pre><p>两者的主要区别在于参数 –ingress-class 设置的值是不一样的。</p><p><strong>Step2:</strong> 对于需要暴露到公网的域名，修改其ingress的定义，相应配置参考如下：</p><pre><code class="hljs yaml"><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">www</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">kubernetes.io/ingress.class:</span> <span class="hljs-string">"nginx-external"</span></code></pre><p><strong>Step3:</strong> 检查是否配置成功，执行 </p><pre><code class="hljs shell">kubectl exec $&#123;POD_NAME&#125; -n kube-system cat /etc/nginx/nginx.conf</code></pre><p>查看配置文件中是否已经包含外网域名的相关配置，并在本地进行测试验证。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第5篇 Ingress Controller - Traefik组件介绍</title>
    <link href="/2019/12/07/2019-12-07-kubernetes5-traefik-introduction/"/>
    <url>/2019/12/07/2019-12-07-kubernetes5-traefik-introduction/</url>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>为了能够让Ingress资源能够工作，在Kubernetes集群中必须至少有一个运行中的ingress controller组件。也就是说如果在kubernetes集群中没有一个ingress controller组件，只是定义了ingress资源，其实并不会实现http、https协议的请求转发、负载均衡等功能。常见的ingress controller组件如下：</p><ul><li>Nginx</li><li>Traefik</li><li>Kong</li><li>Istio</li><li>HAProxy</li></ul><p>关于上述的组件目前并没有详细的对比，后续我们在对每个组件都有一定的了解和使用的基础之上，可以给出一些详细的对比信息。本篇内容将主要介绍traefik组件的安装部署以及会通过一个具体的应用作演示。</p><h1 id="2、Trafeik组件的安装部署"><a href="#2、Trafeik组件的安装部署" class="headerlink" title="2、Trafeik组件的安装部署"></a>2、Trafeik组件的安装部署</h1><h2 id="2-1-通过helm-chart部署traefik"><a href="#2-1-通过helm-chart部署traefik" class="headerlink" title="2.1 通过helm chart部署traefik"></a>2.1 通过helm chart部署traefik</h2><p>helm traefik chart包中包含了部署traefik组件的所需的资源，我们可以通过借助该组件进行快速部署traefik组件，以下是部署命令行信息：</p><pre><code class="hljs bash">&gt; helm install --name inner-traefik --namespace kube-system\  --<span class="hljs-built_in">set</span> image=registry.docker.hankercloud.com/ingress-controller/traefik \  --<span class="hljs-built_in">set</span> serviceType=NodePort \  stable/traefik</code></pre><p>部署完成后，执行kubectl get pods -n kube-system命令，可以看到在kube-system的命名空间中已经存在名为 inner-traefik 的Pod。</p><h2 id="2-2-RBAC配置"><a href="#2-2-RBAC配置" class="headerlink" title="2.2 RBAC配置"></a>2.2 RBAC配置</h2><p>在kubernetes 1.6版本中引入了RBAC（Role Based Access Control）机制来更好的管理资源和API的访问。如果在集群中配置了RBAC，则需要授权Treafik使用Kubernetes的API，有两种方式来进行设置合适的策略：通过特定的命名空间进行角色绑定（RoleBinding）以及全局角色绑定（ClusterRoleBinding）。现在简单起见，我们直接使用ClusterRoleBinding，资源定义如下：</p><pre><code class="hljs yaml"><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">""</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">services</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">endpoints</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">secrets</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">extensions</span>    <span class="hljs-attr">resources:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">ingresses</span>    <span class="hljs-attr">verbs:</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">get</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">list</span>      <span class="hljs-bullet">-</span> <span class="hljs-string">watch</span><span class="hljs-meta">---</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRoleBinding</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">rbac.authorization.k8s.io/v1beta1</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">roleRef:</span>  <span class="hljs-attr">apiGroup:</span> <span class="hljs-string">rbac.authorization.k8s.io</span>  <span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterRole</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span><span class="hljs-attr">subjects:</span><span class="hljs-bullet">-</span> <span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">traefik-ingress-controller</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span></code></pre><p>接下来我们执行如下命令创建资源并修改deployment的资源定义文件。</p><pre><code class="hljs bash">kubectl apply -f traefik-rbac.ymlkubectl edit deploy inner-traefik -n kube-system</code></pre><p>执行完上述的操作之后，我们可以进行校验相关的资源已经正常启动。</p><pre><code class="hljs bash">kubectl logs $(kubectl get pods -n kube-system |grep traefik | awk <span class="hljs-string">'&#123;print $1&#125;'</span>) -n kube-system</code></pre><h2 id="2-3-负载均衡配置"><a href="#2-3-负载均衡配置" class="headerlink" title="2.3 负载均衡配置"></a>2.3 负载均衡配置</h2><p>由于我们使用的是Deployment部署的traefik组件，其Service Type为NodePort，通过kubectl get svc -n kube-system|grep traefik，可以看到端口映射关系，接下来我们在阿里云申请一个负载均衡的设备，然后进行相应的配置之后就完成了这一步操作。</p><p>另外一种替代方式是使用DaemonSet的方式部署traefik组件，设置主机端口和Pod实例端口的映射关系，也可以完成这一任务。</p><h1 id="3、创建ingress资源并进行调试"><a href="#3、创建ingress资源并进行调试" class="headerlink" title="3、创建ingress资源并进行调试"></a>3、创建ingress资源并进行调试</h1><p>接下来我们在kubernetes集群中创建一个ingress资源，由于我们之前已经在集群中部署了一个wordpress应用，资源定义文件如下：</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">extensions/v1beta1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">wordpress-ingress</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">annotations:</span>    <span class="hljs-attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="hljs-string">/</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">rules:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">blog.hankercloud.com</span>    <span class="hljs-attr">http:</span>      <span class="hljs-attr">paths:</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">/</span>        <span class="hljs-attr">backend:</span>          <span class="hljs-attr">serviceName:</span> <span class="hljs-string">wordpress-test-wordpress</span>          <span class="hljs-attr">servicePort:</span> <span class="hljs-number">80</span></code></pre><p>完成上述的操作之后，我们在本地修改/etc/hosts文件，手动配置blog.hankercloud.com的域名解析记录，在浏览器地址栏输入 <a href="http://blog.hankercloud.com" target="_blank" rel="noopener">http://blog.hankercloud.com</a> 就可以看到页面了，到此我们完成了traefik组件的安装部署及调试工作。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第4篇 Kubernetes包管理工具-helm介绍</title>
    <link href="/2019/12/06/2019-12-06-kubernetes4-helm-introduction/"/>
    <url>/2019/12/06/2019-12-06-kubernetes4-helm-introduction/</url>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>Helm是kubernetes包管理工具，可以方便快捷的安装、管理、卸载kubernetes应用，类似于Linux操作系统中yum或apt-get软件的作用。其主要的设计目的：</p><ul><li>创建新的chart包</li><li>将charts包文件打包压缩</li><li>同chart仓库进行集成，获取charts文件</li><li>安装及卸载charts到kubernetes集群</li><li>管理通过helm安装的charts应用</li></ul><h1 id="2、概念介绍"><a href="#2、概念介绍" class="headerlink" title="2、概念介绍"></a>2、概念介绍</h1><p><strong>chart:</strong> 一个 Helm 包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等，还可能包含 Kubernetes 集群中的服务定义。</p><p><strong>release：</strong>在 Kubernetes 集群上运行的 Chart 的一个实例。在同一个集群上，一个 Chart 可以安装很多次，每次安装都会创建一个新的 release。</p><p><strong>repository：</strong>用于发布和存储 Chart 的仓库，Helm客户端通过HTTP协议来访问仓库中Chart的索引文件和压缩包。</p><h1 id="3、组件"><a href="#3、组件" class="headerlink" title="3、组件"></a>3、组件</h1><p><strong>helm:</strong> 提供给用户的客户端程序，可以以命令行的形式同服务端-tiller进行通信。</p><p><strong>tiller：</strong>服务端软件，用来同helm客户端进行交互，并同kubernetes api server组件进行交互。</p><p>架构如下：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-06/4-3.png" srcset="/img/loading.gif" alt="avatar"></p><h1 id="4、安装部署"><a href="#4、安装部署" class="headerlink" title="4、安装部署"></a>4、安装部署</h1><h2 id="1-helm的安装部署"><a href="#1-helm的安装部署" class="headerlink" title="1. helm的安装部署"></a>1. helm的安装部署</h2><ul><li>版本下载，版本列表 <a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">https://github.com/helm/helm/releases</a></li><li>解压缩, tar -zxvf helm-v2.0.0-linux-amd64.tgz</li><li>将解压缩后的二进制文件放在可执行目录下 mv linux-amd64/helm /usr/local/bin/helm，然后执行 helm –help查看帮助文档</li></ul><h2 id="2-tiller的安装部署"><a href="#2-tiller的安装部署" class="headerlink" title="2. tiller的安装部署"></a>2. tiller的安装部署</h2><p>控制台执行 &gt; helm init命令，该命令会将从charts仓库中下载charts包，并根据其中的配置部署至kubernetes集群。</p><p>默认的charts仓库为 <a href="https://kubernetes-charts.storage.googleapis.com/index.yaml" target="_blank" rel="noopener">https://kubernetes-charts.storage.googleapis.com/index.yaml</a></p><p>默认使用的tiller镜像为 gcr.io/kubernetes-helm/tiller:v2.13.1 </p><p>国内由于墙的原因无法直接访问，需要我们自行处理可替代的仓库和镜像版本，通过如下命令进行helm服务端的安装部署：</p><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash"> helm init --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span>Creating /root/.helm/repository/repositories.yamlAdding stable repo with URL: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/chartsAdding local repo with URL: http://127.0.0.1:8879/charts<span class="hljs-meta">$</span><span class="bash">HELM_HOME has been configured at /root/.helm.</span>Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users'policy.To prevent this, run `helm init` with the --tiller-tls-verify flag.For more information on securing your installation see:https://docs.helm.sh/using_helm/#securing-your-helm-installationHappy Helming!</code></pre><p>稍等一会然后执行如下命令，看到如下输出说明安装成功：</p><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash"> helm version</span>Client: &amp;version.Version&#123;SemVer:"v2.13.1", GitCommit:"618447cbf203d147601b4b9bd7f8c37a5d39fbb4", GitTreeState:"clean"&#125;Server: &amp;version.Version&#123;SemVer:"v2.13.1", GitCommit:"618447cbf203d147601b4b9bd7f8c37a5d39fbb4", GitTreeState:"clean"&#125;</code></pre><p>通过执行 helm –help 可以看到常用的命令，说明如下：</p><ul><li>search 在helm仓库进行查找应用</li><li>fetch 从仓库中下载chart包到本地</li><li>list 在该k8s集群的部署的release列表</li><li>status 显示release的具体信息</li><li>install 安装charts</li><li>inspect 描述charts信息</li><li>delete 删除部署的release</li><li>create 创建一个charts</li><li>package 将某一charts进行打包压缩</li><li>repo 显示、添加、移除charts仓库</li></ul><h1 id="5、访问授权"><a href="#5、访问授权" class="headerlink" title="5、访问授权"></a>5、访问授权</h1><p>在上面的步骤中我们将tiller所需的资源部署到了kubernetes集群中，但是由于Deployment tiller-deploy没有定义授权的ServiceAccount导致访问apiserver拒绝，执行如下命令为tiller-deploy进行授权：</p><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash"> kubectl create serviceaccount --namespace kube-system tiller</span><span class="hljs-meta">&gt;</span><span class="bash"> kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><span class="hljs-meta">&gt;</span><span class="bash"> kubectl patch deploy --namespace kube-system tiller-deploy -p <span class="hljs-string">'&#123;"spec":&#123;</span></span></code></pre><h1 id="6、通过helm部署WordPress"><a href="#6、通过helm部署WordPress" class="headerlink" title="6、通过helm部署WordPress"></a>6、通过helm部署WordPress</h1><p>输入如下命令，我们可以通过helm创建一个WordPress博客网站</p><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash"> helm install --name wordpress-test --<span class="hljs-built_in">set</span> <span class="hljs-string">"persistence.enabled=false,mariadb.persistence.enabled=false"</span> stable/wordpress</span></code></pre><p>通过如下命令获取登录信息：</p><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash"> kubectl get svc -o wide</span><span class="hljs-meta">&gt;</span><span class="bash"> kubectl get secret --namespace default wordpress-test-wordpress -o jsonpath=<span class="hljs-string">"&#123;.data.wordpress-password&#125;"</span> | base64 --decode</span></code></pre><p>在浏览器中打开页面，并输入用户名和密码就可以看到搭建好的WordPress博客网站了。</p><h1 id="7、升级"><a href="#7、升级" class="headerlink" title="7、升级"></a>7、升级</h1><p>当有新的chart包发布时或者想改变已有release的配置时，可以通过 helm upgrade命令实现，比如：</p><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash"> helm upgrade wordpress-test \</span><span class="hljs-meta">&gt;</span><span class="bash"> --<span class="hljs-built_in">set</span> <span class="hljs-string">"persistence.enabled=true,mariadb.persistence.enabled=true"</span> \</span><span class="hljs-meta">&gt;</span><span class="bash"> stable/wordpress</span></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第3篇 Kubernetes集群安装部署</title>
    <link href="/2019/12/05/2019-12-05-kubernetes3-cluster-install/"/>
    <url>/2019/12/05/2019-12-05-kubernetes3-cluster-install/</url>
    
    <content type="html"><![CDATA[<p>本文介绍了如何通过Kubespray来进行部署高可用k8s集群，k8s版本为1.12.5。</p><h1 id="1、部署手册"><a href="#1、部署手册" class="headerlink" title="1、部署手册"></a>1、部署手册</h1><p>代码仓库：<a href="https://github.com/kubernetes-sigs/kubespray" target="_blank" rel="noopener">https://github.com/kubernetes-sigs/kubespray</a></p><p>参考文档：<a href="https://kubespray.io/#/" target="_blank" rel="noopener">https://kubespray.io/#/</a></p><h1 id="2、k8s-master机器配置"><a href="#2、k8s-master机器配置" class="headerlink" title="2、k8s master机器配置"></a>2、k8s master机器配置</h1><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-05/2.jpg" srcset="/img/loading.gif" alt="avatar"></p><h1 id="3、k8s-集群安装步骤"><a href="#3、k8s-集群安装步骤" class="headerlink" title="3、k8s 集群安装步骤"></a>3、k8s 集群安装步骤</h1><p><strong>step1: 设置主机间的免密登录</strong></p><p>由于kubespray是依赖于ansible，ansible通过ssh协议进行主机之间的访问，所以部署之前需要设置主机之间免密登录，步骤如下：</p><pre><code class="hljs shell">ssh-keygen -t rsascp ~/.ssh/id_rsa.pub root@IP:/root/.sshssh root@IPcat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</code></pre><p><strong>step2: 下载kubespray</strong></p><p>注意：不要通过使用github仓库master分支的代码，我这里使用的是tag v2.8.3进行部署</p><pre><code class="hljs shell">wget https://github.com/kubernetes-sigs/kubespray/archive/v2.8.3.tar.gztar -xvf v2.8.3cd kubespray-v2.8.3</code></pre><p><strong>step3: 配置调整</strong></p><h2 id="3-1-更换镜像"><a href="#3-1-更换镜像" class="headerlink" title="3.1 更换镜像"></a>3.1 更换镜像</h2><p>Kubernetes安装大部分都是使用的国外的镜像，由于防火墙原因没有办法获取到这些镜像，所以需要自己创建镜像仓库并将这些镜像获取到上传到镜像仓库中。</p><p><strong>3.1.1 新建镜像仓库</strong></p><p>镜像仓库我们选用的组件是Harbor，安装步骤参考：</p><p><a href="https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md" target="_blank" rel="noopener">https://github.com/goharbor/harbor/blob/master/docs/installation_guide.md</a></p><p><strong>3.1.2 整理k8s集群部署中需要使用的镜像</strong></p><p>在文件roles/download/defaults/main.yml文件中，可以看到使用的全量镜像列表，注意某些镜像由于功能未使用的原因所以暂时没有用到，我们主要用到有如下镜像：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-05/3-1-2.jpg" srcset="/img/loading.gif" alt="avatar"></p><p><strong>3.1.3 下载所需镜像并上传至私有镜像仓库</strong></p><p>使用的镜像列表如下，在这里我申请了一台国外的阿里云主机，在该台主机下载所需镜像然后上传至私有镜像仓库</p><p>例如操作某个镜像时，需要执行如下命令：</p><pre><code class="hljs shell">docker pull gcr.io/google_containers/kubernetes-dashboard-amd64:v1.10.0docker tag gcr.io/google_containers/kubernetes-dashboard-amd64:v1.10.0 106.14.219.69:5000/google_containers/kubernetes-dashboard-amd64:v1.10.0docker push 106.14.219.69:5000/google_containers/kubernetes-dashboard-amd64:v1.10.0</code></pre><p><strong>3.1.4 更改镜像地址并修改Docker配置</strong></p><p>在inventory/testcluster/group_vars/k8s-cluster/k8s-cluster.yml文件中添加如下配置：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> kubernetes image repo define</span>kube_image_repo: "10.0.0.183:5000/google_containers"<span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment"># modified by: robbin</span></span><span class="hljs-meta">#</span><span class="bash"> comment: 将使⽤的组件的镜像仓库修改为私有镜像仓库地址</span>etcd_image_repo: "10.0.0.183:5000/coreos/etcd"coredns_image_repo: "10.0.0.183:5000/coredns"calicoctl_image_repo: "10.0.0.183:5000/calico/ctl"calico_node_image_repo: "10.0.0.183:5000/calico/node"calico_cni_image_repo: "10.0.0.183:5000/calico/cni"calico_policy_image_repo: "10.0.0.183:5000/calico/kube-controllers"hyperkube_image_repo: "&#123;&#123; kube_image_repo &#125;&#125;/hyperkube-&#123;&#123; image_arch &#125;&#125;"pod_infra_image_repo: "&#123;&#123; kube_image_repo &#125;&#125;/pause-&#123;&#123; image_arch &#125;&#125;"dnsautoscaler_image_repo: "&#123;&#123; kube_image_repo &#125;&#125;/cluster-proportional-autoscaler-&#123;&#123; image_arch &#125;&#125;"dashboard_image_repo: "&#123;&#123; kube_image_repo &#125;&#125;/kubernetes-dashboard-&#123;&#123; image_arch &#125;&#125;"</code></pre><p>由于我们的私有镜像仓库未配置https证书，需要在 inventory/testcluster/group_vars/all/docker.yml文件中添加如下配置：</p><pre><code class="hljs shell">docker_insecure_registries:- 10.0.0.183:5000</code></pre><h2 id="3-2-Docker安装源更改以及执行文件预处理"><a href="#3-2-Docker安装源更改以及执行文件预处理" class="headerlink" title="3.2 Docker安装源更改以及执行文件预处理"></a>3.2 Docker安装源更改以及执行文件预处理</h2><p><strong>3.2.1 Docker安装源更改</strong></p><p>由于默认从Docker官方源安装docker，速度非常慢，这里我们更换为国内阿里源，在inventory/testcluster/group_vars/k8s-cluster/k8s-cluster.yml文件中添加如下配置：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> CentOS/RedHat docker-ce repodocker_rh_repo_base_url: <span class="hljs-string">'https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable'</span></span>docker_rh_repo_gpgkey: 'https://mirrors.aliyun.com/docker-ce/linux/centos/gpg'dockerproject_rh_repo_base_url: 'https://mirrors.aliyun.com/docker-engine/yum/repo/main/centos/7'dockerproject_rh_repo_gpgkey: 'https://mirrors.aliyun.com/docker-engine/yum/gpg'</code></pre><p><strong>3.2.2 可执行文件预处理</strong></p><p>另外由于需要从google以及github下载一些可执行文件，由于防火墙原因无法直接在服务器上下载，我们可以预先将这些执行文件下载好，然后上传到指定的服务器路径中</p><p>可执行文件下载地址可以在roles/download/defaults/main.yml文件中查找到，下载路径如下：</p><pre><code class="hljs shell">kubeadm_download_url: "https://storage.googleapis.com/kubernetes-release/release/v1.12.5/bin/linux/amd64/kubeadm"hyperkube_download_url: "https://storage.googleapis.com/kubernetes-release/release/v1.12.5/bin/linux/amd64/hyperkube"cni_download_url: "https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz"</code></pre><p>接下来修改文件权限，并上传到每台服务器的/tmp/releases目录下</p><pre><code class="hljs shell">chmod 755 cni-plugins-amd64-v0.6.0.tgz hyperkube kubeadmscp cni-plugins-amd64-v0.6.0.tgz hyperkube kubeadm root@node1:/tmp/releases</code></pre><h2 id="3-3-组件列表"><a href="#3-3-组件列表" class="headerlink" title="3.3 组件列表"></a>3.3 组件列表</h2><p><strong>k8s所需要的组件</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-05/3-3-1.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong>可选插件列表</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-05/3-3-2.png" srcset="/img/loading.gif" alt="avatar"></p><h2 id="3-4-DNS方案"><a href="#3-4-DNS方案" class="headerlink" title="3.4 DNS方案"></a>3.4 DNS方案</h2><p>k8s的服务发现依赖于DNS，涉及到两种类型的网络：主机网络和容器网络，所以Kubespray提供了两种配置来进行管理 </p><p><strong>3.4.1 dns_mode</strong></p><p>dns_mode 主要用于集群内的域名解析，有如下几种类型，我们的技术选型是coredns，注意：选择某种dns_mode，可能需要下载安装多个容器镜像，其镜像版本也可能不同</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-05/3-4-1.png" srcset="/img/loading.gif" alt="avatar"></p><p><strong>3.4.2 resolvconf_mode</strong></p><p>resolvconf_mode主要用来解决当容器部署为host网络模式的时候，如何使用k8s的dns，这里我们使用的是docker_dns</p><pre><code class="hljs shell">resolvconf_mode: docker_dns</code></pre><h2 id="3-5-网络插件选择"><a href="#3-5-网络插件选择" class="headerlink" title="3.5 网络插件选择"></a>3.5 网络插件选择</h2><p><strong>3.5.1 kube-proxy</strong></p><p>kube-proxy可以选择ipvs或者iptables，在这里我们选择的是ipvs模式，关于这两者的区别可以参考 华为云在 K8S 大规模场景下的 Service 性能优化实践(<a href="https://zhuanlan.zhihu.com/p/37230013" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37230013</a>)</p><p><strong>3.5.2 网络插件列表</strong></p><p>网络插件列表如下，我们的技术选型是calico，注意：选择某种网络插件，可能需要一个或多个容器镜像，其镜像版本也可能不同</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-12-05/3-5-2.png" srcset="/img/loading.gif" alt="avatar"></p><h2 id="3-6-高可用方案"><a href="#3-6-高可用方案" class="headerlink" title="3.6 高可用方案"></a>3.6 高可用方案</h2><p><strong>step4: 按照如下步骤进行安装部署</strong></p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Install dependencies from ``requirements.txt``</span>sudo pip install -r requirements.txt<span class="hljs-meta">#</span><span class="bash"> Copy `inventory/sample` as `inventory/mycluster`</span>cp -rfp inventory/sample inventory/mycluster<span class="hljs-meta">#</span><span class="bash"> Update Ansible inventory file with inventory builder</span>declare -a IPS=(10.10.1.3 10.10.1.4 10.10.1.5)CONFIG_FILE=inventory/mycluster/hosts.ini python3 contrib/inventory_builder/inventory.py $&#123;IPS[@]&#125;<span class="hljs-meta">#</span><span class="bash"> Review and change parameters under `inventory/mycluster/group_vars`</span>cat inventory/mycluster/group_vars/all/all.ymlcat inventory/mycluster/group_vars/k8s-cluster/k8s-cluster.yml<span class="hljs-meta">#</span><span class="bash"> Deploy Kubespray with Ansible Playbook - run the playbook as root</span><span class="hljs-meta">#</span><span class="bash"> The option `-b` is required, as <span class="hljs-keyword">for</span> example writing SSL keys <span class="hljs-keyword">in</span> /etc/,</span><span class="hljs-meta">#</span><span class="bash"> installing packages and interacting with various systemd daemons.</span><span class="hljs-meta">#</span><span class="bash"> Without -b the playbook will fail to run!</span>ansible-playbook -i inventory/mycluster/hosts.ini --become --become-user=root cluster.yml</code></pre><p>部署完成，可以登录到k8s-master所在的主机，执行如下命令，可以看到各个组件正常</p><pre><code class="hljs shell">kubectl cluster-infokubectl get nodekubectl get pods --all-namespaces</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第2篇 基础概念介绍</title>
    <link href="/2019/12/04/2019-12-04-kubernetes2-introduction-to-basic-concepts/"/>
    <url>/2019/12/04/2019-12-04-kubernetes2-introduction-to-basic-concepts/</url>
    
    <content type="html"><![CDATA[<h1 id="1、Pod-实例"><a href="#1、Pod-实例" class="headerlink" title="1、Pod - 实例"></a>1、Pod - 实例</h1><p>Pod是一组紧密关联的容器集合，支持多个容器在一个Pod中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式完成服务，是Kubernetes调度的基本单位。Pod的设计理念是 <strong>每个Pod都有一个唯一的IP</strong></p><p>Pod具有如下特征：</p><ul><li>包含多个共享IPC、Network和UTC namespace的容器，可直接通过localhost通信 </li><li>所有Pod内容器都可以访问共享的Volume，可以访问共享数据 </li><li>优雅终止:Pod删除的时候先给其内的进程发送SIGTERM，等待一段时间(grace period)后才强制停止依然还在运行的进程 </li><li>特权容器(通过SecurityContext配置)具有改变系统配置的权限(在网络插件中大量应用)</li><li>支持三种重启策略（restartPolicy），分别是：Always、OnFailure、Never</li><li>支持三种镜像拉取策略（imagePullPolicy），分别是：Always、Never、IfNotPresent</li><li>资源限制，Kubernetes通过CGroup限制容器的CPU以及内存等资源，可以设置request以及limit值</li><li>健康检查，提供两种健康检查探针，分别是livenessProbe和redinessProbe，前者用于探测容器是否存活，如果探测失败，则根据重启策略进行重启操作，后者用于检查容器状态是否正常，如果检查容器状态不正常，则请求不会到达该Pod</li><li>Init container在所有容器运行之前执行，常用来初始化配置</li><li>容器生命周期钩子函数，用于监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数，支持两种钩子函数：postStart和preStop，前者是在容器启动后执行，后者是在容器停止前执行</li></ul><h1 id="2、Namespace-命名空间"><a href="#2、Namespace-命名空间" class="headerlink" title="2、Namespace - 命名空间"></a>2、Namespace - 命名空间</h1><p>Namespace（命名空间）是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或者用户组。常见的pod、service、replicaSet和deployment等都是属于某一个namespace的(默认是default)，而node, persistentVolumes等则不属于任何namespace。</p><p>常用namespace操作：</p><pre><code class="hljs bash"><span class="hljs-comment"># 查询所有namespaces </span>kubectl get namespace<span class="hljs-comment"># </span>创建namespacekubectl create namespace ns-name <span class="hljs-comment"># 删除namespace</span>kubectl delete namespace ns-name</code></pre><p>删除命名空间时，需注意以下几点：</p><ol><li>删除一个namespace会自动删除所有属于该namespace的资源。</li><li>default 和 kube-system 命名空间不可删除。</li><li>PersistentVolumes是不属于任何namespace的，但PersistentVolumeClaim是属于某个特定namespace的。</li><li>Events是否属于namespace取决于产生events的对象。</li></ol><h1 id="3、Node-节点"><a href="#3、Node-节点" class="headerlink" title="3、Node 节点"></a>3、Node 节点</h1><p>Node是Pod真正运行的主机，可以是物理机也可以是虚拟机。Node本质上不是Kubernetes来创建的， Kubernetes只是管理Node上的资源。为了管理Pod，每个Node节点上至少需要运行container runtime（Docker）、kubelet和kube-proxy服务。</p><p>常用node操作：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 查询所有node</span>kubectl get nodes<span class="hljs-meta">#</span><span class="bash"> 将node标志为不可调度</span>kubectl cordon $nodename<span class="hljs-meta">#</span><span class="bash"> 将node标志为可调度</span>kubectl uncordon $nodename</code></pre><p><strong>taint(污点)</strong></p><p>使用kubectl taint命令可以给某个Node节点设置污点，Node被设置上污点之后就和Pod之间存在了一种相斥的关系，可以让Node拒绝Pod的调度执行，甚至将Node已经存在的Pod驱逐出去。每个污点的组成：<code>key=value:effect</code>，当前taint effect支持如下三个选项：</p><ul><li>NoSchedule：表示k8s将不会将Pod调度到具有该污点的Node上</li><li>PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上</li><li>NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去</li></ul><p>常用命令如下：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 为节点node0设置不可调度污点</span>kubectl taint node node0 key1=value1:NoShedule<span class="hljs-meta">#</span><span class="bash"> 将node0上key值为key1的污点移除</span>kubectl taint node node0 key-<span class="hljs-meta">#</span><span class="bash"> 为kube-master节点设置不可调度污点</span>kubectl taint node node1 node-role.kubernetes.io/master=:NoSchedule<span class="hljs-meta">#</span><span class="bash"> 为kube-master节点设置尽量不可调度污点</span>kubectl taint node node1 node-role.kubernetes.io/master=PreferNoSchedule</code></pre><p><strong>容忍(Tolerations)</strong></p><p>设置了污点的Node将根据taint的effect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。 但我们可以在Pod上设置容忍(Toleration)，意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上。</p><h1 id="4、Service-服务"><a href="#4、Service-服务" class="headerlink" title="4、Service 服务"></a>4、Service 服务</h1><p>Service是对一组提供相同功能的Pods的抽象，并为他们提供一个统一的入口，借助 Service 应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。 Service通过标签(label)来选取后端Pod，一般配合ReplicaSet或者Deployment来保证后端容器的正常运行。</p><p>service 有如下四种类型，默认是ClusterIP：</p><ul><li>ClusterIP: 默认类型，自动分配一个仅集群内部可以访问的虚拟IP</li><li>NodePort: 在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过 <code>NodeIP:NodePort</code> 来访问该服务</li><li>LoadBalancer: 在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到 NodeIP:NodePort</li><li>ExternalName: 将服务通过DNS CNAME记录方式转发到指定的域名</li></ul><p>另外，也可以将已有的服务以Service的形式加入到Kubernetes集群中来，只需要在创建 Service 的时候不指定Label selector，而是在Service创建好后手动为其添加endpoint。</p><h1 id="5、Volume-存储卷"><a href="#5、Volume-存储卷" class="headerlink" title="5、Volume 存储卷"></a>5、Volume 存储卷</h1><p>默认情况下容器的数据是非持久化的，容器消亡以后数据也会跟着丢失，所以Docker提供了Volume机制以便将数据持久化存储。Kubernetes提供了更强大的Volume机制和插件，解决了容器数据持久化以及容器间共享数据的问题。</p><p>Kubernetes存储卷的生命周期与Pod绑定</p><ul><li>容器挂掉后Kubelet再次重启容器时，Volume的数据依然还在</li><li>Pod删除时，Volume才会清理。数据是否丢失取决于具体的Volume类型，比如emptyDir的数据会丢失，而PV的数据则不会丢</li></ul><p>目前Kubernetes主要支持以下Volume类型：</p><ul><li>emptyDir：Pod存在，emptyDir就会存在，容器挂掉不会引起emptyDir目录下的数据丢失，但是pod被删除或者迁移，emptyDir也会被删除</li><li>hostPath：hostPath允许挂载Node上的文件系统到Pod里面去</li><li>NFS（Network File System）：网络文件系统，Kubernetes中通过简单地配置就可以挂载NFS到Pod中，而NFS中的数据是可以永久保存的，同时NFS支持同时写操作。</li><li>glusterfs：同NFS一样是一种网络文件系统，Kubernetes可以将glusterfs挂载到Pod中，并进行永久保存</li><li>cephfs：一种分布式网络文件系统，可以挂载到Pod中，并进行永久保存</li><li>subpath：Pod的多个容器使用同一个Volume时，会经常用到</li><li>secret：密钥管理，可以将敏感信息进行加密之后保存并挂载到Pod中</li><li>persistentVolumeClaim：用于将持久化存储（PersistentVolume）挂载到Pod中</li><li>…</li></ul><h1 id="6、PersistentVolume-PV-持久化存储卷"><a href="#6、PersistentVolume-PV-持久化存储卷" class="headerlink" title="6、PersistentVolume(PV) 持久化存储卷"></a>6、PersistentVolume(PV) 持久化存储卷</h1><p>PersistentVolume(PV)是集群之中的一块网络存储。跟 Node 一样，也是集群的资源。PersistentVolume (PV)和PersistentVolumeClaim (PVC)提供了方便的持久化卷: PV提供网络存储资源，而PVC请求存储资源并将其挂载到Pod中。</p><p>PV的访问模式(accessModes)有三种:</p><ul><li>ReadWriteOnce(RWO):是最基本的方式，可读可写，但只支持被单个Pod挂载。</li><li>ReadOnlyMany(ROX):可以以只读的方式被多个Pod挂载。 </li><li>ReadWriteMany(RWX):这种存储可以以读写的方式被多个Pod共享。</li></ul><p>不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是 NFS。在PVC绑定PV时通常根据两个条件来绑定，一个是存储的大小，另一个就是 访问模式。</p><p>PV的回收策略(persistentVolumeReclaimPolicy)也有三种</p><ul><li>Retain，不清理保留Volume(需要手动清理)</li><li>Recycle，删除数据，即 rm -rf /thevolume/* (只有NFS和HostPath支持) </li><li>Delete，删除存储资源</li></ul><h1 id="7、Deployment-无状态应用"><a href="#7、Deployment-无状态应用" class="headerlink" title="7、Deployment 无状态应用"></a>7、Deployment 无状态应用</h1><p>一般情况下我们不需要手动创建Pod实例，而是采用更高一层的抽象或定义来管理Pod，针对无状态类型的应用，Kubernetes使用Deloyment的Controller对象与之对应。其典型的应用场景包括：</p><ul><li>定义Deployment来创建Pod和ReplicaSet </li><li>滚动升级和回滚应用</li><li>扩容和缩容</li><li>暂停和继续Deployment</li></ul><p>常用的操作命令如下：</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 生成一个Deployment对象</span>kubectl run www --image=10.0.0.183:5000/hanker/www:0.0.1 --port=8080<span class="hljs-meta">#</span><span class="bash"> 查找Deployment</span>kubectl get deployment --all-namespaces<span class="hljs-meta">#</span><span class="bash"> 查看某个Deployment</span>kubectl describe deployment www<span class="hljs-meta">#</span><span class="bash"> 编辑Deployment定义</span>kubectl edit deployment www<span class="hljs-meta">#</span><span class="bash"> 删除某Deployment</span>kubectl delete deployment www<span class="hljs-meta">#</span><span class="bash"> 扩缩容操作，即修改Deployment下的Pod实例个数</span>kubectl scale deployment/www --replicas=2<span class="hljs-meta">#</span><span class="bash"> 更新镜像</span>kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1<span class="hljs-meta">#</span><span class="bash"> 回滚操作</span>kubectl rollout undo deployment/nginx-deployment<span class="hljs-meta">#</span><span class="bash"> 查看回滚进度</span>kubectl rollout status deployment/nginx-deployment<span class="hljs-meta">#</span><span class="bash"> 启用水平伸缩（HPA - horizontal pod autoscaling），设置最小、最大实例数量以及目标cpu使用率</span>kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80<span class="hljs-meta">#</span><span class="bash"> 暂停更新Deployment</span>kubectl rollout pause deployment/nginx-deployment<span class="hljs-meta">#</span><span class="bash"> 恢复更新Deployment</span>kubectl rollout resume deploy nginx</code></pre><p><strong>更新策略</strong></p><p><code>.spec.strategy</code> 指新的Pod替换旧的Pod的策略，有以下两种类型</p><ul><li><code>RollingUpdate</code> 滚动升级，可以保证应用在升级期间，对外正常提供服务。</li><li><code>Recreate</code> 重建策略，在创建出新的Pod之前会先杀掉所有已存在的Pod。</li></ul><p><strong>Deployment和ReplicaSet两者之间的关系</strong></p><ul><li>使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod，检查启动状态，看它是成功还是失败。</li><li>当执行更新操作时，会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移 动到新的ReplicaSet中</li></ul><h1 id="8、StatefulSet-有状态应用"><a href="#8、StatefulSet-有状态应用" class="headerlink" title="8、StatefulSet 有状态应用"></a>8、StatefulSet 有状态应用</h1><p>Deployments和ReplicaSets是为无状态服务设计的，那么StatefulSet则是为了有状态服务而设计，其应用场景包括：</p><ul><li>稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现</li><li>稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service(即没有Cluster IP的Service)来实现 </li><li>有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次进行操作(即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态)，基于init containers来实现 </li><li>有序收缩，有序删除(即从N-1到0)</li></ul><p>支持两种更新策略：</p><ul><li>OnDelete:当<code>.spec.template</code>更新时，并不立即删除旧的Pod，而是等待用户手动删除这些旧Pod后自动创建新Pod。这是默认的更新策略，兼容v1.6版本的行为 </li><li>RollingUpdate:当 <code>.spec.template</code> 更新时，自动删除旧的Pod并创建新Pod替换。在更新时这些Pod是按逆序的方式进行，依次删除、创建并等待Pod变成Ready状态才进行下一个Pod的更新。</li></ul><h1 id="9、DaemonSet-守护进程集"><a href="#9、DaemonSet-守护进程集" class="headerlink" title="9、DaemonSet 守护进程集"></a>9、DaemonSet 守护进程集</h1><p>DaemonSet保证在特定或所有Node节点上都运行一个Pod实例，常用来部署一些集群的日志采集、监控或者其他系统管理应用。典型的应用包括:</p><ul><li>日志收集，比如fluentd，logstash等</li><li>系统监控，比如Prometheus Node Exporter，collectd等</li><li>系统程序，比如kube-proxy, kube-dns, glusterd, ceph，ingress-controller等</li></ul><p><strong>指定Node节点</strong></p><p>DaemonSet会忽略Node的unschedulable状态，有两种方式来指定Pod只运行在指定的Node节点上:</p><ul><li>nodeSelector:只调度到匹配指定label的Node上 </li><li>nodeAffinity:功能更丰富的Node选择器，比如支持集合操作 </li><li>podAffinity:调度到满足条件的Pod所在的Node上</li></ul><p>目前支持两种策略</p><ul><li>OnDelete: 默认策略，更新模板后，只有手动删除了旧的Pod后才会创建新的Pod </li><li>RollingUpdate: 更新DaemonSet模版后，自动删除旧的Pod并创建新的Pod</li></ul><h1 id="10、Ingress-负载均衡"><a href="#10、Ingress-负载均衡" class="headerlink" title="10、Ingress 负载均衡"></a>10、Ingress 负载均衡</h1><p>Kubernetes中的负载均衡我们主要用到了以下两种机制：</p><ul><li>Service：使用Service提供集群内部的负载均衡，Kube-proxy负责将service请求负载均衡到后端的Pod中</li><li>Ingress Controller：使用Ingress提供集群外部的负载均衡</li></ul><p>Service和Pod的IP仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到service所在节点暴露的端口上，然后再由kube-proxy通过边缘路由器将其转发到相关的Pod，Ingress可以给service提供集群外部访问的URL、负载均衡、HTTP路由等，为了配置这些Ingress规则，集群管理员需要部署一个Ingress Controller，它监听Ingress和service的变化，并根据规则配置负载均衡并提供访问入口。</p><p>常用的ingress controller：</p><ul><li>nginx</li><li>traefik</li><li>Kong</li><li>Openresty</li></ul><h1 id="11、Job-amp-CronJob-任务和定时任务"><a href="#11、Job-amp-CronJob-任务和定时任务" class="headerlink" title="11、Job &amp; CronJob 任务和定时任务"></a>11、Job &amp; CronJob 任务和定时任务</h1><p>Job负责批量处理短暂的一次性任务 (short lived one-off tasks)，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束。</p><p>CronJob即定时任务，就类似于Linux系统的crontab，在指定的时间周期运行指定的任务。</p><h1 id="12、HPA（Horizontal-Pod-Autoscaling）-水平伸缩"><a href="#12、HPA（Horizontal-Pod-Autoscaling）-水平伸缩" class="headerlink" title="12、HPA（Horizontal Pod Autoscaling） 水平伸缩"></a>12、HPA（Horizontal Pod Autoscaling） 水平伸缩</h1><p>Horizontal Pod Autoscaling可以根据CPU、内存使用率或应用自定义metrics自动扩展Pod数量 (支持replication controller、deployment和replica set)。</p><ul><li><p>控制管理器默认每隔30s查询metrics的资源使用情况(可以通过 –horizontal-pod-autoscaler-sync-period 修改)</p></li><li><p>支持三种metrics类型</p></li><li><ul><li>预定义metrics(比如Pod的CPU)以利用率的方式计算 </li><li>自定义的Pod metrics，以原始值(raw value)的方式计算 </li><li>自定义的object metrics</li></ul></li><li><p>支持两种metrics查询方式:Heapster和自定义的REST API </p></li><li><p>支持多metrics</p></li></ul><p>可以通过如下命令创建HPA：</p><pre><code class="hljs shell">kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10</code></pre><h1 id="13、Service-Account"><a href="#13、Service-Account" class="headerlink" title="13、Service Account"></a>13、Service Account</h1><p>Service account是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的</p><p><strong>授权</strong></p><p>Service Account为服务提供了一种方便的认证机制，但它不关心授权的问题。可以配合RBAC(Role Based Access Control)来为Service Account鉴权，通过定义Role、RoleBinding、ClusterRole、ClusterRoleBinding来对sa进行授权。</p><h1 id="14、-Secret-密钥"><a href="#14、-Secret-密钥" class="headerlink" title="14、 Secret 密钥"></a>14、 Secret 密钥</h1><p>Sercert-密钥解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。有如下三种类型：</p><ul><li>Service Account:用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的 /run/secrets/kubernetes.io/serviceaccount 目录中; </li><li>Opaque:base64编码格式的Secret，用来存储密码、密钥等;</li><li>kubernetes.io/dockerconfigjson: 用来存储私有docker registry的认证信息。</li></ul><h1 id="15、ConfigMap-配置中心"><a href="#15、ConfigMap-配置中心" class="headerlink" title="15、ConfigMap 配置中心"></a>15、ConfigMap 配置中心</h1><p>ConfigMap用于保存配置数据的键值对，可以用来保存单个属性，也可以用来保存配置文件。ConfigMap跟secret很类似，但它可以更方便地处理不包含敏感信息的字符串。ConfigMap可以通过三种方式在Pod中使用，三种分别方式为:设置环境变量、设置容器命令行参数以及在Volume中直接挂载文件或目录。</p><p>可以使用 kubectl create configmap从文件、目录或者key-value字符串创建等创建 ConfigMap。也可以通过 kubectl create -f value.yaml 创建。</p><h1 id="16、Resource-Quotas-资源配额"><a href="#16、Resource-Quotas-资源配额" class="headerlink" title="16、Resource Quotas 资源配额"></a>16、Resource Quotas 资源配额</h1><p>资源配额(Resource Quotas)是用来限制用户资源用量的一种机制。</p><p>资源配额有如下类型：</p><ul><li><p>计算资源，包括cpu和memory</p></li><li><ul><li>cpu, limits.cpu, requests.cpu</li><li>memory, limits.memory, requests.memory</li></ul></li><li><p>存储资源，包括存储资源的总量以及指定storage class的总量 </p></li><li><ul><li>requests.storage:存储资源总量，如500Gi </li><li>persistentvolumeclaims:pvc的个数 </li><li>.storageclass.storage.k8s.io/requests.storage </li><li>.storageclass.storage.k8s.io/persistentvolumeclaims</li></ul></li><li><p>对象数，即可创建的对象的个数</p></li><li><ul><li>pods, replicationcontrollers, configmaps, secrets </li><li>resourcequotas, persistentvolumeclaims</li><li>services, services.loadbalancers, services.nodeports</li></ul></li></ul><p>它的工作原理为:</p><ul><li>资源配额应用在Namespace上，并且每个Namespace最多只能有一个 ResourceQuota 对象 </li><li>开启计算资源配额后，创建容器时必须配置计算资源请求或限制(也可以 用LimitRange设置默认值)</li><li>用户超额后禁止创建新的资源</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Kubernetes系列】第1篇 架构及组件介绍 </title>
    <link href="/2019/11/29/2019-11-29-kubernetes1-architecture-component/"/>
    <url>/2019/11/29/2019-11-29-kubernetes1-architecture-component/</url>
    
    <content type="html"><![CDATA[<h1 id="1、Kubernetes简介"><a href="#1、Kubernetes简介" class="headerlink" title="1、Kubernetes简介"></a>1、Kubernetes简介</h1><p>Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的开源版本，主要功能包括:    </p><ul><li><p>基于容器的应用部署、维护和滚动升级</p></li><li><p>负载均衡和服务发现</p></li><li><p>跨机器和跨地区的集群调度</p></li><li><p>自动伸缩</p></li><li><p>无状态服务和有状态服务 </p></li><li><p>广泛的Volume支持 </p></li><li><p>插件机制保证扩展性</p></li></ul><p>Kubernetes发展非常迅速，已经成为容器编排领域的领导者。</p><h1 id="2、Kubernetes-架构及组件介绍"><a href="#2、Kubernetes-架构及组件介绍" class="headerlink" title="2、Kubernetes 架构及组件介绍"></a>2、Kubernetes 架构及组件介绍</h1><h2 id="2-1-kubernetes-架构"><a href="#2-1-kubernetes-架构" class="headerlink" title="2.1 kubernetes 架构"></a>2.1 kubernetes 架构</h2><p>Kubernetes架构如图所示：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-11-29/2-1.jpg" srcset="/img/loading.gif" alt="avatar"></p><p>Kubernetes主要由以下几个核心组件构成：</p><ul><li><p>etcd 保存整个集群的状态；</p></li><li><p>apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；</p></li><li><p>controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；</p></li><li><p>scheduler 负责资源的调度，按照预定的调度策略将实例（Pod）调度到相应的主机上；</p></li><li><p>kubelet 负责维护容器的生命周期，同时也负责存储卷和网络的管理；</p></li><li><p>container runtime 负责镜像管理以及容器的真正执行，在我们系统中指的是Docker</p></li><li><p>kube-proxy 负责为应用提供集群内部的服务发现和负载均衡</p></li><li><p>推荐的插件</p><ul><li><p>helm - kubernetes包管理工具</p></li><li><p>kube-dns/coreDNS 负责为整个集群提供DNS服务</p></li><li><p>Ingress Controller 为服务提供外网入口</p></li><li><p>Heapster 提供资源监控</p></li><li><p>Dashboard 提供GUI</p></li><li><p>Federation 提供跨可用区的集群</p></li><li><p>Fluentd-elasticsearch 提供集群日志采集、存储与查询</p></li></ul></li></ul><h2 id="2-2-Kubernetes组件介绍"><a href="#2-2-Kubernetes组件介绍" class="headerlink" title="2.2 Kubernetes组件介绍"></a>2.2 Kubernetes组件介绍</h2><h3 id="2-2-1-etcd"><a href="#2-2-1-etcd" class="headerlink" title="2.2.1 etcd"></a>2.2.1 etcd</h3><p>etcd是基于Raft一致性算法开发的分布式key-value存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）</p><p>   <strong>etcd主要功能：</strong></p><ul><li><p>基本的key-value存储</p></li><li><p>监听机制</p></li><li><p>key的过期及续约机制，用于监控和服务发现</p></li><li><p>原子CAS和CAD，用于分布式锁和leader选举</p></li></ul><p> <strong>Etcd基于RAFT的一致性</strong></p><p> <strong>leader节点选举方法</strong></p><ul><li><p>初始启动时，节点处于follower状态并被设定一个election timeout，如果在这一时间周期内没有收到来自leader的心跳检测，节点将发起选举，将自己切换为candidate（候选人）节点之后，向集群中的其他follow节点发送请求，询问其是否选举自己为leader</p></li><li><p>当收到来自集群中过半数节点的接受投票后，节点即成为leader，开始接收保存client的数据并向其他的follower节点同步日志。如果没有达成一致，则candidate节点随机选择一个等待时间（150ms ～ 300ms）再次发起投票，得到集群中半数以上的follower接受的candidate将成为leader</p></li><li><p>leader节点依靠定时向follower节点发送心跳检测来保持其地位</p></li><li><p>任何时候如果其他follower在election timeout期间没有收到来自leader的心跳检测，同样会将自己的状态切换为candidate并发起选举。每成功选举一次，新leader的步进数（Term）都会比之前leader的步进数加1 </p></li></ul><p><strong>失效处理</strong></p><ul><li><p>leader失效：其他没有收到心跳检测的节点将发起新的选举，当leader恢复后由于步进数小自动成为follower（日志会被新leader的日志覆盖）</p></li><li><p>follower节点不可用：follower节点不可用的情况相对比较容易解决。因为集群中的日志内容始终是从leader节点同步，只要这一节点再次加入集群时重新从leader节点处复制日志即可</p></li><li><p>多个候选人（candidate）：冲突后candidate将随机选择一个等待时间（150ms ～ 300ms）再次发起投票，得到集群中半数以上的follower接受的candidate将成为leader  </p></li></ul><p>讲到这里可能有同学发现Etcd和Zookeeper、Consul等一致性协议实现框架有些类似，的确这些中间件是比较类似的，关于其中的异同点，大家可以自行查阅资料。</p><h3 id="2-2-2-kube-apiserver"><a href="#2-2-2-kube-apiserver" class="headerlink" title="2.2.2 kube-apiserver"></a>2.2.2 kube-apiserver</h3><p>kube-apiserver是Kubernetes最重要的核心组件之一，主要提供了如下功能：</p><ul><li><p>提供集群管理的REST API接口，包括认证授权、数据校验以及集群状态变更等</p></li><li><p>提供同其他模块之间的数据交互(其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd)</p></li></ul><h3 id="2-2-3-kube-scheduler"><a href="#2-2-3-kube-scheduler" class="headerlink" title="2.2.3 kube-scheduler"></a>2.2.3 kube-scheduler</h3><p>kube-scheduler负责分配调度Pod到集群内的节点上，它监听kube-<br>apiserver，查询还未分配Node的Pod，然后根据调度策略为这些Pod分配节点</p><p>通过以下三种方式可以指定Pod只运行在特定的Node节点上</p><ul><li><p>nodeSelector:只调度到匹配指定label的Node上 </p></li><li><p>nodeAffinity:功能更丰富的Node选择器，比如支持集合操作 </p></li><li><p>podAffinity:调度到满足条件的Pod所在的Node上</p></li></ul><h3 id="2-2-4-kube-controller-manager"><a href="#2-2-4-kube-controller-manager" class="headerlink" title="2.2.4 kube-controller-manager"></a>2.2.4 kube-controller-manager</h3><p>kube-controller-manager是Kubernetes的大脑，通过kube-<br>apiserver监控整个集群的状态，并确保集群处于预期的工作状态，它由一系列的控制器组成，这些控制器主要包括三组：</p><p> <strong>1. 必须启动的控制器</strong>  </p><ul><li><p>eploymentController</p></li><li><p>DaemonSetController</p></li><li><p>NamesapceController</p></li><li><p>ReplicationController</p></li><li><p>RelicaSet</p></li><li><p>JobController</p></li><li><p>…</p><p><strong>2. 默认启动的控制器</strong>  </p></li><li><p>NodeController</p></li><li><p>ServiceController</p></li><li><p>PVBinderController</p></li><li><p>…</p><p><strong>3. 默认禁止的可选控制器</strong></p></li><li><p>BootstrapSignerController</p></li><li><p>TokenCleanerController</p></li></ul><h3 id="2-2-5-Kubelet"><a href="#2-2-5-Kubelet" class="headerlink" title="2.2.5 Kubelet"></a>2.2.5 Kubelet</h3><p>每个Node节点上都运行一个kubelet守护进程，默认监听10250端口，接收并执行master发来的指令，管理Pod及Pod中的容器。每个kubelet进程会在API<br>Server上注册节点自身信息，定期向master节点汇报节点的资源使用情况</p><p> <strong>节点管理</strong></p><p>主要是节点自注册和节点状态更新:</p><ul><li><p>Kubelet可以通过设置启动参数 –register-node 来确定是否向API Server注册自己; </p></li><li><p>如果Kubelet没有选择自注册模式，则需要用户自己配置Node资源信息，同时需要在Kubelet上配置集群中API Server的信息;</p></li><li><p>Kubelet在启动时通过API Server注册节点信息，并定时向API Server发送节点状态消息，API Server在接收到新消息后，将信息写入etcd  </p><p><strong>容器健康检查</strong></p></li></ul><p>Pod通过两类探针检查容器的健康状态</p><ul><li><p>LivenessProbe 存活探针：通过该探针判断容器是否健康，告诉Kubelet一个容器什么时候处于不健康的状态。如果LivenessProbe探针探测到容器不健康，则kubelet将删除该容器，并根据容器的重启策略做相应的处理。如果一个容器不包含LivenessProbe探针，那么kubelet认为该容器的LivenessProbe探针返回的值永远是“Success”。</p></li><li><p>ReadinessProbe 就绪探针：用于判断容器是否启动完成且准备接收请求。如果 ReadinessProbe 探针探测到失败，则Pod的状态将被修改。Endpoint Controller将从Service的Endpoint中删除包含该容器所在Pod的IP地址的Endpoint条目。</p></li></ul><p>以下是Pod的启动流程：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-11-29/2-2-5.jpg" srcset="/img/loading.gif" alt="avatar"></p><h3 id="2-2-6-kube-proxy"><a href="#2-2-6-kube-proxy" class="headerlink" title="2.2.6 kube-proxy"></a>2.2.6 kube-proxy</h3><p>每台机器上都运行一个kube-proxy服务，它监听API<br>Server中service和Pod的变化情况，并通过userspace、iptables、ipvs等proxier来为服务配置负载均衡</p><p>代理模式（proxy-mode）提供如下三种类型：</p><p> <strong>1. userspace</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-11-29/2-2-6-1.png" srcset="/img/loading.gif" alt="avatar"></p><p>最早的负载均衡方案，它在用户空间监听一个端口，所有请求通过 iptables 转发到这个端口，然后在其内部负载均衡到实际的<br>Pod。service的请求会先从用户空间进入内核iptables，然后再回到用户空间（kube-proxy），由kube-<br>proxy完成后端Endpoints的选择和代理工作，这样流量从用户空间进出内核带来的性能损耗是不可接受的，所以产生了iptables的代理模式</p><p> <strong>2. iptables:</strong></p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-11-29/2-2-6-2.png" srcset="/img/loading.gif" alt="avatar"></p><p>iptables<br>mode完全使用iptables来完成请求过滤和转发。但是如果集群中存在大量的Service/Endpoint，那么Node上的iptables<br>rules将会非常庞大，添加或者删除iptables规则会引起较大的延迟。  </p><p> <strong>3. ipvs:</strong></p><p>为了解决存在大量iptables规则时的网络延迟的问题，Kubernetes引入了ipvs的模式，（ipvs是LVS - Linux Virtual<br>Server 的重要组成部分，最早是由中国的章文嵩博士推出的一个开源项目，提供软件负载均衡的解决方案），下面是ipvs模式的原理图：</p><p><img src="https://gitee.com/like-ycy/images/raw/master/blog/2019-11-29/2-2-6-3.png" srcset="/img/loading.gif" alt="avatar"></p>]]></content>
    
    
    <categories>
      
      <category>Kubernetes</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
